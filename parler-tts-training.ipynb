{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f7cbae",
   "metadata": {
    "id": "AtIsihRyI3kz",
    "papermill": {
     "duration": 0.003292,
     "end_time": "2024-12-01T23:18:51.696644",
     "exception": false,
     "start_time": "2024-12-01T23:18:51.693352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clone Repo and Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2063375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:18:51.703832Z",
     "iopub.status.busy": "2024-12-01T23:18:51.703502Z",
     "iopub.status.idle": "2024-12-01T23:18:51.711697Z",
     "shell.execute_reply": "2024-12-01T23:18:51.710761Z"
    },
    "papermill": {
     "duration": 0.013924,
     "end_time": "2024-12-01T23:18:51.713599",
     "exception": false,
     "start_time": "2024-12-01T23:18:51.699675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GIT_LFS_SKIP_SMUDGE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b03770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:18:51.719870Z",
     "iopub.status.busy": "2024-12-01T23:18:51.719598Z",
     "iopub.status.idle": "2024-12-01T23:19:37.802946Z",
     "shell.execute_reply": "2024-12-01T23:19:37.801807Z"
    },
    "id": "u-Q-xN3nIvNH",
    "outputId": "5926e079-86fc-4e08-a0d9-27e4459d103b",
    "papermill": {
     "duration": 46.08867,
     "end_time": "2024-12-01T23:19:37.805004",
     "exception": false,
     "start_time": "2024-12-01T23:18:51.716334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'parler-tts'...\r\n",
      "remote: Enumerating objects: 1088, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (427/427), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (158/158), done.\u001b[K\r\n",
      "remote: Total 1088 (delta 314), reused 273 (delta 269), pack-reused 661 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (1088/1088), 356.59 KiB | 4.82 MiB/s, done.\r\n",
      "Resolving deltas: 100% (694/694), done.\r\n",
      "/kaggle/working/parler-tts\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\r\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "google-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\r\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/parler-tts.git\n",
    "%cd parler-tts\n",
    "!pip install --quiet -e .[train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c143f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:37.812934Z",
     "iopub.status.busy": "2024-12-01T23:19:37.812600Z",
     "iopub.status.idle": "2024-12-01T23:19:57.049626Z",
     "shell.execute_reply": "2024-12-01T23:19:57.048514Z"
    },
    "papermill": {
     "duration": 19.243469,
     "end_time": "2024-12-01T23:19:57.051732",
     "exception": false,
     "start_time": "2024-12-01T23:19:37.808263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (4.25.3)\r\n",
      "Collecting protobuf\r\n",
      "  Using cached protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\r\n",
      "Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: protobuf\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 4.25.3\r\n",
      "    Uninstalling protobuf-4.25.3:\r\n",
      "      Successfully uninstalled protobuf-4.25.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\r\n",
      "descript-audiotools 0.7.4 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-artifact-registry 1.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-bigquery-connection 1.15.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-dlp 3.18.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-functions 1.16.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-monitoring 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-pubsub 2.21.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-recommendations-ai 0.7.1 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-spanner 3.47.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.2, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "google-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "grpc-google-iam-v1 0.12.7 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "opentelemetry-proto 1.25.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "tensorboard-plugin-profile 2.15.1 requires protobuf<5.0.0dev,>=3.19.6, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "tensorflow 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\r\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 5.29.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-4.25.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ec5f0",
   "metadata": {
    "papermill": {
     "duration": 0.003629,
     "end_time": "2024-12-01T23:19:57.059595",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.055966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare Huggingface Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13ca821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:57.069628Z",
     "iopub.status.busy": "2024-12-01T23:19:57.069260Z",
     "iopub.status.idle": "2024-12-01T23:19:57.417350Z",
     "shell.execute_reply": "2024-12-01T23:19:57.416691Z"
    },
    "papermill": {
     "duration": 0.354872,
     "end_time": "2024-12-01T23:19:57.419510",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.064638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1984f2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:57.428349Z",
     "iopub.status.busy": "2024-12-01T23:19:57.428085Z",
     "iopub.status.idle": "2024-12-01T23:19:57.431785Z",
     "shell.execute_reply": "2024-12-01T23:19:57.431124Z"
    },
    "papermill": {
     "duration": 0.010577,
     "end_time": "2024-12-01T23:19:57.434260",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.423683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708a7771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:57.444066Z",
     "iopub.status.busy": "2024-12-01T23:19:57.443786Z",
     "iopub.status.idle": "2024-12-01T23:19:57.447904Z",
     "shell.execute_reply": "2024-12-01T23:19:57.447120Z"
    },
    "papermill": {
     "duration": 0.010453,
     "end_time": "2024-12-01T23:19:57.449533",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.439080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_api = HfApi()\n",
    "HfFolder.save_token(os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca1646e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:57.457616Z",
     "iopub.status.busy": "2024-12-01T23:19:57.457201Z",
     "iopub.status.idle": "2024-12-01T23:19:57.564107Z",
     "shell.execute_reply": "2024-12-01T23:19:57.563284Z"
    },
    "papermill": {
     "duration": 0.113006,
     "end_time": "2024-12-01T23:19:57.566154",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.453148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as: Amadeus99\n"
     ]
    }
   ],
   "source": [
    "user_info = hf_api.whoami()\n",
    "print(f\"Logged in as: {user_info['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab916cc",
   "metadata": {
    "id": "HrsxyGu2JvdV",
    "papermill": {
     "duration": 0.00367,
     "end_time": "2024-12-01T23:19:57.574009",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.570339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Annotate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb2bfe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T23:19:57.583311Z",
     "iopub.status.busy": "2024-12-01T23:19:57.582978Z",
     "iopub.status.idle": "2024-12-02T04:21:57.179126Z",
     "shell.execute_reply": "2024-12-02T04:21:57.178230Z"
    },
    "id": "2wiGszdNTcmu",
    "outputId": "f85e642b-7686-4928-914b-206f51a337bd",
    "papermill": {
     "duration": 18119.604555,
     "end_time": "2024-12-02T04:21:57.182457",
     "exception": false,
     "start_time": "2024-12-01T23:19:57.577902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\r\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:412: UserWarning: `log_with=[]` was passed but no supported trackers are currently installed.\r\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\r\n",
      "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\r\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:412: UserWarning: `log_with=[]` was passed but no supported trackers are currently installed.\r\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\r\n",
      "preprocessor_config.json: 100%|████████████████| 234/234 [00:00<00:00, 1.19MB/s]\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--parler-tts--dac_44khZ_8kbps/snapshots/db52bea859d9411e0beb44a3ea923a8731ee4197/preprocessor_config.json\r\n",
      "Feature extractor EncodecFeatureExtractor {\r\n",
      "  \"chunk_length_s\": null,\r\n",
      "  \"feature_extractor_type\": \"EncodecFeatureExtractor\",\r\n",
      "  \"feature_size\": 1,\r\n",
      "  \"overlap\": null,\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"return_attention_mask\": true,\r\n",
      "  \"sampling_rate\": 44100\r\n",
      "}\r\n",
      "\r\n",
      "tokenizer_config.json: 100%|███████████████| 20.8k/20.8k [00:00<00:00, 68.6MB/s]\r\n",
      "spiece.model: 100%|██████████████████████████| 792k/792k [00:00<00:00, 15.0MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 2.42M/2.42M [00:00<00:00, 27.6MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████| 2.54k/2.54k [00:00<00:00, 24.6MB/s]\r\n",
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/spiece.model\r\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/tokenizer.json\r\n",
      "loading file added_tokens.json from cache at None\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/tokenizer_config.json\r\n",
      "Combining datasets...:   0%|                              | 0/1 [00:00<?, ?it/s]loading file spiece.model from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/spiece.model\r\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/tokenizer.json\r\n",
      "loading file added_tokens.json from cache at None\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/tokenizer_config.json\r\n",
      "Combining datasets...:   0%|                              | 0/1 [00:00<?, ?it/s]\r\n",
      "README.md: 100%|███████████████████████████████| 593/593 [00:00<00:00, 3.72MB/s]\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\r\n",
      "train-00000-of-00013.parquet:   0%|                  | 0.00/481M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:   0%|                  | 0.00/481M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:   2%|▏        | 10.5M/481M [00:00<00:11, 39.6MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:   2%|▏        | 10.5M/481M [00:00<00:12, 37.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:   4%|▍        | 21.0M/481M [00:00<00:11, 40.9MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:   4%|▍        | 21.0M/481M [00:00<00:11, 40.1MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:   7%|▌        | 31.5M/481M [00:00<00:10, 41.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:   7%|▌        | 31.5M/481M [00:00<00:10, 41.5MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:   9%|▊        | 41.9M/481M [00:01<00:10, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:   9%|▊        | 41.9M/481M [00:01<00:10, 42.5MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  11%|▉        | 52.4M/481M [00:01<00:10, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  11%|▉        | 52.4M/481M [00:01<00:10, 42.5MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  13%|█▏       | 62.9M/481M [00:01<00:10, 41.8MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  13%|█▏       | 62.9M/481M [00:01<00:09, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  15%|█▎       | 73.4M/481M [00:01<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  15%|█▎       | 73.4M/481M [00:01<00:09, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  17%|█▌       | 83.9M/481M [00:02<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  17%|█▌       | 83.9M/481M [00:02<00:09, 42.3MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  20%|█▊       | 94.4M/481M [00:02<00:09, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  20%|█▊       | 94.4M/481M [00:02<00:09, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  22%|██▏       | 105M/481M [00:02<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  22%|██▏       | 105M/481M [00:02<00:09, 41.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  24%|██▍       | 115M/481M [00:02<00:08, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  24%|██▍       | 115M/481M [00:02<00:08, 41.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  26%|██▌       | 126M/481M [00:02<00:08, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  26%|██▌       | 126M/481M [00:03<00:08, 41.5MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  28%|██▊       | 136M/481M [00:03<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  28%|██▊       | 136M/481M [00:03<00:08, 41.8MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  31%|███       | 147M/481M [00:03<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  31%|███       | 147M/481M [00:03<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  33%|███▎      | 157M/481M [00:03<00:07, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  33%|███▎      | 157M/481M [00:03<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  35%|███▍      | 168M/481M [00:04<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  35%|███▍      | 168M/481M [00:04<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  37%|███▋      | 178M/481M [00:04<00:07, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  37%|███▋      | 178M/481M [00:04<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  39%|███▉      | 189M/481M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  39%|███▉      | 189M/481M [00:04<00:06, 41.9MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  41%|████▏     | 199M/481M [00:04<00:06, 42.1MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  41%|████▏     | 199M/481M [00:04<00:06, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  44%|████▎     | 210M/481M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  44%|████▎     | 210M/481M [00:05<00:06, 42.1MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  46%|████▌     | 220M/481M [00:05<00:06, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  46%|████▌     | 220M/481M [00:05<00:06, 41.9MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  48%|████▊     | 231M/481M [00:05<00:05, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  48%|████▊     | 231M/481M [00:05<00:05, 41.8MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  50%|█████     | 241M/481M [00:05<00:05, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  50%|█████     | 241M/481M [00:05<00:05, 41.8MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  52%|█████▏    | 252M/481M [00:06<00:05, 41.9MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  52%|█████▏    | 252M/481M [00:06<00:05, 41.3MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  54%|█████▍    | 262M/481M [00:06<00:05, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  55%|█████▍    | 262M/481M [00:06<00:05, 41.4MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  57%|█████▋    | 273M/481M [00:06<00:04, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  57%|█████▋    | 273M/481M [00:06<00:04, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  59%|█████▉    | 283M/481M [00:06<00:04, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  59%|█████▉    | 283M/481M [00:06<00:04, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  61%|██████    | 294M/481M [00:07<00:04, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  61%|██████    | 294M/481M [00:07<00:04, 42.0MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  63%|██████▎   | 304M/481M [00:07<00:04, 42.0MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  63%|██████▎   | 304M/481M [00:07<00:04, 42.1MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  65%|██████▌   | 315M/481M [00:07<00:03, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  65%|██████▌   | 315M/481M [00:07<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  68%|██████▊   | 325M/481M [00:07<00:03, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  68%|██████▊   | 325M/481M [00:07<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  70%|██████▉   | 336M/481M [00:08<00:03, 42.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  70%|██████▉   | 336M/481M [00:08<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  72%|███████▏  | 346M/481M [00:08<00:03, 42.4MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  72%|███████▏  | 346M/481M [00:08<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  74%|███████▍  | 357M/481M [00:08<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  74%|███████▍  | 357M/481M [00:08<00:02, 42.0MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  76%|███████▋  | 367M/481M [00:08<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  76%|███████▋  | 367M/481M [00:08<00:02, 41.9MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  78%|███████▊  | 377M/481M [00:09<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  79%|███████▊  | 377M/481M [00:08<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  81%|████████  | 388M/481M [00:09<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  81%|████████  | 388M/481M [00:09<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  83%|████████▎ | 398M/481M [00:09<00:01, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  83%|████████▎ | 398M/481M [00:09<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  85%|████████▌ | 409M/481M [00:09<00:01, 42.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  85%|████████▌ | 409M/481M [00:09<00:01, 42.0MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  87%|████████▋ | 419M/481M [00:09<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  87%|████████▋ | 419M/481M [00:09<00:01, 42.3MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  89%|████████▉ | 430M/481M [00:10<00:01, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  89%|████████▉ | 430M/481M [00:10<00:01, 42.5MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  92%|█████████▏| 440M/481M [00:10<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  92%|█████████▏| 440M/481M [00:10<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  94%|█████████▎| 451M/481M [00:10<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  94%|█████████▍| 451M/481M [00:10<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  96%|█████████▌| 461M/481M [00:10<00:00, 42.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  96%|█████████▌| 461M/481M [00:10<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet:  98%|█████████▊| 472M/481M [00:11<00:00, 42.3MB/s]\u001b[A\r\n",
      "train-00007-of-00013.parquet:  98%|█████████▊| 472M/481M [00:11<00:00, 41.6MB/s]\u001b[A\r\n",
      "train-00000-of-00013.parquet: 100%|██████████| 481M/481M [00:11<00:00, 42.0MB/s]\r\n",
      "\r\n",
      "train-00007-of-00013.parquet: 100%|██████████| 481M/481M [00:11<00:00, 41.9MB/s]\r\n",
      "\r\n",
      "train-00001-of-00013.parquet:   0%|                  | 0.00/481M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:   0%|                  | 0.00/481M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:   2%|▏        | 10.5M/481M [00:00<00:11, 42.0MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:   2%|▏        | 10.5M/481M [00:00<00:11, 41.6MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:   4%|▍        | 21.0M/481M [00:00<00:10, 42.1MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:   4%|▍        | 21.0M/481M [00:00<00:11, 39.4MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:   7%|▌        | 31.5M/481M [00:00<00:10, 42.8MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:   7%|▌        | 31.5M/481M [00:00<00:10, 41.1MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:   9%|▊        | 41.9M/481M [00:00<00:10, 42.5MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:   9%|▊        | 41.9M/481M [00:01<00:10, 41.4MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  11%|▉        | 52.4M/481M [00:01<00:10, 42.6MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  11%|▉        | 52.4M/481M [00:01<00:10, 41.6MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  13%|█▏       | 62.9M/481M [00:01<00:09, 42.5MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  13%|█▏       | 62.9M/481M [00:01<00:10, 41.2MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  15%|█▎       | 73.4M/481M [00:01<00:09, 42.3MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  15%|█▎       | 73.4M/481M [00:01<00:09, 41.5MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  17%|█▌       | 83.9M/481M [00:01<00:09, 42.4MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  17%|█▌       | 83.9M/481M [00:02<00:09, 41.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  20%|█▊       | 94.4M/481M [00:02<00:09, 42.5MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  20%|█▊       | 94.4M/481M [00:02<00:09, 41.8MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  22%|██▏       | 105M/481M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  22%|██▏       | 105M/481M [00:02<00:09, 41.7MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  24%|██▍       | 115M/481M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  24%|██▍       | 115M/481M [00:02<00:08, 41.6MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  26%|██▌       | 126M/481M [00:02<00:08, 41.5MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  26%|██▌       | 126M/481M [00:03<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  28%|██▊       | 136M/481M [00:03<00:08, 41.7MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  28%|██▊       | 136M/481M [00:03<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  31%|███       | 147M/481M [00:03<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  31%|███       | 147M/481M [00:03<00:08, 41.5MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  33%|███▎      | 157M/481M [00:03<00:07, 42.0MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  33%|███▎      | 157M/481M [00:03<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  35%|███▍      | 168M/481M [00:04<00:08, 34.8MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  35%|███▍      | 168M/481M [00:04<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  37%|███▋      | 178M/481M [00:04<00:08, 36.8MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  37%|███▋      | 178M/481M [00:04<00:07, 41.6MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  39%|███▉      | 189M/481M [00:04<00:07, 38.4MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  39%|███▉      | 189M/481M [00:04<00:06, 41.8MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  41%|████▏     | 199M/481M [00:04<00:07, 39.6MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  41%|████▏     | 199M/481M [00:04<00:06, 41.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  44%|████▎     | 210M/481M [00:05<00:06, 40.4MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  44%|████▎     | 210M/481M [00:05<00:06, 42.0MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  46%|████▌     | 220M/481M [00:05<00:06, 40.9MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  46%|████▌     | 220M/481M [00:05<00:06, 41.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  48%|████▊     | 231M/481M [00:05<00:06, 41.2MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  48%|████▊     | 231M/481M [00:05<00:06, 40.1MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  50%|█████     | 241M/481M [00:05<00:05, 41.4MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  50%|█████     | 241M/481M [00:05<00:05, 40.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  52%|█████▏    | 252M/481M [00:06<00:05, 41.4MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  52%|█████▏    | 252M/481M [00:06<00:05, 41.3MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  55%|█████▍    | 262M/481M [00:06<00:05, 41.2MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  55%|█████▍    | 262M/481M [00:06<00:05, 41.8MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  57%|█████▋    | 273M/481M [00:06<00:05, 41.3MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  57%|█████▋    | 273M/481M [00:06<00:05, 41.5MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  59%|█████▉    | 283M/481M [00:06<00:04, 41.7MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  59%|█████▉    | 283M/481M [00:06<00:04, 41.5MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  61%|██████    | 294M/481M [00:07<00:04, 41.9MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  61%|██████    | 294M/481M [00:07<00:04, 41.2MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  63%|██████▎   | 304M/481M [00:07<00:04, 41.7MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  63%|██████▎   | 304M/481M [00:07<00:04, 41.2MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  65%|██████▌   | 315M/481M [00:07<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  65%|██████▌   | 315M/481M [00:07<00:04, 41.5MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  68%|██████▊   | 325M/481M [00:07<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  68%|██████▊   | 325M/481M [00:08<00:04, 34.4MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  70%|██████▉   | 336M/481M [00:08<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  70%|██████▉   | 336M/481M [00:08<00:03, 36.4MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  72%|███████▏  | 346M/481M [00:08<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  74%|███████▍  | 357M/481M [00:08<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  72%|███████▏  | 346M/481M [00:08<00:03, 37.1MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  76%|███████▋  | 367M/481M [00:08<00:02, 41.6MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  74%|███████▍  | 357M/481M [00:08<00:03, 37.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  79%|███████▊  | 377M/481M [00:09<00:02, 41.8MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  76%|███████▋  | 367M/481M [00:09<00:02, 38.8MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  81%|████████  | 388M/481M [00:09<00:02, 42.1MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  79%|███████▊  | 377M/481M [00:09<00:02, 39.4MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  83%|████████▎ | 398M/481M [00:09<00:01, 41.9MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  81%|████████  | 388M/481M [00:09<00:02, 39.4MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  85%|████████▌ | 409M/481M [00:09<00:01, 42.1MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  83%|████████▎ | 398M/481M [00:09<00:02, 40.3MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  87%|████████▋ | 419M/481M [00:10<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  85%|████████▌ | 409M/481M [00:10<00:01, 41.0MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  89%|████████▉ | 430M/481M [00:10<00:01, 42.1MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  87%|████████▋ | 419M/481M [00:10<00:01, 41.0MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  92%|█████████▏| 440M/481M [00:10<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  89%|████████▉ | 430M/481M [00:10<00:01, 41.0MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  94%|█████████▍| 451M/481M [00:10<00:00, 41.9MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  92%|█████████▏| 440M/481M [00:10<00:00, 40.9MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  96%|█████████▌| 461M/481M [00:11<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  94%|█████████▍| 451M/481M [00:11<00:00, 41.1MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet:  98%|█████████▊| 472M/481M [00:11<00:00, 42.4MB/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet:  96%|█████████▌| 461M/481M [00:11<00:00, 41.1MB/s]\u001b[A\r\n",
      "train-00001-of-00013.parquet: 100%|██████████| 481M/481M [00:11<00:00, 41.5MB/s]\r\n",
      "\r\n",
      "train-00008-of-00013.parquet:  98%|█████████▊| 472M/481M [00:11<00:00, 40.6MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:   0%|                  | 0.00/480M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00008-of-00013.parquet: 100%|██████████| 481M/481M [00:11<00:00, 40.7MB/s]\r\n",
      "\r\n",
      "train-00002-of-00013.parquet:   2%|▏        | 10.5M/480M [00:00<00:11, 42.0MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:   0%|                  | 0.00/479M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:   4%|▍        | 21.0M/480M [00:00<00:10, 41.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:   2%|▏        | 10.5M/479M [00:00<00:11, 41.7MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:   7%|▌        | 31.5M/480M [00:00<00:10, 42.8MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:   4%|▍        | 21.0M/479M [00:00<00:10, 41.9MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:   9%|▊        | 41.9M/480M [00:00<00:10, 42.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:   7%|▌        | 31.5M/479M [00:00<00:10, 42.4MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  11%|▉        | 52.4M/480M [00:01<00:10, 42.6MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:   9%|▊        | 41.9M/479M [00:00<00:10, 42.2MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  13%|█▏       | 62.9M/480M [00:01<00:09, 42.6MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  11%|▉        | 52.4M/479M [00:01<00:10, 41.6MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  15%|█▍       | 73.4M/480M [00:01<00:09, 42.7MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  13%|█▏       | 62.9M/479M [00:01<00:10, 41.7MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  17%|█▌       | 83.9M/480M [00:01<00:09, 42.6MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  15%|█▍       | 73.4M/479M [00:01<00:09, 41.8MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  20%|█▊       | 94.4M/480M [00:02<00:09, 42.4MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  17%|█▌       | 83.9M/479M [00:02<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  22%|██▏       | 105M/480M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  20%|█▊       | 94.4M/479M [00:02<00:09, 42.1MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  24%|██▍       | 115M/480M [00:02<00:08, 41.7MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  22%|██▏       | 105M/479M [00:02<00:08, 42.2MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  26%|██▌       | 126M/480M [00:02<00:08, 41.7MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  24%|██▍       | 115M/479M [00:02<00:08, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  28%|██▊       | 136M/480M [00:03<00:08, 41.7MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  26%|██▌       | 126M/479M [00:02<00:08, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  31%|███       | 147M/480M [00:03<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  28%|██▊       | 136M/479M [00:03<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  33%|███▎      | 157M/480M [00:03<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  31%|███       | 147M/479M [00:03<00:07, 42.1MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  35%|███▍      | 168M/480M [00:03<00:07, 42.0MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  33%|███▎      | 157M/479M [00:03<00:07, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  37%|███▋      | 178M/480M [00:04<00:07, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  35%|███▍      | 168M/479M [00:03<00:07, 42.2MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  39%|███▉      | 189M/480M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  37%|███▋      | 178M/479M [00:04<00:07, 42.3MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  41%|████▏     | 199M/480M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  39%|███▉      | 189M/479M [00:04<00:06, 42.4MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  44%|████▎     | 210M/480M [00:04<00:06, 41.6MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  42%|████▏     | 199M/479M [00:04<00:06, 41.9MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  46%|████▌     | 220M/480M [00:05<00:06, 41.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  44%|████▎     | 210M/479M [00:04<00:06, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  48%|████▊     | 231M/480M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  46%|████▌     | 220M/479M [00:05<00:06, 42.3MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  50%|█████     | 241M/480M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  48%|████▊     | 231M/479M [00:05<00:06, 41.4MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  52%|█████▏    | 252M/480M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  50%|█████     | 241M/479M [00:05<00:05, 41.3MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  55%|█████▍    | 262M/480M [00:06<00:05, 42.3MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  52%|█████▏    | 252M/479M [00:06<00:05, 41.5MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  57%|█████▋    | 273M/480M [00:06<00:04, 42.4MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  55%|█████▍    | 262M/479M [00:06<00:05, 40.5MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  59%|█████▉    | 283M/480M [00:06<00:04, 42.5MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  57%|█████▋    | 273M/479M [00:06<00:05, 41.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  61%|██████    | 294M/480M [00:06<00:04, 42.4MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  59%|█████▉    | 283M/479M [00:06<00:04, 41.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  63%|██████▎   | 304M/480M [00:07<00:04, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  61%|██████    | 294M/479M [00:07<00:04, 41.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  66%|██████▌   | 315M/480M [00:07<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  63%|██████▎   | 304M/479M [00:07<00:04, 41.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  68%|██████▊   | 325M/480M [00:07<00:03, 42.1MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  66%|██████▌   | 315M/479M [00:07<00:03, 41.4MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  70%|██████▉   | 336M/480M [00:07<00:03, 41.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  68%|██████▊   | 325M/479M [00:07<00:03, 41.6MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  72%|███████▏  | 346M/480M [00:08<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  70%|██████▉   | 336M/479M [00:08<00:03, 41.3MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  74%|███████▍  | 357M/480M [00:08<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  72%|███████▏  | 346M/479M [00:08<00:03, 41.5MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  76%|███████▋  | 367M/480M [00:08<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  74%|███████▍  | 357M/479M [00:08<00:02, 41.4MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  79%|███████▊  | 377M/480M [00:08<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  77%|███████▋  | 367M/479M [00:08<00:02, 41.8MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  81%|████████  | 388M/480M [00:09<00:02, 38.6MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  79%|███████▊  | 377M/479M [00:09<00:02, 41.3MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  83%|████████▎ | 398M/480M [00:09<00:01, 43.4MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  81%|████████  | 388M/479M [00:09<00:02, 42.5MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  85%|████████▌ | 409M/480M [00:09<00:01, 43.3MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  83%|████████▎ | 398M/479M [00:09<00:01, 42.7MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  87%|████████▋ | 419M/480M [00:09<00:01, 43.2MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  85%|████████▌ | 409M/479M [00:09<00:01, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  90%|████████▉ | 430M/480M [00:10<00:01, 42.8MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  87%|████████▋ | 419M/479M [00:10<00:01, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  92%|█████████▏| 440M/480M [00:10<00:00, 42.7MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  90%|████████▉ | 430M/479M [00:10<00:01, 42.0MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  94%|█████████▍| 451M/480M [00:10<00:00, 42.7MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  92%|█████████▏| 440M/479M [00:10<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  96%|█████████▌| 461M/480M [00:10<00:00, 42.8MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  94%|█████████▍| 451M/479M [00:10<00:00, 41.8MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet:  98%|█████████▊| 472M/480M [00:11<00:00, 42.5MB/s]\u001b[A\r\n",
      "train-00002-of-00013.parquet: 100%|██████████| 480M/480M [00:11<00:00, 42.2MB/s]\r\n",
      "\r\n",
      "train-00009-of-00013.parquet:  96%|█████████▌| 461M/479M [00:11<00:00, 41.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet:  98%|█████████▊| 472M/479M [00:11<00:00, 40.9MB/s]\u001b[A\r\n",
      "train-00009-of-00013.parquet: 100%|██████████| 479M/479M [00:11<00:00, 41.7MB/s]\r\n",
      "\r\n",
      "train-00003-of-00013.parquet:   0%|                  | 0.00/481M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:   0%|                  | 0.00/482M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:   2%|▏        | 10.5M/481M [00:00<00:11, 41.5MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:   2%|▏        | 10.5M/482M [00:00<00:11, 41.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:   4%|▍        | 21.0M/481M [00:00<00:10, 42.1MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:   4%|▍        | 21.0M/482M [00:00<00:11, 41.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:   7%|▌        | 31.5M/481M [00:00<00:10, 43.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:   7%|▌        | 31.5M/482M [00:00<00:10, 42.6MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:   9%|▊        | 41.9M/481M [00:00<00:10, 42.7MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:   9%|▊        | 41.9M/482M [00:00<00:10, 42.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  11%|▉        | 52.4M/481M [00:01<00:10, 42.3MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  11%|▉        | 52.4M/482M [00:01<00:10, 42.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  13%|█▏       | 62.9M/481M [00:01<00:09, 42.1MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  13%|█▏       | 62.9M/482M [00:01<00:09, 41.9MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  15%|█▎       | 73.4M/481M [00:01<00:09, 42.1MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  15%|█▎       | 73.4M/482M [00:01<00:09, 41.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  17%|█▌       | 83.9M/482M [00:02<00:09, 41.9MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  17%|█▌       | 83.9M/481M [00:02<00:09, 41.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  20%|█▊       | 94.4M/482M [00:02<00:09, 41.7MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  20%|█▊       | 94.4M/481M [00:02<00:09, 41.2MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  22%|██▏       | 105M/482M [00:02<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  22%|██▏       | 105M/481M [00:02<00:09, 41.5MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  24%|██▍       | 115M/482M [00:02<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  24%|██▍       | 115M/481M [00:02<00:08, 41.6MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  26%|██▌       | 126M/482M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  26%|██▌       | 126M/481M [00:03<00:08, 41.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  28%|██▊       | 136M/482M [00:03<00:08, 42.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  28%|██▊       | 136M/481M [00:03<00:08, 41.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  30%|███       | 147M/482M [00:03<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  31%|███       | 147M/481M [00:03<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  33%|███▎      | 157M/482M [00:03<00:07, 42.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  33%|███▎      | 157M/481M [00:03<00:07, 41.9MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  35%|███▍      | 168M/482M [00:03<00:07, 42.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  35%|███▍      | 168M/481M [00:04<00:09, 33.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  37%|███▋      | 178M/482M [00:04<00:08, 37.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  39%|███▉      | 189M/482M [00:04<00:06, 44.0MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  39%|███▉      | 189M/481M [00:04<00:06, 44.4MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  41%|████▏     | 199M/482M [00:04<00:06, 43.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  41%|████▏     | 199M/481M [00:04<00:06, 44.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  44%|████▎     | 210M/482M [00:04<00:06, 43.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  44%|████▎     | 210M/481M [00:04<00:06, 43.6MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  46%|████▌     | 220M/482M [00:05<00:06, 43.0MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  46%|████▌     | 220M/481M [00:05<00:06, 43.1MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  48%|████▊     | 231M/482M [00:05<00:05, 42.5MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  48%|████▊     | 231M/481M [00:05<00:05, 42.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  50%|█████     | 241M/482M [00:05<00:05, 42.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  50%|█████     | 241M/481M [00:05<00:05, 42.7MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  52%|█████▏    | 252M/482M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  52%|█████▏    | 252M/481M [00:05<00:05, 42.3MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  54%|█████▍    | 262M/482M [00:06<00:05, 42.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  55%|█████▍    | 262M/481M [00:06<00:05, 42.3MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  57%|█████▋    | 273M/482M [00:06<00:04, 41.8MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  57%|█████▋    | 273M/481M [00:06<00:04, 41.9MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  59%|█████▉    | 283M/482M [00:06<00:04, 42.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  59%|█████▉    | 283M/481M [00:06<00:04, 42.2MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  61%|██████    | 294M/482M [00:06<00:04, 42.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  61%|██████    | 294M/481M [00:06<00:04, 42.3MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  63%|██████▎   | 304M/482M [00:07<00:04, 42.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  63%|██████▎   | 304M/481M [00:07<00:04, 42.2MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  65%|██████▌   | 315M/482M [00:07<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  65%|██████▌   | 315M/481M [00:07<00:03, 42.3MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  67%|██████▋   | 325M/482M [00:07<00:03, 41.9MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  68%|██████▊   | 325M/481M [00:07<00:03, 39.5MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  70%|██████▉   | 336M/482M [00:07<00:03, 41.7MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  70%|██████▉   | 336M/481M [00:08<00:03, 38.6MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  72%|███████▏  | 346M/482M [00:08<00:03, 42.1MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  72%|███████▏  | 346M/481M [00:08<00:03, 39.6MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  74%|███████▍  | 357M/482M [00:08<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  74%|███████▍  | 357M/481M [00:08<00:03, 40.4MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  76%|███████▌  | 367M/482M [00:08<00:02, 41.8MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  76%|███████▋  | 367M/481M [00:08<00:02, 41.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  78%|███████▊  | 377M/482M [00:08<00:02, 41.7MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  79%|███████▊  | 377M/481M [00:09<00:02, 40.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  81%|████████  | 388M/482M [00:09<00:02, 41.9MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  81%|████████  | 388M/481M [00:09<00:02, 41.0MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  83%|████████▎ | 398M/482M [00:09<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  83%|████████▎ | 398M/481M [00:09<00:02, 41.2MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  85%|████████▍ | 409M/482M [00:09<00:01, 39.2MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  85%|████████▌ | 409M/481M [00:09<00:01, 41.5MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  87%|████████▋ | 419M/482M [00:10<00:01, 39.7MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  87%|████████▋ | 419M/481M [00:10<00:01, 41.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  89%|████████▉ | 430M/482M [00:10<00:01, 40.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  89%|████████▉ | 430M/481M [00:10<00:01, 41.8MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  91%|█████████▏| 440M/482M [00:10<00:01, 40.8MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  92%|█████████▏| 440M/481M [00:10<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  94%|█████████▎| 451M/482M [00:10<00:00, 41.3MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  94%|█████████▍| 451M/481M [00:10<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  96%|█████████▌| 461M/482M [00:11<00:00, 41.2MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  96%|█████████▌| 461M/481M [00:11<00:00, 42.5MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet:  98%|█████████▊| 472M/482M [00:11<00:00, 41.4MB/s]\u001b[A\r\n",
      "train-00003-of-00013.parquet:  98%|█████████▊| 472M/481M [00:11<00:00, 42.5MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet: 100%|██████████| 482M/482M [00:11<00:00, 41.9MB/s]\u001b[A\r\n",
      "train-00010-of-00013.parquet: 100%|██████████| 482M/482M [00:11<00:00, 41.8MB/s]\r\n",
      "train-00003-of-00013.parquet: 100%|██████████| 481M/481M [00:11<00:00, 41.7MB/s]\r\n",
      "\r\n",
      "train-00004-of-00013.parquet:   0%|                  | 0.00/480M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:   0%|                  | 0.00/480M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:   2%|▏        | 10.5M/480M [00:00<00:11, 40.9MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:   2%|▏        | 10.5M/480M [00:00<00:11, 39.4MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:   4%|▍        | 21.0M/480M [00:00<00:10, 42.0MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:   4%|▍        | 21.0M/480M [00:00<00:11, 41.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:   7%|▌        | 31.5M/480M [00:00<00:10, 42.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:   7%|▌        | 31.5M/480M [00:00<00:10, 42.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:   9%|▊        | 41.9M/480M [00:00<00:10, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:   9%|▊        | 41.9M/480M [00:01<00:10, 41.4MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  11%|▉        | 52.4M/480M [00:01<00:10, 42.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  11%|▉        | 52.4M/480M [00:01<00:10, 42.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  13%|█▏       | 62.9M/480M [00:01<00:09, 42.5MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  13%|█▏       | 62.9M/480M [00:01<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  15%|█▍       | 73.4M/480M [00:01<00:09, 42.5MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  15%|█▍       | 73.4M/480M [00:01<00:09, 41.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  17%|█▌       | 83.9M/480M [00:01<00:09, 42.8MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  17%|█▌       | 83.9M/480M [00:02<00:09, 41.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  20%|█▊       | 94.4M/480M [00:02<00:09, 42.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  20%|█▊       | 94.4M/480M [00:02<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  22%|██▏       | 105M/480M [00:02<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  22%|██▏       | 105M/480M [00:02<00:08, 41.8MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  24%|██▍       | 115M/480M [00:02<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  24%|██▍       | 115M/480M [00:02<00:08, 41.8MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  26%|██▌       | 126M/480M [00:02<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  26%|██▌       | 126M/480M [00:03<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  28%|██▊       | 136M/480M [00:03<00:08, 42.2MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  28%|██▊       | 136M/480M [00:03<00:08, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  31%|███       | 147M/480M [00:03<00:07, 42.0MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  31%|███       | 147M/480M [00:03<00:07, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  33%|███▎      | 157M/480M [00:03<00:07, 42.2MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  33%|███▎      | 157M/480M [00:03<00:07, 41.7MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  35%|███▍      | 168M/480M [00:03<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  35%|███▍      | 168M/480M [00:04<00:07, 41.5MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  37%|███▋      | 178M/480M [00:04<00:07, 42.3MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  37%|███▋      | 178M/480M [00:04<00:08, 35.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  39%|███▉      | 189M/480M [00:04<00:06, 42.3MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  39%|███▉      | 189M/480M [00:04<00:07, 37.6MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  42%|████▏     | 199M/480M [00:04<00:06, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  41%|████▏     | 199M/480M [00:04<00:07, 39.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  44%|████▎     | 210M/480M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  46%|████▌     | 220M/480M [00:05<00:06, 42.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  44%|████▎     | 210M/480M [00:05<00:07, 37.8MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  48%|████▊     | 231M/480M [00:05<00:05, 42.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  46%|████▌     | 220M/480M [00:05<00:06, 37.6MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  50%|█████     | 241M/480M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  48%|████▊     | 231M/480M [00:05<00:06, 38.5MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  52%|█████▏    | 252M/480M [00:05<00:05, 42.3MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  50%|█████     | 241M/480M [00:05<00:06, 39.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  52%|█████▏    | 252M/480M [00:06<00:05, 39.2MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  55%|█████▍    | 262M/480M [00:06<00:05, 38.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  57%|█████▋    | 273M/480M [00:06<00:04, 43.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  55%|█████▍    | 262M/480M [00:06<00:05, 40.2MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  59%|█████▉    | 283M/480M [00:06<00:04, 43.2MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  57%|█████▋    | 273M/480M [00:06<00:05, 39.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  61%|██████    | 294M/480M [00:06<00:04, 43.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  59%|█████▉    | 283M/480M [00:07<00:04, 40.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  63%|██████▎   | 304M/480M [00:07<00:04, 42.8MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  61%|██████    | 294M/480M [00:07<00:04, 40.7MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  66%|██████▌   | 315M/480M [00:07<00:03, 42.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  63%|██████▎   | 304M/480M [00:07<00:04, 40.4MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  68%|██████▊   | 325M/480M [00:07<00:03, 42.5MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  65%|██████▌   | 315M/480M [00:07<00:04, 40.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  70%|██████▉   | 336M/480M [00:07<00:03, 42.3MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  68%|██████▊   | 325M/480M [00:08<00:03, 41.4MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  72%|███████▏  | 346M/480M [00:08<00:03, 42.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  70%|██████▉   | 336M/480M [00:08<00:03, 41.7MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  74%|███████▍  | 357M/480M [00:08<00:02, 42.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  72%|███████▏  | 346M/480M [00:08<00:03, 41.7MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  77%|███████▋  | 367M/480M [00:08<00:02, 42.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  74%|███████▍  | 357M/480M [00:08<00:02, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  79%|███████▊  | 377M/480M [00:08<00:02, 42.9MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  76%|███████▋  | 367M/480M [00:09<00:02, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  81%|████████  | 388M/480M [00:09<00:02, 42.7MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  79%|███████▊  | 377M/480M [00:09<00:02, 42.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  83%|████████▎ | 398M/480M [00:09<00:01, 42.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  81%|████████  | 388M/480M [00:09<00:02, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  85%|████████▌ | 409M/480M [00:09<00:01, 42.6MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  83%|████████▎ | 398M/480M [00:09<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  87%|████████▋ | 419M/480M [00:09<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  85%|████████▌ | 409M/480M [00:10<00:01, 41.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  90%|████████▉ | 430M/480M [00:10<00:01, 42.3MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  87%|████████▋ | 419M/480M [00:10<00:01, 41.9MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  92%|█████████▏| 440M/480M [00:10<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  90%|████████▉ | 430M/480M [00:10<00:01, 40.8MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  94%|█████████▍| 451M/480M [00:10<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  92%|█████████▏| 440M/480M [00:10<00:00, 41.1MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  96%|█████████▌| 461M/480M [00:10<00:00, 41.9MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  94%|█████████▍| 451M/480M [00:11<00:00, 41.6MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet:  98%|█████████▊| 472M/480M [00:11<00:00, 42.1MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  96%|█████████▌| 461M/480M [00:11<00:00, 42.0MB/s]\u001b[A\r\n",
      "train-00004-of-00013.parquet: 100%|██████████| 480M/480M [00:11<00:00, 42.3MB/s]\r\n",
      "\r\n",
      "train-00005-of-00013.parquet:   0%|                  | 0.00/482M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet:  98%|█████████▊| 472M/480M [00:11<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:   2%|▏        | 10.5M/482M [00:00<00:11, 41.4MB/s]\u001b[A\r\n",
      "train-00011-of-00013.parquet: 100%|██████████| 480M/480M [00:11<00:00, 40.9MB/s]\r\n",
      "\r\n",
      "train-00012-of-00013.parquet:   0%|                  | 0.00/479M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:   4%|▍        | 21.0M/482M [00:00<00:11, 40.6MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:   2%|▏        | 10.5M/479M [00:00<00:11, 41.5MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:   7%|▌        | 31.5M/482M [00:00<00:10, 41.7MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:   4%|▍        | 21.0M/479M [00:00<00:11, 41.3MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:   9%|▊        | 41.9M/482M [00:01<00:10, 41.8MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:   7%|▌        | 31.5M/479M [00:00<00:10, 42.6MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  11%|▉        | 52.4M/482M [00:01<00:10, 41.9MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:   9%|▊        | 41.9M/479M [00:00<00:10, 42.5MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  13%|█▏       | 62.9M/482M [00:01<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  11%|▉        | 52.4M/479M [00:01<00:10, 42.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  15%|█▎       | 73.4M/482M [00:01<00:09, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  13%|█▏       | 62.9M/479M [00:01<00:09, 42.4MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  17%|█▌       | 83.9M/482M [00:02<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  15%|█▍       | 73.4M/479M [00:01<00:09, 41.9MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  20%|█▊       | 94.4M/482M [00:02<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  18%|█▌       | 83.9M/479M [00:01<00:09, 42.0MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  22%|██▏       | 105M/482M [00:02<00:09, 41.7MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  20%|█▊       | 94.4M/479M [00:02<00:09, 41.4MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  24%|██▍       | 115M/482M [00:02<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  22%|██▏       | 105M/479M [00:02<00:08, 41.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  26%|██▌       | 126M/482M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  24%|██▍       | 115M/479M [00:02<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  28%|██▊       | 136M/482M [00:03<00:08, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  26%|██▋       | 126M/479M [00:02<00:08, 41.9MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  30%|███       | 147M/482M [00:03<00:07, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  28%|██▊       | 136M/479M [00:03<00:08, 42.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  33%|███▎      | 157M/482M [00:03<00:07, 42.6MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  31%|███       | 147M/479M [00:03<00:07, 42.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  35%|███▍      | 168M/482M [00:03<00:07, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  33%|███▎      | 157M/479M [00:03<00:07, 41.8MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  37%|███▋      | 178M/482M [00:04<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  35%|███▌      | 168M/479M [00:03<00:07, 42.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  39%|███▉      | 189M/482M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  37%|███▋      | 178M/479M [00:04<00:07, 42.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  41%|████▏     | 199M/482M [00:04<00:06, 42.3MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  39%|███▉      | 189M/479M [00:04<00:06, 41.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  43%|████▎     | 210M/482M [00:04<00:06, 42.3MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  42%|████▏     | 199M/479M [00:04<00:06, 41.8MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  46%|████▌     | 220M/482M [00:05<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  44%|████▍     | 210M/479M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  48%|████▊     | 231M/482M [00:05<00:05, 42.1MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  46%|████▌     | 220M/479M [00:05<00:06, 41.8MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  50%|█████     | 241M/482M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  48%|████▊     | 231M/479M [00:05<00:05, 42.0MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  52%|█████▏    | 252M/482M [00:05<00:05, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  50%|█████     | 241M/479M [00:05<00:05, 42.0MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  54%|█████▍    | 262M/482M [00:06<00:05, 42.4MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  53%|█████▎    | 252M/479M [00:06<00:05, 41.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  57%|█████▋    | 273M/482M [00:06<00:04, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  55%|█████▍    | 262M/479M [00:06<00:05, 41.5MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  59%|█████▊    | 283M/482M [00:06<00:04, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  57%|█████▋    | 273M/479M [00:06<00:04, 41.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  61%|██████    | 294M/482M [00:06<00:04, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  59%|█████▉    | 283M/479M [00:06<00:04, 41.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  63%|██████▎   | 304M/482M [00:07<00:04, 42.1MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  61%|██████▏   | 294M/479M [00:07<00:04, 41.9MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  65%|██████▌   | 315M/482M [00:07<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  63%|██████▎   | 304M/479M [00:07<00:04, 42.0MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  67%|██████▋   | 325M/482M [00:07<00:03, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  66%|██████▌   | 315M/479M [00:07<00:03, 42.0MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  70%|██████▉   | 336M/482M [00:07<00:03, 42.4MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  68%|██████▊   | 325M/479M [00:07<00:03, 40.5MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  72%|███████▏  | 346M/482M [00:08<00:03, 42.1MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  70%|███████   | 336M/479M [00:08<00:03, 41.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  74%|███████▍  | 357M/482M [00:08<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  76%|███████▌  | 367M/482M [00:08<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  72%|███████▏  | 346M/479M [00:08<00:03, 41.1MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  78%|███████▊  | 377M/482M [00:08<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  74%|███████▍  | 357M/479M [00:08<00:02, 41.4MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  80%|████████  | 388M/482M [00:09<00:02, 42.3MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  77%|███████▋  | 367M/479M [00:08<00:02, 41.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  83%|████████▎ | 398M/482M [00:09<00:01, 42.1MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  79%|███████▉  | 377M/479M [00:09<00:02, 41.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  85%|████████▍ | 409M/482M [00:09<00:01, 42.1MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  81%|████████  | 388M/479M [00:09<00:02, 39.3MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  83%|████████▎ | 398M/479M [00:09<00:01, 43.0MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  87%|████████▋ | 419M/482M [00:09<00:01, 42.3MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  85%|████████▌ | 409M/479M [00:09<00:01, 42.7MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  89%|████████▉ | 430M/482M [00:10<00:01, 35.0MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  88%|████████▊ | 419M/479M [00:10<00:01, 42.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  90%|████████▉ | 430M/479M [00:10<00:01, 42.4MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  94%|█████████▎| 451M/482M [00:10<00:00, 43.5MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  92%|█████████▏| 440M/479M [00:10<00:00, 42.3MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  96%|█████████▌| 461M/482M [00:10<00:00, 43.1MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  94%|█████████▍| 451M/479M [00:10<00:00, 42.4MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet:  98%|█████████▊| 472M/482M [00:11<00:00, 42.9MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  96%|█████████▋| 461M/479M [00:11<00:00, 42.2MB/s]\u001b[A\r\n",
      "train-00005-of-00013.parquet: 100%|██████████| 482M/482M [00:11<00:00, 42.1MB/s]\r\n",
      "\r\n",
      "train-00006-of-00013.parquet:   0%|                  | 0.00/481M [00:00<?, ?B/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet:  98%|█████████▊| 472M/479M [00:11<00:00, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:   2%|▏        | 10.5M/481M [00:00<00:11, 41.2MB/s]\u001b[A\r\n",
      "train-00012-of-00013.parquet: 100%|██████████| 479M/479M [00:11<00:00, 41.9MB/s]\r\n",
      "\r\n",
      "train-00006-of-00013.parquet:   4%|▍        | 21.0M/481M [00:00<00:10, 41.9MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:   7%|▌        | 31.5M/481M [00:00<00:10, 42.9MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:   9%|▊        | 41.9M/481M [00:00<00:10, 42.7MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  11%|▉        | 52.4M/481M [00:01<00:10, 42.8MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  13%|█▏       | 62.9M/481M [00:01<00:09, 42.8MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  15%|█▎       | 73.4M/481M [00:01<00:09, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  17%|█▌       | 83.9M/481M [00:01<00:09, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  20%|█▊       | 94.4M/481M [00:02<00:09, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  22%|██▏       | 105M/481M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  24%|██▍       | 115M/481M [00:02<00:08, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  26%|██▌       | 126M/481M [00:02<00:08, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  28%|██▊       | 136M/481M [00:03<00:08, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  31%|███       | 147M/481M [00:03<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  33%|███▎      | 157M/481M [00:03<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  35%|███▍      | 168M/481M [00:03<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  37%|███▋      | 178M/481M [00:04<00:07, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  39%|███▉      | 189M/481M [00:04<00:06, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  41%|████▏     | 199M/481M [00:04<00:06, 42.2MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  44%|████▎     | 210M/481M [00:04<00:06, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  46%|████▌     | 220M/481M [00:05<00:06, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  48%|████▊     | 231M/481M [00:05<00:05, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  50%|█████     | 241M/481M [00:05<00:05, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  52%|█████▏    | 252M/481M [00:05<00:05, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  55%|█████▍    | 262M/481M [00:06<00:05, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  57%|█████▋    | 273M/481M [00:06<00:04, 42.6MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  59%|█████▉    | 283M/481M [00:06<00:04, 42.5MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  61%|██████    | 294M/481M [00:06<00:04, 42.2MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  63%|██████▎   | 304M/481M [00:07<00:04, 42.2MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  65%|██████▌   | 315M/481M [00:07<00:03, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  68%|██████▊   | 325M/481M [00:07<00:03, 42.1MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  70%|██████▉   | 336M/481M [00:07<00:03, 41.7MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  72%|███████▏  | 346M/481M [00:08<00:03, 41.7MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  74%|███████▍  | 357M/481M [00:08<00:02, 41.8MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  76%|███████▋  | 367M/481M [00:08<00:02, 42.1MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  79%|███████▊  | 377M/481M [00:08<00:02, 42.0MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  81%|████████  | 388M/481M [00:09<00:02, 42.2MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  83%|████████▎ | 398M/481M [00:09<00:01, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  85%|████████▌ | 409M/481M [00:09<00:01, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  87%|████████▋ | 419M/481M [00:09<00:01, 42.2MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  89%|████████▉ | 430M/481M [00:10<00:01, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  92%|█████████▏| 440M/481M [00:10<00:00, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  94%|█████████▍| 451M/481M [00:10<00:00, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  96%|█████████▌| 461M/481M [00:10<00:00, 42.3MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet:  98%|█████████▊| 472M/481M [00:11<00:00, 42.4MB/s]\u001b[A\r\n",
      "train-00006-of-00013.parquet: 100%|██████████| 481M/481M [00:11<00:00, 42.3MB/s]\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\r\n",
      "test-00002-of-00004.parquet:   0%|                   | 0.00/390M [00:00<?, ?B/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:   0%|                   | 0.00/388M [00:00<?, ?B/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:   3%|▎         | 10.5M/390M [00:00<00:09, 38.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:   3%|▎         | 10.5M/388M [00:00<00:09, 37.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:   5%|▌         | 21.0M/390M [00:00<00:09, 40.7MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:   5%|▌         | 21.0M/388M [00:00<00:08, 41.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:   8%|▊         | 31.5M/390M [00:00<00:08, 41.7MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:   8%|▊         | 31.5M/388M [00:00<00:08, 41.4MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  11%|█         | 41.9M/390M [00:01<00:08, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  11%|█         | 41.9M/388M [00:01<00:08, 42.4MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  13%|█▎        | 52.4M/390M [00:01<00:07, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  14%|█▎        | 52.4M/388M [00:01<00:07, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  16%|█▌        | 62.9M/390M [00:01<00:07, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  16%|█▌        | 62.9M/388M [00:01<00:07, 42.4MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  19%|█▉        | 73.4M/390M [00:01<00:07, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  19%|█▉        | 73.4M/388M [00:01<00:07, 42.3MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  22%|██▏       | 83.9M/390M [00:02<00:07, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  22%|██▏       | 83.9M/388M [00:02<00:07, 42.0MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  24%|██▍       | 94.4M/390M [00:02<00:07, 42.1MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  24%|██▍       | 94.4M/388M [00:02<00:06, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  27%|██▉        | 105M/390M [00:02<00:06, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  27%|██▉        | 105M/388M [00:02<00:06, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  30%|███▎       | 115M/390M [00:02<00:06, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  30%|███▎       | 115M/388M [00:02<00:06, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  32%|███▌       | 126M/390M [00:02<00:06, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  32%|███▌       | 126M/388M [00:03<00:06, 42.0MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  35%|███▊       | 136M/390M [00:03<00:05, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  35%|███▊       | 136M/388M [00:03<00:05, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  38%|████▏      | 147M/390M [00:03<00:05, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  38%|████▏      | 147M/388M [00:03<00:05, 41.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  40%|████▍      | 157M/390M [00:03<00:05, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  41%|████▍      | 157M/388M [00:03<00:05, 41.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  43%|████▋      | 168M/390M [00:03<00:05, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  43%|████▊      | 168M/388M [00:04<00:05, 41.6MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  46%|█████      | 178M/390M [00:04<00:04, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  46%|█████      | 178M/388M [00:04<00:04, 41.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  48%|█████▎     | 189M/390M [00:04<00:04, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  49%|█████▎     | 189M/388M [00:04<00:04, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  51%|█████▌     | 199M/390M [00:04<00:04, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  51%|█████▋     | 199M/388M [00:04<00:04, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  54%|█████▉     | 210M/390M [00:04<00:04, 42.5MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  54%|█████▉     | 210M/388M [00:04<00:04, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  56%|██████▏    | 220M/390M [00:05<00:04, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  57%|██████▏    | 220M/388M [00:05<00:04, 41.4MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  59%|██████▌    | 231M/390M [00:05<00:03, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  59%|██████▌    | 231M/388M [00:05<00:03, 41.8MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  62%|██████▊    | 241M/390M [00:05<00:03, 42.5MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  62%|██████▊    | 241M/388M [00:05<00:03, 41.5MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  65%|███████    | 252M/390M [00:05<00:03, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  65%|███████▏   | 252M/388M [00:06<00:03, 41.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  67%|███████▍   | 262M/390M [00:06<00:03, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  68%|███████▍   | 262M/388M [00:06<00:03, 41.8MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  70%|███████▋   | 273M/390M [00:06<00:02, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  70%|███████▋   | 273M/388M [00:06<00:02, 41.8MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  73%|███████▉   | 283M/390M [00:06<00:02, 42.5MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  73%|████████   | 283M/388M [00:06<00:02, 41.8MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  75%|████████▎  | 294M/390M [00:06<00:02, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  76%|████████▎  | 294M/388M [00:07<00:02, 41.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  78%|████████▌  | 304M/390M [00:07<00:02, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  78%|████████▋  | 304M/388M [00:07<00:01, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  81%|████████▊  | 315M/390M [00:07<00:01, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  81%|████████▉  | 315M/388M [00:07<00:01, 42.4MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  83%|█████████▏ | 325M/390M [00:07<00:01, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  84%|█████████▏ | 325M/388M [00:07<00:01, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  86%|█████████▍ | 336M/390M [00:07<00:01, 42.0MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  87%|█████████▌ | 336M/388M [00:08<00:01, 41.9MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  89%|█████████▊ | 346M/390M [00:08<00:01, 42.3MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  89%|█████████▊ | 346M/388M [00:08<00:00, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  91%|██████████ | 357M/390M [00:08<00:00, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  92%|██████████ | 357M/388M [00:08<00:00, 42.1MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  94%|██████████▎| 367M/390M [00:08<00:00, 42.2MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  95%|██████████▍| 367M/388M [00:08<00:00, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet:  97%|██████████▋| 377M/390M [00:08<00:00, 42.4MB/s]\u001b[A\r\n",
      "test-00000-of-00004.parquet:  97%|██████████▋| 377M/388M [00:08<00:00, 42.2MB/s]\u001b[A\r\n",
      "test-00002-of-00004.parquet: 100%|███████████| 390M/390M [00:09<00:00, 42.2MB/s]\r\n",
      "\r\n",
      "test-00000-of-00004.parquet: 100%|███████████| 388M/388M [00:09<00:00, 41.9MB/s]\r\n",
      "\r\n",
      "test-00003-of-00004.parquet:   0%|                   | 0.00/390M [00:00<?, ?B/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:   0%|                   | 0.00/390M [00:00<?, ?B/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:   3%|▎         | 10.5M/390M [00:00<00:09, 41.6MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:   5%|▌         | 21.0M/390M [00:00<00:08, 42.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:   3%|▎         | 10.5M/390M [00:00<00:09, 40.2MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:   8%|▊         | 31.5M/390M [00:00<00:08, 43.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:   5%|▌         | 21.0M/390M [00:00<00:08, 41.5MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  11%|█         | 41.9M/390M [00:00<00:08, 42.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:   8%|▊         | 31.5M/390M [00:00<00:08, 42.4MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  13%|█▎        | 52.4M/390M [00:01<00:07, 42.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  11%|█         | 41.9M/390M [00:00<00:08, 42.2MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  16%|█▌        | 62.9M/390M [00:01<00:07, 42.6MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  13%|█▎        | 52.4M/390M [00:01<00:08, 41.9MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  19%|█▉        | 73.4M/390M [00:01<00:07, 42.5MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  16%|█▌        | 62.9M/390M [00:01<00:07, 42.2MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  22%|██▏       | 83.9M/390M [00:01<00:07, 42.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  19%|█▉        | 73.4M/390M [00:01<00:07, 41.9MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  24%|██▍       | 94.4M/390M [00:02<00:07, 42.1MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  21%|██▏       | 83.9M/390M [00:02<00:07, 41.3MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  27%|██▉        | 105M/390M [00:02<00:06, 42.1MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  24%|██▍       | 94.4M/390M [00:02<00:07, 41.7MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  30%|███▎       | 115M/390M [00:02<00:06, 41.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  27%|██▉        | 105M/390M [00:02<00:06, 41.7MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  32%|███▌       | 126M/390M [00:02<00:06, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  30%|███▎       | 115M/390M [00:02<00:06, 41.6MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  35%|███▊       | 136M/390M [00:03<00:06, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  32%|███▌       | 126M/390M [00:03<00:06, 41.8MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  38%|████▏      | 147M/390M [00:03<00:05, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  35%|███▊       | 136M/390M [00:03<00:06, 42.0MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  40%|████▍      | 157M/390M [00:03<00:05, 42.3MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  38%|████▏      | 147M/390M [00:03<00:05, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  40%|████▍      | 157M/390M [00:03<00:05, 41.9MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  43%|████▋      | 168M/390M [00:04<00:06, 36.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  43%|████▋      | 168M/390M [00:04<00:05, 42.1MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  46%|█████      | 178M/390M [00:04<00:05, 37.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  46%|█████      | 178M/390M [00:04<00:05, 42.1MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  48%|█████▎     | 189M/390M [00:04<00:05, 39.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  48%|█████▎     | 189M/390M [00:04<00:04, 42.0MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  51%|█████▋     | 199M/390M [00:04<00:04, 39.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  51%|█████▌     | 199M/390M [00:04<00:04, 42.1MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  54%|█████▉     | 210M/390M [00:05<00:04, 40.7MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  54%|█████▉     | 210M/390M [00:05<00:04, 42.0MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  57%|██████▏    | 220M/390M [00:05<00:04, 41.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  56%|██████▏    | 220M/390M [00:05<00:04, 41.8MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  59%|██████▌    | 231M/390M [00:05<00:03, 41.5MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  59%|██████▌    | 231M/390M [00:05<00:03, 41.7MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  62%|██████▊    | 241M/390M [00:05<00:03, 41.7MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  65%|███████    | 252M/390M [00:06<00:03, 41.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  62%|██████▊    | 241M/390M [00:05<00:04, 35.9MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  67%|███████▍   | 262M/390M [00:06<00:03, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  64%|███████    | 252M/390M [00:06<00:03, 37.6MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  70%|███████▋   | 273M/390M [00:06<00:02, 42.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  67%|███████▍   | 262M/390M [00:06<00:03, 39.0MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  73%|███████▉   | 283M/390M [00:06<00:02, 41.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  70%|███████▋   | 273M/390M [00:06<00:02, 39.9MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  75%|████████▎  | 294M/390M [00:07<00:02, 42.1MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  73%|███████▉   | 283M/390M [00:06<00:02, 40.7MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  78%|████████▌  | 304M/390M [00:07<00:02, 42.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  75%|████████▎  | 294M/390M [00:07<00:02, 41.0MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  81%|████████▉  | 315M/390M [00:07<00:01, 42.3MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  78%|████████▌  | 304M/390M [00:07<00:02, 41.3MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  83%|█████████▏ | 325M/390M [00:07<00:01, 42.3MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  81%|████████▊  | 315M/390M [00:07<00:01, 41.8MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  86%|█████████▍ | 336M/390M [00:08<00:01, 42.1MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  83%|█████████▏ | 325M/390M [00:07<00:01, 41.9MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  89%|█████████▊ | 346M/390M [00:08<00:01, 42.2MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  86%|█████████▍ | 336M/390M [00:08<00:01, 42.2MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  92%|██████████ | 357M/390M [00:08<00:00, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  89%|█████████▊ | 346M/390M [00:08<00:01, 42.3MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  94%|██████████▎| 367M/390M [00:08<00:00, 41.9MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  91%|██████████ | 357M/390M [00:08<00:00, 42.3MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet:  97%|██████████▋| 377M/390M [00:09<00:00, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet:  94%|██████████▎| 367M/390M [00:08<00:00, 42.3MB/s]\u001b[A\r\n",
      "test-00003-of-00004.parquet: 100%|███████████| 390M/390M [00:09<00:00, 41.6MB/s]\r\n",
      "\r\n",
      "test-00001-of-00004.parquet:  97%|██████████▋| 377M/390M [00:09<00:00, 42.0MB/s]\u001b[A\r\n",
      "test-00001-of-00004.parquet: 100%|███████████| 390M/390M [00:09<00:00, 41.4MB/s]\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\r\n",
      "Generating train split:   0%|                  | 0/20440 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\r\n",
      "Generating train split:   0%|       | 100/20440 [00:00<00:57, 352.64 examples/s]\u001b[A\r\n",
      "Generating train split:   1%|       | 300/20440 [00:00<00:27, 740.56 examples/s]\u001b[A\r\n",
      "Generating train split:   2%|▏      | 500/20440 [00:00<00:21, 918.65 examples/s]\u001b[A\r\n",
      "Generating train split:   3%|▏     | 700/20440 [00:00<00:19, 1005.49 examples/s]\u001b[A\r\n",
      "Generating train split:   4%|▎      | 900/20440 [00:01<00:20, 965.67 examples/s]\u001b[A\r\n",
      "Generating train split:   5%|▎     | 1100/20440 [00:01<00:19, 985.96 examples/s]\u001b[A\r\n",
      "Generating train split:   6%|▎    | 1300/20440 [00:01<00:18, 1035.54 examples/s]\u001b[A\r\n",
      "Generating train split:   7%|▍     | 1500/20440 [00:01<00:19, 980.11 examples/s]\u001b[A\r\n",
      "Generating train split:   8%|▍     | 1700/20440 [00:01<00:18, 990.75 examples/s]\u001b[A\r\n",
      "Generating train split:   9%|▍    | 1900/20440 [00:01<00:18, 1009.16 examples/s]\u001b[A\r\n",
      "Generating train split:  10%|▌     | 2100/20440 [00:02<00:18, 990.80 examples/s]\u001b[A\r\n",
      "Generating train split:  11%|▋     | 2300/20440 [00:02<00:18, 957.19 examples/s]\u001b[A\r\n",
      "Generating train split:  12%|▌    | 2500/20440 [00:02<00:17, 1025.48 examples/s]\u001b[A\r\n",
      "Generating train split:  13%|▋    | 2700/20440 [00:02<00:16, 1104.44 examples/s]\u001b[A\r\n",
      "Generating train split:  14%|▋    | 2900/20440 [00:02<00:16, 1094.78 examples/s]\u001b[A\r\n",
      "Generating train split:  15%|▊    | 3073/20440 [00:03<00:15, 1127.14 examples/s]\u001b[A\r\n",
      "Generating train split:  16%|▊    | 3245/20440 [00:03<00:15, 1090.78 examples/s]\u001b[A\r\n",
      "Generating train split:  17%|▊    | 3445/20440 [00:03<00:15, 1114.54 examples/s]\u001b[A\r\n",
      "Generating train split:  18%|▉    | 3645/20440 [00:03<00:15, 1083.30 examples/s]\u001b[A\r\n",
      "Generating train split:  19%|▉    | 3845/20440 [00:03<00:15, 1099.97 examples/s]\u001b[A\r\n",
      "Generating train split:  20%|▉    | 4045/20440 [00:03<00:14, 1133.91 examples/s]\u001b[A\r\n",
      "Generating train split:  21%|█    | 4245/20440 [00:04<00:14, 1083.37 examples/s]\u001b[A\r\n",
      "Generating train split:  22%|█    | 4445/20440 [00:04<00:14, 1118.97 examples/s]\u001b[A\r\n",
      "Generating train split:  23%|█▏   | 4645/20440 [00:04<00:14, 1072.11 examples/s]\u001b[A\r\n",
      "Generating train split:  24%|█▏   | 4845/20440 [00:04<00:14, 1041.53 examples/s]\u001b[A\r\n",
      "Generating train split:  25%|█▏   | 5045/20440 [00:04<00:14, 1045.38 examples/s]\u001b[A\r\n",
      "Generating train split:  26%|█▎   | 5245/20440 [00:05<00:13, 1101.83 examples/s]\u001b[A\r\n",
      "Generating train split:  27%|█▎   | 5445/20440 [00:05<00:13, 1084.28 examples/s]\u001b[A\r\n",
      "Generating train split:  28%|█▍   | 5645/20440 [00:05<00:13, 1086.52 examples/s]\u001b[A\r\n",
      "Generating train split:  29%|█▍   | 5845/20440 [00:05<00:13, 1069.96 examples/s]\u001b[A\r\n",
      "Generating train split:  30%|█▍   | 6045/20440 [00:05<00:12, 1132.16 examples/s]\u001b[A\r\n",
      "Generating train split:  30%|█▌   | 6218/20440 [00:05<00:12, 1183.03 examples/s]\u001b[A\r\n",
      "Generating train split:  31%|█▌   | 6390/20440 [00:06<00:11, 1210.51 examples/s]\u001b[A\r\n",
      "Generating train split:  32%|█▌   | 6590/20440 [00:06<00:11, 1210.96 examples/s]\u001b[A\r\n",
      "Generating train split:  33%|█▋   | 6790/20440 [00:06<00:11, 1235.13 examples/s]\u001b[A\r\n",
      "Generating train split:  34%|█▋   | 6990/20440 [00:06<00:11, 1214.73 examples/s]\u001b[A\r\n",
      "Generating train split:  35%|█▊   | 7190/20440 [00:06<00:11, 1172.90 examples/s]\u001b[A\r\n",
      "Generating train split:  36%|█▊   | 7390/20440 [00:06<00:10, 1202.98 examples/s]\u001b[A\r\n",
      "Generating train split:  37%|█▊   | 7590/20440 [00:07<00:10, 1183.69 examples/s]\u001b[A\r\n",
      "Generating train split:  38%|█▉   | 7790/20440 [00:07<00:10, 1175.74 examples/s]\u001b[A\r\n",
      "Generating train split:  39%|█▉   | 7990/20440 [00:07<00:11, 1097.43 examples/s]\u001b[A\r\n",
      "Generating train split:  40%|██   | 8190/20440 [00:07<00:11, 1078.03 examples/s]\u001b[A\r\n",
      "Generating train split:  41%|██   | 8390/20440 [00:07<00:10, 1107.53 examples/s]\u001b[A\r\n",
      "Generating train split:  42%|██   | 8590/20440 [00:07<00:10, 1173.31 examples/s]\u001b[A\r\n",
      "Generating train split:  43%|██▏  | 8790/20440 [00:08<00:09, 1187.25 examples/s]\u001b[A\r\n",
      "Generating train split:  44%|██▏  | 8990/20440 [00:08<00:09, 1246.71 examples/s]\u001b[A\r\n",
      "Generating train split:  45%|██▏  | 9190/20440 [00:08<00:08, 1291.81 examples/s]\u001b[A\r\n",
      "Generating train split:  46%|██▎  | 9363/20440 [00:08<00:09, 1176.17 examples/s]\u001b[A\r\n",
      "Generating train split:  47%|██▎  | 9535/20440 [00:08<00:09, 1206.80 examples/s]\u001b[A\r\n",
      "Generating train split:  48%|██▍  | 9735/20440 [00:08<00:08, 1217.07 examples/s]\u001b[A\r\n",
      "Generating train split:  49%|██▍  | 9935/20440 [00:09<00:08, 1271.63 examples/s]\u001b[A\r\n",
      "Generating train split:  50%|█▉  | 10135/20440 [00:09<00:08, 1249.02 examples/s]\u001b[A\r\n",
      "Generating train split:  51%|██  | 10335/20440 [00:09<00:08, 1206.30 examples/s]\u001b[A\r\n",
      "Generating train split:  52%|██  | 10535/20440 [00:09<00:07, 1265.72 examples/s]\u001b[A\r\n",
      "Generating train split:  53%|██  | 10735/20440 [00:09<00:07, 1253.19 examples/s]\u001b[A\r\n",
      "Generating train split:  53%|██▏ | 10935/20440 [00:09<00:07, 1309.35 examples/s]\u001b[A\r\n",
      "Generating train split:  54%|██▏ | 11135/20440 [00:09<00:07, 1261.01 examples/s]\u001b[A\r\n",
      "Generating train split:  55%|██▏ | 11335/20440 [00:10<00:06, 1304.51 examples/s]\u001b[A\r\n",
      "Generating train split:  56%|██▎ | 11535/20440 [00:10<00:07, 1253.14 examples/s]\u001b[A\r\n",
      "Generating train split:  57%|██▎ | 11735/20440 [00:10<00:06, 1270.80 examples/s]\u001b[A\r\n",
      "Generating train split:  58%|██▎ | 11935/20440 [00:10<00:06, 1318.71 examples/s]\u001b[A\r\n",
      "Generating train split:  59%|██▎ | 12135/20440 [00:10<00:06, 1319.74 examples/s]\u001b[A\r\n",
      "Generating train split:  60%|██▍ | 12335/20440 [00:10<00:06, 1256.19 examples/s]\u001b[A\r\n",
      "Generating train split:  61%|██▍ | 12508/20440 [00:11<00:06, 1217.28 examples/s]\u001b[A\r\n",
      "Generating train split:  62%|██▍ | 12680/20440 [00:11<00:06, 1211.46 examples/s]\u001b[A\r\n",
      "Generating train split:  63%|██▌ | 12880/20440 [00:11<00:05, 1265.54 examples/s]\u001b[A\r\n",
      "Generating train split:  64%|██▌ | 13080/20440 [00:11<00:05, 1249.28 examples/s]\u001b[A\r\n",
      "Generating train split:  65%|██▌ | 13280/20440 [00:11<00:05, 1286.58 examples/s]\u001b[A\r\n",
      "Generating train split:  66%|██▋ | 13480/20440 [00:11<00:05, 1323.57 examples/s]\u001b[A\r\n",
      "Generating train split:  67%|██▋ | 13680/20440 [00:11<00:04, 1355.21 examples/s]\u001b[A\r\n",
      "Generating train split:  68%|██▋ | 13880/20440 [00:12<00:05, 1294.85 examples/s]\u001b[A\r\n",
      "Generating train split:  69%|██▊ | 14080/20440 [00:12<00:04, 1333.63 examples/s]\u001b[A\r\n",
      "Generating train split:  70%|██▊ | 14280/20440 [00:12<00:04, 1268.69 examples/s]\u001b[A\r\n",
      "Generating train split:  71%|██▊ | 14480/20440 [00:12<00:04, 1285.54 examples/s]\u001b[A\r\n",
      "Generating train split:  72%|██▊ | 14680/20440 [00:12<00:04, 1155.08 examples/s]\u001b[A\r\n",
      "Generating train split:  73%|██▉ | 14880/20440 [00:12<00:04, 1165.66 examples/s]\u001b[A\r\n",
      "Generating train split:  74%|██▉ | 15080/20440 [00:13<00:04, 1120.52 examples/s]\u001b[A\r\n",
      "Generating train split:  75%|██▉ | 15280/20440 [00:13<00:04, 1145.76 examples/s]\u001b[A\r\n",
      "Generating train split:  76%|███ | 15480/20440 [00:13<00:04, 1077.73 examples/s]\u001b[A\r\n",
      "Generating train split:  77%|███ | 15652/20440 [00:13<00:04, 1083.91 examples/s]\u001b[A\r\n",
      "Generating train split:  77%|███ | 15824/20440 [00:13<00:04, 1057.87 examples/s]\u001b[A\r\n",
      "Generating train split:  78%|███▏| 16024/20440 [00:14<00:03, 1148.23 examples/s]\u001b[A\r\n",
      "Generating train split:  79%|███▏| 16224/20440 [00:14<00:03, 1149.94 examples/s]\u001b[A\r\n",
      "Generating train split:  80%|███▏| 16424/20440 [00:14<00:03, 1117.98 examples/s]\u001b[A\r\n",
      "Generating train split:  81%|███▎| 16624/20440 [00:14<00:03, 1049.52 examples/s]\u001b[A\r\n",
      "Generating train split:  82%|███▎| 16824/20440 [00:14<00:03, 1112.38 examples/s]\u001b[A\r\n",
      "Generating train split:  83%|███▎| 17024/20440 [00:14<00:02, 1203.17 examples/s]\u001b[A\r\n",
      "Generating train split:  84%|███▎| 17224/20440 [00:15<00:02, 1275.12 examples/s]\u001b[A\r\n",
      "Generating train split:  85%|███▍| 17424/20440 [00:15<00:02, 1256.65 examples/s]\u001b[A\r\n",
      "Generating train split:  86%|███▍| 17624/20440 [00:15<00:02, 1308.88 examples/s]\u001b[A\r\n",
      "Generating train split:  87%|███▍| 17824/20440 [00:15<00:02, 1286.32 examples/s]\u001b[A\r\n",
      "Generating train split:  88%|███▌| 18024/20440 [00:15<00:01, 1334.02 examples/s]\u001b[A\r\n",
      "Generating train split:  89%|███▌| 18224/20440 [00:15<00:01, 1305.55 examples/s]\u001b[A\r\n",
      "Generating train split:  90%|███▌| 18424/20440 [00:15<00:01, 1350.95 examples/s]\u001b[A\r\n",
      "Generating train split:  91%|███▋| 18624/20440 [00:16<00:01, 1107.55 examples/s]\u001b[A\r\n",
      "Generating train split:  92%|███▋| 18796/20440 [00:16<00:01, 1049.95 examples/s]\u001b[A\r\n",
      "Generating train split:  93%|████▋| 18968/20440 [00:16<00:01, 971.90 examples/s]\u001b[A\r\n",
      "Generating train split:  94%|████▋| 19168/20440 [00:17<00:02, 582.13 examples/s]\u001b[A\r\n",
      "Generating train split:  94%|████▋| 19268/20440 [00:17<00:02, 425.29 examples/s]\u001b[A\r\n",
      "Generating train split:  95%|████▋| 19368/20440 [00:17<00:02, 444.14 examples/s]\u001b[A\r\n",
      "Generating train split:  95%|████▊| 19468/20440 [00:18<00:02, 479.84 examples/s]\u001b[A\r\n",
      "Generating train split:  96%|████▊| 19568/20440 [00:18<00:01, 498.45 examples/s]\u001b[A\r\n",
      "Generating train split:  96%|████▊| 19668/20440 [00:18<00:01, 531.16 examples/s]\u001b[A\r\n",
      "Generating train split:  97%|████▊| 19768/20440 [00:18<00:01, 533.56 examples/s]\u001b[A\r\n",
      "Generating train split:  97%|████▊| 19868/20440 [00:18<00:01, 557.97 examples/s]\u001b[A\r\n",
      "Generating train split:  98%|████▉| 19968/20440 [00:18<00:00, 563.45 examples/s]\u001b[A\r\n",
      "Generating train split:  98%|████▉| 20068/20440 [00:19<00:00, 580.08 examples/s]\u001b[A\r\n",
      "Generating train split:  99%|████▉| 20168/20440 [00:19<00:00, 569.95 examples/s]\u001b[A\r\n",
      "Generating train split:  99%|████▉| 20268/20440 [00:19<00:00, 598.68 examples/s]\u001b[A\r\n",
      "Generating train split: 100%|████▉| 20368/20440 [00:19<00:00, 589.76 examples/s]\u001b[A\r\n",
      "Generating train split: 100%|████| 20440/20440 [00:19<00:00, 1033.07 examples/s]\r\n",
      "\r\n",
      "Generating test split:   0%|                    | 0/5110 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Generating test split:   2%|▏        | 100/5110 [00:00<00:17, 289.52 examples/s]\u001b[A\r\n",
      "Generating test split:   6%|▌        | 300/5110 [00:00<00:08, 540.48 examples/s]\u001b[A\r\n",
      "Generating test split:  10%|▉        | 500/5110 [00:00<00:06, 689.56 examples/s]\u001b[A\r\n",
      "Generating test split:  14%|█▏       | 700/5110 [00:01<00:06, 711.83 examples/s]\u001b[A\r\n",
      "Generating test split:  18%|█▌       | 900/5110 [00:01<00:05, 764.92 examples/s]\u001b[A\r\n",
      "Generating test split:  22%|█▋      | 1100/5110 [00:01<00:04, 806.38 examples/s]\u001b[A\r\n",
      "Generating test split:  25%|██      | 1300/5110 [00:01<00:04, 843.89 examples/s]\u001b[A\r\n",
      "Generating test split:  29%|██▎     | 1500/5110 [00:01<00:04, 870.90 examples/s]\u001b[A\r\n",
      "Generating test split:  33%|██▋     | 1700/5110 [00:02<00:03, 964.01 examples/s]\u001b[A\r\n",
      "Generating test split:  37%|██▉     | 1900/5110 [00:02<00:03, 969.59 examples/s]\u001b[A\r\n",
      "Generating test split:  41%|██▉    | 2100/5110 [00:02<00:02, 1065.14 examples/s]\u001b[A\r\n",
      "Generating test split:  45%|███▏   | 2300/5110 [00:02<00:02, 1056.35 examples/s]\u001b[A\r\n",
      "Generating test split:  48%|███▍   | 2478/5110 [00:02<00:02, 1110.90 examples/s]\u001b[A\r\n",
      "Generating test split:  52%|███▋   | 2655/5110 [00:02<00:02, 1056.66 examples/s]\u001b[A\r\n",
      "Generating test split:  56%|███▉   | 2855/5110 [00:03<00:01, 1138.29 examples/s]\u001b[A\r\n",
      "Generating test split:  60%|████▏  | 3055/5110 [00:03<00:01, 1170.29 examples/s]\u001b[A\r\n",
      "Generating test split:  64%|████▍  | 3255/5110 [00:03<00:01, 1203.04 examples/s]\u001b[A\r\n",
      "Generating test split:  68%|████▋  | 3455/5110 [00:03<00:01, 1170.15 examples/s]\u001b[A\r\n",
      "Generating test split:  72%|█████  | 3655/5110 [00:03<00:01, 1218.00 examples/s]\u001b[A\r\n",
      "Generating test split:  75%|█████▎ | 3855/5110 [00:03<00:01, 1176.86 examples/s]\u001b[A\r\n",
      "Generating test split:  79%|█████▌ | 4055/5110 [00:04<00:00, 1236.92 examples/s]\u001b[A\r\n",
      "Generating test split:  83%|█████▊ | 4255/5110 [00:04<00:00, 1179.35 examples/s]\u001b[A\r\n",
      "Generating test split:  87%|██████ | 4455/5110 [00:04<00:00, 1220.29 examples/s]\u001b[A\r\n",
      "Generating test split:  91%|██████▍| 4655/5110 [00:04<00:00, 1181.16 examples/s]\u001b[A\r\n",
      "Generating test split:  95%|██████▋| 4855/5110 [00:04<00:00, 1246.76 examples/s]\u001b[A\r\n",
      "Generating test split: 100%|███████| 5110/5110 [00:04<00:00, 1024.58 examples/s]\r\n",
      "\r\n",
      "README.md: 100%|███████████████████████████| 1.09k/1.09k [00:00<00:00, 8.19MB/s]\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\r\n",
      "train-00000-of-00001.parquet:   0%|                 | 0.00/8.00M [00:00<?, ?B/s]\u001b[A\r\n",
      "test-00000-of-00001.parquet: 100%|█████████| 2.02M/2.02M [00:00<00:00, 31.5MB/s]\r\n",
      "\r\n",
      "train-00000-of-00001.parquet: 100%|████████| 8.00M/8.00M [00:00<00:00, 36.5MB/s]\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Setting num_proc from 2 back to 1 for the train split to disable multiprocessing as it only contains one shard.\r\n",
      "\r\n",
      "Generating train split:   0%|                  | 0/20440 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Generating train split: 100%|██| 20440/20440 [00:00<00:00, 180880.13 examples/s]\r\n",
      "Setting num_proc from 2 back to 1 for the test split to disable multiprocessing as it only contains one shard.\r\n",
      "\r\n",
      "Generating test split: 100%|█████| 5110/5110 [00:00<00:00, 207602.61 examples/s]\r\n",
      "Combining datasets...: 100%|█████████████████████| 1/1 [02:13<00:00, 133.80s/it]\r\n",
      "Combining datasets...: 100%|█████████████████████| 1/1 [02:17<00:00, 137.40s/it]\r\n",
      "Combining datasets...: 100%|██████████████████████| 1/1 [00:03<00:00,  3.27s/it]\r\n",
      "Combining datasets...: 100%|██████████████████████| 1/1 [00:01<00:00,  1.17s/it]\r\n",
      "config.json: 100%|█████████████████████████| 6.93k/6.93k [00:00<00:00, 37.3MB/s]\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/config.json\r\n",
      "Model config ParlerTTSConfig {\r\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/training-50K-mini-without-accents-3-mononode/\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"ParlerTTSForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"audio_encoder\": {\r\n",
      "    \"_attn_implementation_autoset\": false,\r\n",
      "    \"_name_or_path\": \"parler-tts/dac_44khZ_8kbps\",\r\n",
      "    \"add_cross_attention\": false,\r\n",
      "    \"architectures\": [\r\n",
      "      \"DACModel\"\r\n",
      "    ],\r\n",
      "    \"bad_words_ids\": null,\r\n",
      "    \"begin_suppress_tokens\": null,\r\n",
      "    \"bos_token_id\": null,\r\n",
      "    \"chunk_size_feed_forward\": 0,\r\n",
      "    \"codebook_size\": 1024,\r\n",
      "    \"cross_attention_hidden_size\": null,\r\n",
      "    \"decoder_start_token_id\": null,\r\n",
      "    \"diversity_penalty\": 0.0,\r\n",
      "    \"do_sample\": false,\r\n",
      "    \"early_stopping\": false,\r\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\r\n",
      "    \"eos_token_id\": null,\r\n",
      "    \"exponential_decay_length_penalty\": null,\r\n",
      "    \"finetuning_task\": null,\r\n",
      "    \"forced_bos_token_id\": null,\r\n",
      "    \"forced_eos_token_id\": null,\r\n",
      "    \"frame_rate\": 86,\r\n",
      "    \"id2label\": {\r\n",
      "      \"0\": \"LABEL_0\",\r\n",
      "      \"1\": \"LABEL_1\"\r\n",
      "    },\r\n",
      "    \"is_decoder\": false,\r\n",
      "    \"is_encoder_decoder\": false,\r\n",
      "    \"label2id\": {\r\n",
      "      \"LABEL_0\": 0,\r\n",
      "      \"LABEL_1\": 1\r\n",
      "    },\r\n",
      "    \"latent_dim\": 1024,\r\n",
      "    \"length_penalty\": 1.0,\r\n",
      "    \"max_length\": 20,\r\n",
      "    \"min_length\": 0,\r\n",
      "    \"model_bitrate\": 8,\r\n",
      "    \"model_type\": \"dac_on_the_hub\",\r\n",
      "    \"no_repeat_ngram_size\": 0,\r\n",
      "    \"num_beam_groups\": 1,\r\n",
      "    \"num_beams\": 1,\r\n",
      "    \"num_codebooks\": 9,\r\n",
      "    \"num_return_sequences\": 1,\r\n",
      "    \"output_attentions\": false,\r\n",
      "    \"output_hidden_states\": false,\r\n",
      "    \"output_scores\": false,\r\n",
      "    \"pad_token_id\": null,\r\n",
      "    \"prefix\": null,\r\n",
      "    \"problem_type\": null,\r\n",
      "    \"pruned_heads\": {},\r\n",
      "    \"remove_invalid_values\": false,\r\n",
      "    \"repetition_penalty\": 1.0,\r\n",
      "    \"return_dict\": true,\r\n",
      "    \"return_dict_in_generate\": false,\r\n",
      "    \"sampling_rate\": 44100,\r\n",
      "    \"sep_token_id\": null,\r\n",
      "    \"suppress_tokens\": null,\r\n",
      "    \"task_specific_params\": null,\r\n",
      "    \"temperature\": 1.0,\r\n",
      "    \"tf_legacy_loss\": false,\r\n",
      "    \"tie_encoder_decoder\": false,\r\n",
      "    \"tie_word_embeddings\": true,\r\n",
      "    \"tokenizer_class\": null,\r\n",
      "    \"top_k\": 50,\r\n",
      "    \"top_p\": 1.0,\r\n",
      "    \"torch_dtype\": \"float32\",\r\n",
      "    \"torchscript\": false,\r\n",
      "    \"typical_p\": 1.0,\r\n",
      "    \"use_bfloat16\": false\r\n",
      "  },\r\n",
      "  \"decoder\": {\r\n",
      "    \"_attn_implementation_autoset\": false,\r\n",
      "    \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini/decoder\",\r\n",
      "    \"activation_dropout\": 0.0,\r\n",
      "    \"activation_function\": \"gelu\",\r\n",
      "    \"add_cross_attention\": true,\r\n",
      "    \"architectures\": [\r\n",
      "      \"ParlerTTSForCausalLM\"\r\n",
      "    ],\r\n",
      "    \"attention_dropout\": 0.0,\r\n",
      "    \"bad_words_ids\": null,\r\n",
      "    \"begin_suppress_tokens\": null,\r\n",
      "    \"bos_token_id\": 1025,\r\n",
      "    \"chunk_size_feed_forward\": 0,\r\n",
      "    \"codebook_weights\": null,\r\n",
      "    \"cross_attention_hidden_size\": null,\r\n",
      "    \"cross_attention_implementation_strategy\": null,\r\n",
      "    \"decoder_start_token_id\": null,\r\n",
      "    \"diversity_penalty\": 0.0,\r\n",
      "    \"do_sample\": false,\r\n",
      "    \"dropout\": 0.1,\r\n",
      "    \"early_stopping\": false,\r\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\r\n",
      "    \"eos_token_id\": 1024,\r\n",
      "    \"exponential_decay_length_penalty\": null,\r\n",
      "    \"ffn_dim\": 4096,\r\n",
      "    \"finetuning_task\": null,\r\n",
      "    \"forced_bos_token_id\": null,\r\n",
      "    \"forced_eos_token_id\": null,\r\n",
      "    \"hidden_size\": 1024,\r\n",
      "    \"id2label\": {\r\n",
      "      \"0\": \"LABEL_0\",\r\n",
      "      \"1\": \"LABEL_1\"\r\n",
      "    },\r\n",
      "    \"initializer_factor\": 0.02,\r\n",
      "    \"is_decoder\": true,\r\n",
      "    \"is_encoder_decoder\": false,\r\n",
      "    \"label2id\": {\r\n",
      "      \"LABEL_0\": 0,\r\n",
      "      \"LABEL_1\": 1\r\n",
      "    },\r\n",
      "    \"layerdrop\": 0.0,\r\n",
      "    \"length_penalty\": 1.0,\r\n",
      "    \"max_length\": 20,\r\n",
      "    \"max_position_embeddings\": 4096,\r\n",
      "    \"min_length\": 0,\r\n",
      "    \"model_type\": \"parler_tts_decoder\",\r\n",
      "    \"no_repeat_ngram_size\": 0,\r\n",
      "    \"num_attention_heads\": 16,\r\n",
      "    \"num_beam_groups\": 1,\r\n",
      "    \"num_beams\": 1,\r\n",
      "    \"num_codebooks\": 9,\r\n",
      "    \"num_cross_attention_key_value_heads\": 16,\r\n",
      "    \"num_hidden_layers\": 24,\r\n",
      "    \"num_key_value_heads\": 16,\r\n",
      "    \"num_return_sequences\": 1,\r\n",
      "    \"output_attentions\": false,\r\n",
      "    \"output_hidden_states\": false,\r\n",
      "    \"output_scores\": false,\r\n",
      "    \"pad_token_id\": 1024,\r\n",
      "    \"prefix\": null,\r\n",
      "    \"problem_type\": null,\r\n",
      "    \"pruned_heads\": {},\r\n",
      "    \"remove_invalid_values\": false,\r\n",
      "    \"repetition_penalty\": 1.0,\r\n",
      "    \"return_dict\": true,\r\n",
      "    \"return_dict_in_generate\": false,\r\n",
      "    \"rope_embeddings\": false,\r\n",
      "    \"rope_theta\": 10000.0,\r\n",
      "    \"scale_embedding\": false,\r\n",
      "    \"sep_token_id\": null,\r\n",
      "    \"suppress_tokens\": null,\r\n",
      "    \"task_specific_params\": null,\r\n",
      "    \"temperature\": 1.0,\r\n",
      "    \"tf_legacy_loss\": false,\r\n",
      "    \"tie_encoder_decoder\": false,\r\n",
      "    \"tie_word_embeddings\": false,\r\n",
      "    \"tokenizer_class\": null,\r\n",
      "    \"top_k\": 50,\r\n",
      "    \"top_p\": 1.0,\r\n",
      "    \"torch_dtype\": \"float32\",\r\n",
      "    \"torchscript\": false,\r\n",
      "    \"typical_p\": 1.0,\r\n",
      "    \"use_bfloat16\": false,\r\n",
      "    \"use_cache\": true,\r\n",
      "    \"use_fused_lm_heads\": false,\r\n",
      "    \"vocab_size\": 1088\r\n",
      "  },\r\n",
      "  \"decoder_start_token_id\": 1025,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"model_type\": \"parler_tts\",\r\n",
      "  \"pad_token_id\": 1024,\r\n",
      "  \"prompt_cross_attention\": false,\r\n",
      "  \"text_encoder\": {\r\n",
      "    \"_attn_implementation_autoset\": false,\r\n",
      "    \"_name_or_path\": \"google/flan-t5-large\",\r\n",
      "    \"add_cross_attention\": false,\r\n",
      "    \"architectures\": [\r\n",
      "      \"T5ForConditionalGeneration\"\r\n",
      "    ],\r\n",
      "    \"bad_words_ids\": null,\r\n",
      "    \"begin_suppress_tokens\": null,\r\n",
      "    \"bos_token_id\": null,\r\n",
      "    \"chunk_size_feed_forward\": 0,\r\n",
      "    \"classifier_dropout\": 0.0,\r\n",
      "    \"cross_attention_hidden_size\": null,\r\n",
      "    \"d_ff\": 2816,\r\n",
      "    \"d_kv\": 64,\r\n",
      "    \"d_model\": 1024,\r\n",
      "    \"decoder_start_token_id\": 0,\r\n",
      "    \"dense_act_fn\": \"gelu_new\",\r\n",
      "    \"diversity_penalty\": 0.0,\r\n",
      "    \"do_sample\": false,\r\n",
      "    \"dropout_rate\": 0.1,\r\n",
      "    \"early_stopping\": false,\r\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\r\n",
      "    \"eos_token_id\": 1,\r\n",
      "    \"exponential_decay_length_penalty\": null,\r\n",
      "    \"feed_forward_proj\": \"gated-gelu\",\r\n",
      "    \"finetuning_task\": null,\r\n",
      "    \"forced_bos_token_id\": null,\r\n",
      "    \"forced_eos_token_id\": null,\r\n",
      "    \"id2label\": {\r\n",
      "      \"0\": \"LABEL_0\",\r\n",
      "      \"1\": \"LABEL_1\"\r\n",
      "    },\r\n",
      "    \"initializer_factor\": 1.0,\r\n",
      "    \"is_decoder\": false,\r\n",
      "    \"is_encoder_decoder\": true,\r\n",
      "    \"is_gated_act\": true,\r\n",
      "    \"label2id\": {\r\n",
      "      \"LABEL_0\": 0,\r\n",
      "      \"LABEL_1\": 1\r\n",
      "    },\r\n",
      "    \"layer_norm_epsilon\": 1e-06,\r\n",
      "    \"length_penalty\": 1.0,\r\n",
      "    \"max_length\": 20,\r\n",
      "    \"min_length\": 0,\r\n",
      "    \"model_type\": \"t5\",\r\n",
      "    \"n_positions\": 512,\r\n",
      "    \"no_repeat_ngram_size\": 0,\r\n",
      "    \"num_beam_groups\": 1,\r\n",
      "    \"num_beams\": 1,\r\n",
      "    \"num_decoder_layers\": 24,\r\n",
      "    \"num_heads\": 16,\r\n",
      "    \"num_layers\": 24,\r\n",
      "    \"num_return_sequences\": 1,\r\n",
      "    \"output_attentions\": false,\r\n",
      "    \"output_hidden_states\": false,\r\n",
      "    \"output_past\": true,\r\n",
      "    \"output_scores\": false,\r\n",
      "    \"pad_token_id\": 0,\r\n",
      "    \"prefix\": null,\r\n",
      "    \"problem_type\": null,\r\n",
      "    \"pruned_heads\": {},\r\n",
      "    \"relative_attention_max_distance\": 128,\r\n",
      "    \"relative_attention_num_buckets\": 32,\r\n",
      "    \"remove_invalid_values\": false,\r\n",
      "    \"repetition_penalty\": 1.0,\r\n",
      "    \"return_dict\": true,\r\n",
      "    \"return_dict_in_generate\": false,\r\n",
      "    \"sep_token_id\": null,\r\n",
      "    \"suppress_tokens\": null,\r\n",
      "    \"task_specific_params\": null,\r\n",
      "    \"temperature\": 1.0,\r\n",
      "    \"tf_legacy_loss\": false,\r\n",
      "    \"tie_encoder_decoder\": false,\r\n",
      "    \"tie_word_embeddings\": false,\r\n",
      "    \"tokenizer_class\": null,\r\n",
      "    \"top_k\": 50,\r\n",
      "    \"top_p\": 1.0,\r\n",
      "    \"torch_dtype\": null,\r\n",
      "    \"torchscript\": false,\r\n",
      "    \"typical_p\": 1.0,\r\n",
      "    \"use_bfloat16\": false,\r\n",
      "    \"use_cache\": true,\r\n",
      "    \"vocab_size\": 32128\r\n",
      "  },\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"vocab_size\": 32128\r\n",
      "}\r\n",
      "\r\n",
      "model.safetensors: 100%|███████████████████| 3.51G/3.51G [01:24<00:00, 41.6MB/s]\r\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/model.safetensors\r\n",
      "Generate config GenerationConfig {\r\n",
      "  \"decoder_start_token_id\": 1025,\r\n",
      "  \"pad_token_id\": 1024\r\n",
      "}\r\n",
      "\r\n",
      "Instantiating T5EncoderModel model under default dtype torch.float32.\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\r\n",
      "  WeightNorm.apply(module, name, dim)\r\n",
      "Instantiating DACModel model under default dtype torch.float32.\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\r\n",
      "  WeightNorm.apply(module, name, dim)\r\n",
      "Instantiating ParlerTTSForCausalLM model under default dtype torch.float32.\r\n",
      "Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 1025,\r\n",
      "  \"eos_token_id\": 1024,\r\n",
      "  \"pad_token_id\": 1024\r\n",
      "}\r\n",
      "\r\n",
      "All model checkpoint weights were used when initializing ParlerTTSForConditionalGeneration.\r\n",
      "\r\n",
      "All the weights of ParlerTTSForConditionalGeneration were initialized from the model checkpoint at parler-tts/parler-tts-mini-v1.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ParlerTTSForConditionalGeneration for predictions without further training.\r\n",
      "generation_config.json: 100%|██████████████████| 265/265 [00:00<00:00, 1.62MB/s]\r\n",
      "gathered_tensor loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--parler-tts--parler-tts-mini-v1/snapshots/0392b9451a601e528fd863bbb0598431fee810d9/generation_config.json\r\n",
      "Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 1025,\r\n",
      "  \"decoder_start_token_id\": 1025,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": 1024,\r\n",
      "  \"guidance_scale\": 1,\r\n",
      "  \"max_length\": 2580,\r\n",
      "  \"min_new_tokens\": 10,\r\n",
      "  \"pad_token_id\": 1024\r\n",
      "}\r\n",
      "\r\n",
      "gathered_tensor tensor([0, 1], device='cuda:1')\r\n",
      "tensor([0, 1], device='cuda:0')\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Filter (num_proc=2): 100%|██████| 20440/20440 [00:00<00:00, 77825.44 examples/s]\r\n",
      "Filter (num_proc=2): 100%|█████████████████| 8/8 [00:00<00:00, 35.36 examples/s]\r\n",
      "preprocess datasets (num_proc=2): 100%|█| 20440/20440 [00:07<00:00, 2742.24 exam\r\n",
      "preprocess datasets (num_proc=2): 100%|████| 8/8 [00:00<00:00, 17.08 examples/s]\r\n",
      "  0%|                                                  | 0/2044 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      " 24%|█████████▊                              | 499/2044 [11:15<34:30,  1.34s/it]\r\n",
      "Postprocessing labeling (num_proc=2):   0%|     | 0/5000 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   0%| | 4/5000 [00:01<32:07,  2.59 example\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   0%| | 25/5000 [00:01<04:17, 19.35 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 37/5000 [00:01<03:00, 27.53 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 47/5000 [00:01<02:18, 35.67 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 62/5000 [00:02<01:36, 51.11 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 83/5000 [00:02<01:03, 77.20 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 103/5000 [00:02<00:49, 99.92 examp\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 122/5000 [00:02<00:41, 118.44 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 156/5000 [00:02<00:28, 168.39 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 189/5000 [00:02<00:23, 206.65 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 223/5000 [00:02<00:20, 236.63 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   5%| | 257/5000 [00:02<00:18, 259.19 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   6%| | 291/5000 [00:02<00:17, 272.58 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   6%| | 324/5000 [00:03<00:16, 284.64 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   7%| | 357/5000 [00:03<00:18, 252.64 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   8%| | 385/5000 [00:03<00:18, 249.66 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   8%| | 419/5000 [00:03<00:19, 239.21 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 452/5000 [00:03<00:17, 259.83 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  10%| | 487/5000 [00:03<00:16, 278.79 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  10%| | 521/5000 [00:03<00:15, 290.50 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  11%| | 554/5000 [00:03<00:14, 297.17 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  12%| | 588/5000 [00:04<00:14, 302.62 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  12%| | 624/5000 [00:04<00:14, 312.50 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  13%|▏| 658/5000 [00:04<00:13, 314.86 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 691/5000 [00:04<00:13, 315.35 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  15%|▏| 728/5000 [00:04<00:14, 292.84 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  15%|▏| 763/5000 [00:04<00:14, 297.62 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  16%|▏| 798/5000 [00:04<00:13, 307.92 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 833/5000 [00:04<00:13, 307.62 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 870/5000 [00:04<00:12, 320.30 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  18%|▏| 909/5000 [00:05<00:12, 325.34 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  19%|▏| 946/5000 [00:05<00:14, 288.70 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 980/5000 [00:05<00:13, 296.59 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 1014/5000 [00:05<00:13, 304.46 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  21%|▏| 1050/5000 [00:05<00:12, 311.50 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1086/5000 [00:05<00:12, 317.79 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1121/5000 [00:05<00:12, 320.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  23%|▏| 1155/5000 [00:05<00:12, 317.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1190/5000 [00:05<00:11, 320.85 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1225/5000 [00:06<00:11, 321.33 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▎| 1260/5000 [00:06<00:11, 317.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  26%|▎| 1293/5000 [00:06<00:12, 297.76 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1334/5000 [00:06<00:12, 285.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1369/5000 [00:06<00:12, 296.34 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  28%|▎| 1404/5000 [00:06<00:11, 300.64 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1438/5000 [00:06<00:11, 305.07 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1472/5000 [00:06<00:11, 303.88 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  30%|▎| 1504/5000 [00:06<00:11, 299.67 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1537/5000 [00:07<00:11, 300.59 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1570/5000 [00:07<00:11, 300.31 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 1606/5000 [00:07<00:12, 268.97 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1634/5000 [00:07<00:19, 174.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1659/5000 [00:07<00:23, 143.67 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1682/5000 [00:08<00:25, 132.19 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1703/5000 [00:08<00:25, 131.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1721/5000 [00:08<00:23, 137.10 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  35%|▎| 1757/5000 [00:08<00:18, 177.12 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1791/5000 [00:08<00:15, 209.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  37%|▎| 1826/5000 [00:08<00:13, 237.89 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  37%|▎| 1862/5000 [00:08<00:11, 261.84 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1899/5000 [00:09<00:10, 285.24 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  39%|▍| 1933/5000 [00:09<00:10, 293.12 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  39%|▍| 1973/5000 [00:09<00:09, 314.52 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 2008/5000 [00:09<00:09, 320.66 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2042/5000 [00:09<00:09, 323.85 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  42%|▍| 2077/5000 [00:09<00:08, 326.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  42%|▍| 2113/5000 [00:09<00:08, 330.10 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2149/5000 [00:09<00:08, 331.84 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  44%|▍| 2183/5000 [00:09<00:08, 327.63 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  44%|▍| 2221/5000 [00:09<00:08, 319.27 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  45%|▍| 2264/5000 [00:10<00:08, 314.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  46%|▍| 2297/5000 [00:10<00:08, 316.12 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2333/5000 [00:10<00:08, 322.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2368/5000 [00:10<00:08, 325.45 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  48%|▍| 2401/5000 [00:10<00:08, 322.40 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  49%|▍| 2437/5000 [00:10<00:07, 327.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  49%|▍| 2473/5000 [00:10<00:07, 331.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  50%|▌| 2518/5000 [00:10<00:09, 273.66 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  51%|▌| 2551/5000 [00:11<00:09, 251.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  52%|▌| 2578/5000 [00:11<00:10, 227.22 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  52%|▌| 2609/5000 [00:11<00:10, 218.19 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  53%|▌| 2642/5000 [00:11<00:10, 215.23 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  53%|▌| 2672/5000 [00:11<00:11, 205.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  54%|▌| 2703/5000 [00:11<00:11, 204.22 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2738/5000 [00:12<00:09, 234.11 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2774/5000 [00:12<00:08, 260.78 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  56%|▌| 2810/5000 [00:12<00:07, 282.34 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  57%|▌| 2844/5000 [00:12<00:07, 293.65 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  58%|▌| 2879/5000 [00:12<00:06, 307.27 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  58%|▌| 2915/5000 [00:12<00:06, 318.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2951/5000 [00:12<00:06, 323.73 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  60%|▌| 2986/5000 [00:12<00:06, 326.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  60%|▌| 3022/5000 [00:12<00:05, 329.68 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  61%|▌| 3058/5000 [00:12<00:05, 332.71 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▌| 3092/5000 [00:13<00:05, 328.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  63%|▋| 3126/5000 [00:13<00:05, 326.93 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  63%|▋| 3161/5000 [00:13<00:06, 285.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3197/5000 [00:13<00:06, 297.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  65%|▋| 3236/5000 [00:13<00:05, 301.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  65%|▋| 3270/5000 [00:13<00:06, 270.49 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  66%|▋| 3303/5000 [00:13<00:06, 249.97 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3335/5000 [00:14<00:07, 234.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3371/5000 [00:14<00:06, 260.78 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  68%|▋| 3406/5000 [00:14<00:05, 280.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3441/5000 [00:14<00:05, 295.70 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3473/5000 [00:14<00:05, 301.29 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  70%|▋| 3510/5000 [00:14<00:04, 315.00 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  71%|▋| 3545/5000 [00:14<00:04, 320.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3582/5000 [00:14<00:04, 329.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3617/5000 [00:14<00:04, 331.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  73%|▋| 3655/5000 [00:14<00:03, 337.61 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  74%|▋| 3692/5000 [00:15<00:03, 339.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  75%|▋| 3729/5000 [00:15<00:03, 343.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  75%|▊| 3765/5000 [00:15<00:03, 345.32 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  76%|▊| 3801/5000 [00:15<00:03, 341.36 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3838/5000 [00:15<00:03, 346.33 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3874/5000 [00:15<00:03, 346.92 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  78%|▊| 3911/5000 [00:15<00:03, 344.12 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 3946/5000 [00:15<00:03, 341.54 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 3983/5000 [00:15<00:02, 342.33 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 4018/5000 [00:16<00:02, 334.09 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  81%|▊| 4055/5000 [00:16<00:02, 330.89 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  82%|▊| 4098/5000 [00:16<00:02, 329.38 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  83%|▊| 4133/5000 [00:16<00:02, 329.17 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  83%|▊| 4169/5000 [00:16<00:02, 336.47 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  84%|▊| 4205/5000 [00:16<00:02, 336.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  85%|▊| 4241/5000 [00:16<00:02, 337.50 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4275/5000 [00:16<00:02, 336.32 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4310/5000 [00:16<00:02, 337.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  87%|▊| 4345/5000 [00:17<00:01, 337.49 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  88%|▉| 4381/5000 [00:17<00:01, 339.22 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  88%|▉| 4419/5000 [00:17<00:02, 268.87 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  89%|▉| 4453/5000 [00:17<00:02, 203.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  90%|▉| 4484/5000 [00:17<00:02, 221.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  90%|▉| 4520/5000 [00:17<00:01, 249.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  91%|▉| 4550/5000 [00:17<00:01, 252.45 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4581/5000 [00:18<00:02, 206.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4606/5000 [00:18<00:02, 148.58 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  93%|▉| 4626/5000 [00:19<00:04, 82.52 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  93%|▉| 4670/5000 [00:19<00:02, 122.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  94%|▉| 4702/5000 [00:19<00:02, 138.07 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4732/5000 [00:19<00:01, 150.67 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4755/5000 [00:19<00:01, 162.31 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4778/5000 [00:19<00:01, 174.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4800/5000 [00:19<00:01, 183.39 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4823/5000 [00:19<00:00, 192.02 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  97%|▉| 4845/5000 [00:20<00:00, 197.22 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  97%|▉| 4867/5000 [00:20<00:00, 199.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4893/5000 [00:20<00:00, 182.53 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4913/5000 [00:20<00:00, 182.91 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  99%|▉| 4933/5000 [00:20<00:00, 183.93 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  99%|▉| 4955/5000 [00:20<00:00, 192.35 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2): 100%|▉| 4975/5000 [00:20<00:00, 190.60 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2): 100%|█| 5000/5000 [00:22<00:00, 224.68 exa\r\n",
      "\r\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Saving the dataset (0/1 shards): 100%|█| 5000/5000 [00:00<00:00, 48969.70 exampl\u001b[A\r\n",
      "Saving the dataset (1/1 shards): 100%|█| 5000/5000 [00:00<00:00, 48668.21 exampl\r\n",
      " 49%|███████████████████▌                    | 999/2044 [22:43<22:30,  1.29s/it]\r\n",
      "Postprocessing labeling (num_proc=2):   0%|     | 0/5000 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   0%| | 11/5000 [00:01<11:28,  7.25 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 55/5000 [00:01<01:51, 44.33 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 88/5000 [00:01<01:09, 70.77 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 113/5000 [00:02<01:01, 79.00 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 137/5000 [00:02<00:56, 86.66 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 158/5000 [00:02<00:50, 95.61 examp\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 182/5000 [00:02<00:41, 116.40 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 216/5000 [00:02<00:31, 153.54 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   5%| | 252/5000 [00:02<00:24, 192.26 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   6%| | 287/5000 [00:02<00:21, 223.32 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   6%| | 320/5000 [00:02<00:19, 244.94 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   7%| | 356/5000 [00:03<00:17, 267.64 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   8%| | 392/5000 [00:03<00:15, 288.32 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 427/5000 [00:03<00:15, 296.50 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 462/5000 [00:03<00:14, 307.74 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  10%| | 497/5000 [00:03<00:14, 310.75 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  11%| | 532/5000 [00:03<00:14, 314.95 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  11%| | 567/5000 [00:03<00:13, 319.20 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  12%| | 603/5000 [00:03<00:13, 327.12 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  13%|▏| 642/5000 [00:03<00:12, 335.41 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 678/5000 [00:04<00:12, 337.62 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 712/5000 [00:04<00:12, 336.72 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  15%|▏| 749/5000 [00:04<00:14, 289.25 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  16%|▏| 780/5000 [00:04<00:14, 282.38 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  16%|▏| 811/5000 [00:04<00:16, 253.15 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 842/5000 [00:04<00:16, 259.74 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 874/5000 [00:04<00:17, 238.24 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  18%|▏| 904/5000 [00:04<00:17, 240.00 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  19%|▏| 930/5000 [00:05<00:18, 218.01 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  19%|▏| 956/5000 [00:05<00:20, 196.98 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 978/5000 [00:05<00:20, 197.56 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 1003/5000 [00:05<00:21, 189.98 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  21%|▏| 1028/5000 [00:05<00:19, 199.00 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  21%|▏| 1053/5000 [00:05<00:18, 208.57 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1079/5000 [00:05<00:18, 216.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1104/5000 [00:05<00:17, 219.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  23%|▏| 1130/5000 [00:06<00:17, 226.78 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  23%|▏| 1156/5000 [00:06<00:16, 231.77 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1181/5000 [00:06<00:16, 228.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1211/5000 [00:06<00:15, 239.89 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▏| 1237/5000 [00:06<00:15, 242.17 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▎| 1267/5000 [00:06<00:15, 244.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  26%|▎| 1297/5000 [00:06<00:16, 228.76 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  26%|▎| 1323/5000 [00:06<00:15, 231.19 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1347/5000 [00:06<00:15, 230.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1372/5000 [00:07<00:15, 228.76 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  28%|▎| 1396/5000 [00:07<00:15, 228.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  28%|▎| 1421/5000 [00:07<00:15, 226.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1447/5000 [00:07<00:15, 231.86 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1472/5000 [00:07<00:15, 230.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  30%|▎| 1496/5000 [00:07<00:15, 231.49 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  30%|▎| 1520/5000 [00:07<00:14, 232.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1545/5000 [00:07<00:14, 234.29 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1571/5000 [00:07<00:14, 236.88 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 1598/5000 [00:08<00:14, 236.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 1624/5000 [00:08<00:14, 238.53 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1648/5000 [00:08<00:14, 233.79 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1673/5000 [00:08<00:14, 235.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1698/5000 [00:08<00:14, 230.30 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1723/5000 [00:08<00:14, 232.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  35%|▎| 1747/5000 [00:08<00:14, 231.81 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  35%|▎| 1773/5000 [00:08<00:13, 233.54 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1798/5000 [00:08<00:13, 233.54 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1823/5000 [00:09<00:13, 234.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  37%|▎| 1847/5000 [00:09<00:13, 233.88 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  37%|▎| 1872/5000 [00:09<00:13, 236.02 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1898/5000 [00:09<00:13, 234.74 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1924/5000 [00:09<00:13, 234.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  39%|▍| 1948/5000 [00:09<00:13, 230.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  39%|▍| 1972/5000 [00:09<00:15, 194.79 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 1994/5000 [00:10<00:31, 96.13 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 2011/5000 [00:10<00:31, 95.53 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2026/5000 [00:10<00:31, 93.79 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2049/5000 [00:10<00:25, 115.94 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2073/5000 [00:10<00:21, 136.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  42%|▍| 2108/5000 [00:10<00:16, 179.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2135/5000 [00:11<00:14, 196.66 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2161/5000 [00:11<00:13, 208.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  44%|▍| 2187/5000 [00:11<00:12, 218.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  44%|▍| 2212/5000 [00:11<00:12, 225.86 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  45%|▍| 2236/5000 [00:11<00:12, 225.59 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  45%|▍| 2261/5000 [00:11<00:12, 227.24 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  46%|▍| 2285/5000 [00:11<00:11, 227.92 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  46%|▍| 2312/5000 [00:11<00:11, 235.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2337/5000 [00:11<00:11, 232.23 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2361/5000 [00:12<00:11, 231.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  48%|▍| 2393/5000 [00:12<00:10, 253.50 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  49%|▍| 2428/5000 [00:12<00:09, 277.69 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  49%|▍| 2464/5000 [00:12<00:08, 299.33 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  50%|▌| 2501/5000 [00:12<00:07, 314.92 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  51%|▌| 2537/5000 [00:12<00:07, 318.96 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  51%|▌| 2571/5000 [00:12<00:07, 318.70 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  52%|▌| 2607/5000 [00:12<00:07, 327.10 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  53%|▌| 2644/5000 [00:12<00:07, 328.99 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  54%|▌| 2681/5000 [00:12<00:06, 336.32 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  54%|▌| 2716/5000 [00:13<00:06, 333.59 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2753/5000 [00:13<00:06, 335.77 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  56%|▌| 2788/5000 [00:13<00:06, 336.09 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  56%|▌| 2824/5000 [00:13<00:06, 332.02 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  57%|▌| 2860/5000 [00:13<00:06, 334.96 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  58%|▌| 2896/5000 [00:13<00:06, 333.52 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2932/5000 [00:13<00:06, 331.87 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2969/5000 [00:13<00:06, 335.74 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  60%|▌| 3007/5000 [00:13<00:05, 342.12 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  61%|▌| 3043/5000 [00:14<00:05, 340.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▌| 3078/5000 [00:14<00:05, 337.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▌| 3114/5000 [00:14<00:05, 333.53 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  63%|▋| 3150/5000 [00:14<00:05, 334.37 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3186/5000 [00:14<00:05, 332.83 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3222/5000 [00:14<00:05, 334.96 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  65%|▋| 3261/5000 [00:14<00:05, 345.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  66%|▋| 3297/5000 [00:14<00:04, 340.67 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3335/5000 [00:14<00:04, 348.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  68%|▋| 3375/5000 [00:15<00:04, 345.65 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  68%|▋| 3411/5000 [00:15<00:05, 307.74 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3451/5000 [00:15<00:04, 320.22 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  70%|▋| 3487/5000 [00:15<00:04, 319.94 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  70%|▋| 3523/5000 [00:15<00:04, 326.69 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  71%|▋| 3559/5000 [00:15<00:04, 333.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3594/5000 [00:15<00:04, 335.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  73%|▋| 3630/5000 [00:15<00:04, 338.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  73%|▋| 3666/5000 [00:15<00:03, 341.46 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  74%|▋| 3702/5000 [00:16<00:03, 341.37 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  75%|▋| 3738/5000 [00:16<00:03, 344.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  75%|▊| 3773/5000 [00:16<00:03, 341.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  76%|▊| 3808/5000 [00:16<00:04, 297.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3840/5000 [00:16<00:04, 262.60 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3873/5000 [00:16<00:05, 200.39 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  78%|▊| 3905/5000 [00:16<00:04, 222.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 3941/5000 [00:17<00:04, 250.16 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 3977/5000 [00:17<00:03, 274.24 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 4012/5000 [00:17<00:03, 291.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  81%|▊| 4048/5000 [00:17<00:03, 304.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  82%|▊| 4087/5000 [00:17<00:03, 254.30 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  82%|▊| 4119/5000 [00:17<00:04, 201.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  83%|▊| 4154/5000 [00:17<00:03, 230.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  84%|▊| 4189/5000 [00:17<00:03, 254.64 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  84%|▊| 4225/5000 [00:18<00:02, 277.39 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  85%|▊| 4261/5000 [00:18<00:02, 294.39 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4296/5000 [00:18<00:02, 307.34 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  87%|▊| 4330/5000 [00:18<00:02, 313.31 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  87%|▊| 4366/5000 [00:18<00:01, 320.49 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  88%|▉| 4402/5000 [00:18<00:01, 326.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  89%|▉| 4437/5000 [00:18<00:01, 322.46 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  90%|▉| 4479/5000 [00:18<00:01, 307.23 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  90%|▉| 4515/5000 [00:18<00:01, 316.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  91%|▉| 4551/5000 [00:19<00:01, 322.94 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4585/5000 [00:19<00:01, 324.41 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4620/5000 [00:19<00:01, 328.22 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  93%|▉| 4655/5000 [00:19<00:01, 330.48 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  94%|▉| 4690/5000 [00:19<00:00, 334.63 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4727/5000 [00:19<00:00, 337.54 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4763/5000 [00:19<00:00, 335.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4799/5000 [00:19<00:00, 338.61 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  97%|▉| 4835/5000 [00:19<00:00, 336.26 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4879/5000 [00:20<00:00, 319.89 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4915/5000 [00:20<00:00, 323.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  99%|▉| 4949/5000 [00:20<00:00, 254.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2): 100%|█| 5000/5000 [00:22<00:00, 224.10 exa\r\n",
      "\r\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Saving the dataset (0/1 shards): 100%|█| 5000/5000 [00:00<00:00, 48410.04 exampl\u001b[A\r\n",
      "Saving the dataset (1/1 shards): 100%|█| 5000/5000 [00:00<00:00, 48120.57 exampl\r\n",
      " 73%|████████████████████████████▌          | 1499/2044 [34:12<12:40,  1.40s/it]\r\n",
      "Postprocessing labeling (num_proc=2):   0%|     | 0/5000 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   0%| | 9/5000 [00:01<13:58,  5.95 example\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 34/5000 [00:01<03:04, 26.92 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 55/5000 [00:01<01:45, 46.67 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 75/5000 [00:01<01:13, 66.98 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 95/5000 [00:01<00:56, 87.49 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 116/5000 [00:02<00:45, 108.51 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 138/5000 [00:02<00:42, 115.39 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 160/5000 [00:02<00:36, 133.25 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 180/5000 [00:02<00:33, 144.76 exam\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 201/5000 [00:02<00:30, 158.69 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   5%| | 226/5000 [00:02<00:26, 179.59 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   5%| | 255/5000 [00:02<00:23, 201.24 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   6%| | 293/5000 [00:02<00:19, 244.76 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   7%| | 329/5000 [00:02<00:17, 270.93 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   7%| | 364/5000 [00:03<00:16, 289.52 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   8%| | 398/5000 [00:03<00:15, 300.30 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 434/5000 [00:03<00:14, 312.57 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 468/5000 [00:03<00:14, 318.96 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  10%| | 501/5000 [00:03<00:14, 320.24 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  11%| | 536/5000 [00:03<00:13, 323.17 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  11%| | 572/5000 [00:03<00:13, 327.74 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  12%| | 608/5000 [00:03<00:13, 328.52 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  13%|▏| 643/5000 [00:03<00:13, 330.97 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 679/5000 [00:04<00:13, 330.16 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 713/5000 [00:04<00:13, 324.90 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  15%|▏| 749/5000 [00:04<00:12, 328.11 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  16%|▏| 784/5000 [00:04<00:12, 325.28 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  16%|▏| 817/5000 [00:04<00:13, 318.86 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 852/5000 [00:04<00:13, 318.10 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  18%|▏| 888/5000 [00:04<00:12, 320.12 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  18%|▏| 924/5000 [00:04<00:13, 304.73 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  19%|▏| 956/5000 [00:04<00:14, 277.76 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 994/5000 [00:05<00:14, 285.63 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  21%|▏| 1029/5000 [00:05<00:13, 297.68 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  21%|▏| 1064/5000 [00:05<00:12, 307.84 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1098/5000 [00:05<00:12, 312.52 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  23%|▏| 1131/5000 [00:05<00:12, 307.56 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  23%|▏| 1165/5000 [00:05<00:12, 312.07 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1197/5000 [00:05<00:12, 306.11 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▏| 1232/5000 [00:05<00:12, 309.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▎| 1265/5000 [00:05<00:12, 310.88 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  26%|▎| 1299/5000 [00:06<00:11, 314.94 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1331/5000 [00:06<00:12, 290.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1363/5000 [00:06<00:14, 255.54 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  28%|▎| 1390/5000 [00:06<00:14, 250.38 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  28%|▎| 1421/5000 [00:06<00:13, 260.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1455/5000 [00:06<00:12, 277.73 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  30%|▎| 1490/5000 [00:06<00:12, 290.35 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1526/5000 [00:06<00:11, 302.66 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1558/5000 [00:06<00:11, 304.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 1591/5000 [00:07<00:11, 307.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1628/5000 [00:07<00:10, 318.45 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1670/5000 [00:07<00:10, 321.78 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1707/5000 [00:07<00:10, 304.33 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  35%|▎| 1740/5000 [00:07<00:10, 309.64 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1775/5000 [00:07<00:10, 310.42 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1809/5000 [00:07<00:10, 314.79 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  37%|▎| 1843/5000 [00:07<00:10, 311.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1877/5000 [00:07<00:10, 310.68 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1913/5000 [00:08<00:09, 317.88 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  39%|▍| 1952/5000 [00:08<00:10, 288.31 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 1982/5000 [00:08<00:13, 226.12 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 2010/5000 [00:08<00:17, 167.94 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2040/5000 [00:08<00:18, 159.69 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2070/5000 [00:09<00:16, 182.80 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  42%|▍| 2106/5000 [00:09<00:13, 215.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2140/5000 [00:09<00:11, 239.24 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2171/5000 [00:09<00:12, 231.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  44%|▍| 2197/5000 [00:09<00:12, 233.50 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  45%|▍| 2233/5000 [00:09<00:10, 260.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  45%|▍| 2269/5000 [00:09<00:09, 282.85 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  46%|▍| 2304/5000 [00:09<00:09, 299.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2336/5000 [00:09<00:08, 301.59 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2370/5000 [00:10<00:08, 307.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  48%|▍| 2407/5000 [00:10<00:08, 319.65 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  49%|▍| 2443/5000 [00:10<00:07, 324.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  50%|▍| 2478/5000 [00:10<00:07, 326.61 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  50%|▌| 2513/5000 [00:10<00:07, 331.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  51%|▌| 2549/5000 [00:10<00:07, 337.11 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  52%|▌| 2584/5000 [00:10<00:07, 336.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  52%|▌| 2620/5000 [00:10<00:07, 337.11 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  53%|▌| 2656/5000 [00:10<00:06, 335.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  54%|▌| 2692/5000 [00:11<00:06, 338.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2726/5000 [00:11<00:06, 330.65 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2760/5000 [00:11<00:06, 328.38 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  56%|▌| 2796/5000 [00:11<00:06, 331.99 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  57%|▌| 2831/5000 [00:11<00:06, 330.93 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  57%|▌| 2866/5000 [00:11<00:06, 330.99 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  58%|▌| 2901/5000 [00:11<00:06, 329.58 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2936/5000 [00:11<00:06, 327.00 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2971/5000 [00:11<00:06, 322.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  60%|▌| 3007/5000 [00:11<00:06, 324.81 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  61%|▌| 3043/5000 [00:12<00:05, 326.45 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▌| 3079/5000 [00:12<00:05, 327.37 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▌| 3115/5000 [00:12<00:05, 328.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  63%|▋| 3150/5000 [00:12<00:05, 327.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3186/5000 [00:12<00:05, 327.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3222/5000 [00:12<00:05, 329.73 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  65%|▋| 3259/5000 [00:12<00:05, 330.69 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  66%|▋| 3295/5000 [00:12<00:05, 333.68 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3329/5000 [00:12<00:05, 331.19 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3365/5000 [00:13<00:04, 332.16 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  68%|▋| 3401/5000 [00:13<00:04, 338.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3436/5000 [00:13<00:04, 341.02 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3472/5000 [00:13<00:04, 340.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  70%|▋| 3508/5000 [00:13<00:04, 339.46 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  71%|▋| 3543/5000 [00:13<00:04, 338.54 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3579/5000 [00:13<00:04, 338.80 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3615/5000 [00:13<00:04, 338.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  73%|▋| 3651/5000 [00:13<00:03, 338.45 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  74%|▋| 3685/5000 [00:14<00:04, 303.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  74%|▋| 3716/5000 [00:14<00:04, 290.41 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  75%|▋| 3749/5000 [00:14<00:04, 260.72 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  76%|▊| 3780/5000 [00:14<00:04, 260.65 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  76%|▊| 3812/5000 [00:14<00:04, 240.40 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3837/5000 [00:14<00:04, 239.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3865/5000 [00:14<00:04, 242.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  78%|▊| 3891/5000 [00:15<00:06, 165.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  78%|▊| 3914/5000 [00:15<00:07, 147.16 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 3938/5000 [00:15<00:06, 162.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 3964/5000 [00:15<00:05, 179.52 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 3990/5000 [00:15<00:05, 191.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 4016/5000 [00:15<00:04, 205.26 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  81%|▊| 4041/5000 [00:15<00:04, 212.09 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  81%|▊| 4065/5000 [00:15<00:04, 212.84 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  82%|▊| 4091/5000 [00:16<00:04, 221.80 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  82%|▊| 4116/5000 [00:16<00:05, 176.51 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  83%|▊| 4142/5000 [00:16<00:05, 155.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  83%|▊| 4172/5000 [00:16<00:04, 182.81 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  84%|▊| 4209/5000 [00:16<00:03, 220.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  85%|▊| 4245/5000 [00:16<00:03, 248.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4281/5000 [00:16<00:02, 271.92 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4321/5000 [00:17<00:02, 298.27 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  87%|▊| 4359/5000 [00:17<00:02, 314.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  88%|▉| 4396/5000 [00:17<00:01, 321.36 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  89%|▉| 4432/5000 [00:17<00:01, 328.25 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  89%|▉| 4472/5000 [00:17<00:01, 298.74 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  90%|▉| 4512/5000 [00:17<00:01, 320.52 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  91%|▉| 4555/5000 [00:17<00:01, 342.09 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4592/5000 [00:17<00:01, 342.98 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  93%|▉| 4627/5000 [00:17<00:01, 338.68 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  93%|▉| 4663/5000 [00:18<00:01, 336.39 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  94%|▉| 4698/5000 [00:18<00:00, 337.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4734/5000 [00:18<00:00, 337.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4770/5000 [00:18<00:00, 338.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4806/5000 [00:18<00:00, 339.23 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  97%|▉| 4840/5000 [00:18<00:00, 329.35 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4875/5000 [00:18<00:00, 327.91 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4911/5000 [00:18<00:00, 335.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  99%|▉| 4947/5000 [00:18<00:00, 337.71 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2): 100%|█| 5000/5000 [00:20<00:00, 240.77 exa\r\n",
      "\r\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Saving the dataset (0/1 shards): 100%|█| 5000/5000 [00:00<00:00, 42882.51 exampl\u001b[A\r\n",
      "Saving the dataset (1/1 shards): 100%|█| 5000/5000 [00:00<00:00, 42618.37 exampl\r\n",
      " 98%|██████████████████████████████████████▏| 1999/2044 [45:36<00:55,  1.23s/it]\r\n",
      "Postprocessing labeling (num_proc=2):   0%|     | 0/5000 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   0%| | 10/5000 [00:01<11:34,  7.18 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 31/5000 [00:01<03:14, 25.52 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 50/5000 [00:01<01:54, 43.26 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 64/5000 [00:01<01:35, 51.67 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 78/5000 [00:01<01:24, 58.10 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 100/5000 [00:02<01:00, 81.59 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   2%| | 118/5000 [00:02<00:53, 90.62 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 131/5000 [00:02<00:50, 96.27 examp\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 153/5000 [00:02<00:40, 121.09 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 188/5000 [00:02<00:28, 170.71 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   4%| | 223/5000 [00:02<00:22, 207.72 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   5%| | 258/5000 [00:02<00:19, 237.73 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   6%| | 292/5000 [00:02<00:18, 253.34 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   7%| | 327/5000 [00:03<00:17, 269.51 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   7%| | 367/5000 [00:03<00:15, 294.14 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   8%| | 403/5000 [00:03<00:15, 305.69 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 437/5000 [00:03<00:14, 311.77 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 472/5000 [00:03<00:14, 320.33 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  10%| | 508/5000 [00:03<00:13, 328.22 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  11%| | 544/5000 [00:03<00:13, 331.54 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  12%| | 580/5000 [00:03<00:13, 332.47 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  12%| | 615/5000 [00:03<00:13, 332.26 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  13%|▏| 651/5000 [00:03<00:12, 335.50 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 687/5000 [00:04<00:12, 335.18 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 721/5000 [00:04<00:12, 332.21 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  15%|▏| 757/5000 [00:04<00:12, 337.00 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  16%|▏| 792/5000 [00:04<00:12, 336.93 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 828/5000 [00:04<00:12, 338.02 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  17%|▏| 864/5000 [00:04<00:12, 338.94 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  18%|▏| 900/5000 [00:04<00:12, 340.32 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  19%|▏| 935/5000 [00:04<00:12, 330.89 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  19%|▏| 970/5000 [00:04<00:12, 331.15 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 1006/5000 [00:05<00:12, 332.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  21%|▏| 1042/5000 [00:05<00:12, 325.16 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1076/5000 [00:05<00:12, 322.46 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  22%|▏| 1112/5000 [00:05<00:12, 322.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  23%|▏| 1149/5000 [00:05<00:11, 327.81 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1185/5000 [00:05<00:11, 331.17 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  24%|▏| 1220/5000 [00:05<00:11, 331.47 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▎| 1255/5000 [00:05<00:11, 322.85 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  26%|▎| 1290/5000 [00:05<00:11, 321.24 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1326/5000 [00:06<00:11, 324.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  27%|▎| 1365/5000 [00:06<00:10, 334.86 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  28%|▎| 1400/5000 [00:06<00:10, 331.96 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1435/5000 [00:06<00:10, 332.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  29%|▎| 1469/5000 [00:06<00:10, 330.53 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  30%|▎| 1508/5000 [00:06<00:10, 330.83 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  31%|▎| 1544/5000 [00:06<00:10, 331.10 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 1580/5000 [00:06<00:10, 328.36 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 1613/5000 [00:06<00:11, 293.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  33%|▎| 1648/5000 [00:07<00:12, 270.86 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1681/5000 [00:07<00:11, 282.74 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  34%|▎| 1715/5000 [00:07<00:11, 294.36 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  35%|▎| 1750/5000 [00:07<00:10, 304.80 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1788/5000 [00:07<00:13, 246.91 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  36%|▎| 1816/5000 [00:07<00:16, 190.67 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  37%|▎| 1851/5000 [00:07<00:14, 219.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1887/5000 [00:08<00:12, 248.10 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  38%|▍| 1922/5000 [00:08<00:11, 267.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  39%|▍| 1958/5000 [00:08<00:10, 285.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 1994/5000 [00:08<00:09, 301.81 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2028/5000 [00:08<00:09, 308.52 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  41%|▍| 2065/5000 [00:08<00:09, 317.89 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  42%|▍| 2101/5000 [00:08<00:08, 327.45 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2141/5000 [00:08<00:09, 301.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  43%|▍| 2174/5000 [00:08<00:09, 306.32 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  44%|▍| 2210/5000 [00:09<00:08, 314.30 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  45%|▍| 2246/5000 [00:09<00:08, 319.28 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  46%|▍| 2287/5000 [00:09<00:08, 337.86 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2328/5000 [00:09<00:08, 309.17 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  47%|▍| 2360/5000 [00:09<00:09, 264.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  48%|▍| 2393/5000 [00:09<00:10, 246.83 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  48%|▍| 2424/5000 [00:09<00:10, 240.78 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  49%|▍| 2460/5000 [00:10<00:09, 261.49 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  50%|▍| 2497/5000 [00:10<00:08, 281.75 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  51%|▌| 2533/5000 [00:10<00:08, 298.89 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  51%|▌| 2565/5000 [00:10<00:08, 291.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  52%|▌| 2595/5000 [00:10<00:08, 282.19 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  53%|▌| 2627/5000 [00:10<00:09, 252.15 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  53%|▌| 2659/5000 [00:10<00:08, 260.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  54%|▌| 2690/5000 [00:10<00:09, 244.39 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  54%|▌| 2718/5000 [00:11<00:09, 242.27 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2743/5000 [00:11<00:09, 241.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 2769/5000 [00:11<00:09, 237.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  56%|▌| 2795/5000 [00:11<00:09, 240.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  56%|▌| 2821/5000 [00:11<00:09, 239.72 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  57%|▌| 2852/5000 [00:11<00:09, 224.08 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  58%|▌| 2878/5000 [00:11<00:09, 228.40 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  58%|▌| 2904/5000 [00:11<00:10, 205.59 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2930/5000 [00:11<00:09, 213.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  59%|▌| 2955/5000 [00:12<00:09, 220.16 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  60%|▌| 2979/5000 [00:12<00:09, 224.16 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  60%|▌| 3004/5000 [00:12<00:08, 228.23 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  61%|▌| 3028/5000 [00:12<00:08, 230.49 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  61%|▌| 3053/5000 [00:12<00:08, 229.72 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▌| 3088/5000 [00:12<00:07, 260.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  62%|▋| 3125/5000 [00:12<00:06, 287.60 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  63%|▋| 3159/5000 [00:12<00:06, 296.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3190/5000 [00:12<00:06, 276.80 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  64%|▋| 3225/5000 [00:13<00:06, 264.06 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  65%|▋| 3255/5000 [00:13<00:06, 269.46 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  66%|▋| 3287/5000 [00:13<00:06, 259.59 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  66%|▋| 3317/5000 [00:13<00:06, 250.90 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3348/5000 [00:13<00:06, 239.86 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  67%|▋| 3373/5000 [00:13<00:06, 237.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  68%|▋| 3403/5000 [00:13<00:06, 249.40 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3434/5000 [00:13<00:06, 259.24 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  69%|▋| 3468/5000 [00:14<00:05, 277.72 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  70%|▋| 3505/5000 [00:14<00:05, 296.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  71%|▋| 3541/5000 [00:14<00:04, 309.71 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3576/5000 [00:14<00:05, 275.01 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  72%|▋| 3609/5000 [00:14<00:05, 250.72 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  73%|▋| 3644/5000 [00:14<00:05, 238.74 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  74%|▋| 3678/5000 [00:14<00:05, 260.85 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  74%|▋| 3714/5000 [00:14<00:04, 283.55 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  75%|▊| 3750/5000 [00:15<00:04, 298.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  76%|▊| 3786/5000 [00:15<00:03, 310.20 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  76%|▊| 3823/5000 [00:15<00:03, 320.58 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  77%|▊| 3858/5000 [00:15<00:03, 321.10 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  78%|▊| 3896/5000 [00:15<00:03, 330.36 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 3931/5000 [00:15<00:03, 328.26 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 3967/5000 [00:15<00:03, 332.87 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  80%|▊| 4002/5000 [00:15<00:03, 332.36 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  81%|▊| 4038/5000 [00:15<00:02, 337.04 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  81%|▊| 4073/5000 [00:15<00:02, 339.19 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  82%|▊| 4109/5000 [00:16<00:02, 342.58 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  83%|▊| 4145/5000 [00:16<00:02, 340.96 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  84%|▊| 4181/5000 [00:16<00:02, 342.18 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  84%|▊| 4220/5000 [00:16<00:02, 270.17 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  85%|▊| 4255/5000 [00:16<00:03, 205.67 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4287/5000 [00:16<00:03, 224.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 4323/5000 [00:17<00:02, 248.43 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  87%|▊| 4364/5000 [00:17<00:02, 279.05 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  88%|▉| 4396/5000 [00:17<00:02, 286.37 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  89%|▉| 4432/5000 [00:17<00:01, 299.40 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  89%|▉| 4468/5000 [00:17<00:01, 307.88 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  90%|▉| 4509/5000 [00:17<00:01, 330.27 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  91%|▉| 4546/5000 [00:17<00:01, 335.47 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4582/5000 [00:17<00:01, 339.66 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  92%|▉| 4617/5000 [00:17<00:01, 340.21 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  93%|▉| 4653/5000 [00:17<00:01, 342.82 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  94%|▉| 4693/5000 [00:18<00:00, 333.60 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4729/5000 [00:18<00:00, 295.47 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  95%|▉| 4760/5000 [00:18<00:00, 273.57 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4791/5000 [00:18<00:00, 261.58 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  96%|▉| 4824/5000 [00:18<00:00, 252.95 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  97%|▉| 4851/5000 [00:18<00:00, 188.14 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  97%|▉| 4873/5000 [00:19<00:00, 160.62 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  98%|▉| 4894/5000 [00:20<00:01, 63.75 exam\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  99%|▉| 4958/5000 [00:20<00:00, 115.44 exa\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2): 100%|█| 5000/5000 [00:21<00:00, 232.64 exa\r\n",
      "\r\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Saving the dataset (0/1 shards): 100%|█| 5000/5000 [00:00<00:00, 37217.40 exampl\u001b[A\r\n",
      "Saving the dataset (1/1 shards): 100%|█| 5000/5000 [00:00<00:00, 37023.05 exampl\r\n",
      "100%|██████████████████████████████████████▉| 2043/2044 [46:57<00:01,  1.29s/it]\r\n",
      "Postprocessing labeling (num_proc=2):   0%|      | 0/440 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   1%| | 4/440 [00:00<00:33, 13.16 examples\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2):   3%| | 14/440 [00:00<00:10, 39.93 example\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):   9%| | 39/440 [00:00<00:03, 101.17 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  14%|▏| 63/440 [00:00<00:02, 138.75 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  20%|▏| 87/440 [00:00<00:02, 165.04 exampl\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  25%|▎| 111/440 [00:00<00:01, 182.77 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  32%|▎| 143/440 [00:00<00:01, 219.29 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  40%|▍| 177/440 [00:01<00:01, 251.47 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  48%|▍| 210/440 [00:01<00:00, 273.48 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  55%|▌| 244/440 [00:01<00:00, 289.44 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  63%|▋| 279/440 [00:01<00:00, 300.26 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  71%|▋| 312/440 [00:01<00:00, 303.95 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  79%|▊| 346/440 [00:01<00:00, 310.29 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2):  86%|▊| 379/440 [00:01<00:00, 312.02 examp\u001b[A\r\n",
      "Postprocessing labeling (num_proc=2): 100%|█| 440/440 [00:02<00:00, 191.31 examp\r\n",
      "\r\n",
      "Saving the dataset (0/1 shards):   0%|           | 0/440 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Saving the dataset (1/1 shards): 100%|█| 440/440 [00:00<00:00, 24249.95 examples\r\n",
      "100%|███████████████████████████████████████| 2044/2044 [47:02<00:00,  1.38s/it]\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\r\n",
      "Postprocessing labeling (num_proc=2):   0%|        | 0/8 [00:00<?, ? examples/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  return torch.load(io.BytesIO(b))\r\n",
      "\r\n",
      "Postprocessing labeling (num_proc=2): 100%|█| 8/8 [00:00<00:00, 29.16 examples/s\r\n",
      "\r\n",
      "Saving the dataset (0/1 shards):   0%|             | 0/8 [00:00<?, ? examples/s]\u001b[A\r\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 1764.72 examples/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.03s/it]\r\n",
      "Filter (num_proc=2): 100%|██████| 20440/20440 [00:00<00:00, 77156.55 examples/s]\r\n",
      "Filter (num_proc=2): 100%|█████████████████| 8/8 [00:00<00:00, 37.21 examples/s]\r\n",
      "Saving the dataset (2/2 shards): 100%|█| 20410/20410 [00:01<00:00, 19570.21 exam\r\n",
      "Saving the dataset (2/2 shards): 100%|█████| 8/8 [00:00<00:00, 26.61 examples/s]\r\n",
      "Map (num_proc=2): 100%|██████████| 20410/20410 [00:03<00:00, 5729.48 examples/s]\r\n",
      "Map (num_proc=2): 100%|████████████████████| 8/8 [00:00<00:00, 31.46 examples/s]\r\n",
      "Train steps ... :   0%|                                 | 0/849 [00:00<?, ?it/s]tokenizer config file saved in ./output_dir_training/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output_dir_training/special_tokens_map.json\r\n",
      "Copy vocab file to ./output_dir_training/spiece.model\r\n",
      "Feature extractor saved in ./output_dir_training/preprocessor_config.json\r\n",
      "Configuration saved in ./output_dir_training/config.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\r\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\r\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\r\n",
      "Step... (2 / 849 | Loss: 5.310250282287598, Learning Rate: 4.000000000000001e-06)\r\n",
      "Train steps ... :   0%|                       | 2/849 [00:37<4:15:10, 18.08s/it]Step... (2 / 849 | Loss: 5.238241672515869, Learning Rate: 4.000000000000001e-06)\r\n",
      "Step... (4 / 849 | Loss: 5.088620662689209, Learning Rate: 8.000000000000001e-06)\r\n",
      "Train steps ... :   0%|                       | 4/849 [01:12<4:12:33, 17.93s/it]Step... (4 / 849 | Loss: 4.878906726837158, Learning Rate: 8.000000000000001e-06)\r\n",
      "Step... (6 / 849 | Loss: 5.25244665145874, Learning Rate: 1.2e-05)\r\n",
      "Train steps ... :   1%|▏                      | 6/849 [01:45<4:02:24, 17.25s/it]Step... (6 / 849 | Loss: 5.471423625946045, Learning Rate: 1.2e-05)\r\n",
      "Step... (8 / 849 | Loss: 5.144472599029541, Learning Rate: 1.6000000000000003e-05)\r\n",
      "Train steps ... :   1%|▏                      | 8/849 [02:18<3:54:23, 16.72s/it]Step... (8 / 849 | Loss: 5.213436603546143, Learning Rate: 1.6000000000000003e-05)\r\n",
      "Train steps ... :   1%|▎                     | 10/849 [02:54<4:01:16, 17.25s/it]Step... (10 / 849 | Loss: 4.79561185836792, Learning Rate: 2e-05)\r\n",
      "Step... (10 / 849 | Loss: 5.214640140533447, Learning Rate: 2e-05)\r\n",
      "Step... (12 / 849 | Loss: 5.250784873962402, Learning Rate: 2.4e-05)\r\n",
      "Train steps ... :   1%|▎                     | 12/849 [03:29<4:05:27, 17.60s/it]Step... (12 / 849 | Loss: 4.835555553436279, Learning Rate: 2.4e-05)\r\n",
      "Step... (14 / 849 | Loss: 5.060945987701416, Learning Rate: 2.8000000000000003e-05)\r\n",
      "Train steps ... :   2%|▎                     | 14/849 [04:02<3:55:41, 16.94s/it]Step... (14 / 849 | Loss: 5.15551233291626, Learning Rate: 2.8000000000000003e-05)\r\n",
      "Step... (16 / 849 | Loss: 4.970108985900879, Learning Rate: 3.2000000000000005e-05)\r\n",
      "Train steps ... :   2%|▍                     | 16/849 [04:36<3:54:13, 16.87s/it]Step... (16 / 849 | Loss: 4.86550760269165, Learning Rate: 3.2000000000000005e-05)\r\n",
      "Train steps ... :   2%|▍                     | 18/849 [05:11<3:59:08, 17.27s/it]Step... (18 / 849 | Loss: 5.29210901260376, Learning Rate: 3.6e-05)\r\n",
      "Step... (18 / 849 | Loss: 5.180130481719971, Learning Rate: 3.6e-05)\r\n",
      "Step... (20 / 849 | Loss: 5.175961494445801, Learning Rate: 4e-05)\r\n",
      "Train steps ... :   2%|▌                     | 20/849 [05:45<3:58:49, 17.29s/it]Step... (20 / 849 | Loss: 4.777679443359375, Learning Rate: 4e-05)\r\n",
      "Step... (22 / 849 | Loss: 5.047147274017334, Learning Rate: 4.4000000000000006e-05)\r\n",
      "Step... (22 / 849 | Loss: 5.1809983253479, Learning Rate: 4.4000000000000006e-05)\r\n",
      "Step... (24 / 849 | Loss: 4.92056131362915, Learning Rate: 4.8e-05)\r\n",
      "Train steps ... :   3%|▌                     | 24/849 [06:52<3:51:22, 16.83s/it]Step... (24 / 849 | Loss: 5.150511264801025, Learning Rate: 4.8e-05)\r\n",
      "Step... (26 / 849 | Loss: 5.00777006149292, Learning Rate: 5.2000000000000004e-05)\r\n",
      "Train steps ... :   3%|▋                     | 26/849 [07:26<3:55:11, 17.15s/it]Step... (26 / 849 | Loss: 4.717978477478027, Learning Rate: 5.2000000000000004e-05)\r\n",
      "Step... (28 / 849 | Loss: 5.055173397064209, Learning Rate: 5.6000000000000006e-05)\r\n",
      "Train steps ... :   3%|▋                     | 28/849 [07:59<3:50:21, 16.84s/it]Step... (28 / 849 | Loss: 5.192061901092529, Learning Rate: 5.6000000000000006e-05)\r\n",
      "Step... (30 / 849 | Loss: 5.189043045043945, Learning Rate: 6e-05)\r\n",
      "Train steps ... :   4%|▊                     | 30/849 [08:32<3:46:29, 16.59s/it]Step... (30 / 849 | Loss: 5.235755920410156, Learning Rate: 6e-05)\r\n",
      "Step... (32 / 849 | Loss: 4.86181116104126, Learning Rate: 6.400000000000001e-05)\r\n",
      "Train steps ... :   4%|▊                     | 32/849 [09:06<3:50:16, 16.91s/it]Step... (32 / 849 | Loss: 4.731207847595215, Learning Rate: 6.400000000000001e-05)\r\n",
      "Step... (34 / 849 | Loss: 4.7308878898620605, Learning Rate: 6.800000000000001e-05)\r\n",
      "Train steps ... :   4%|▉                     | 34/849 [09:40<3:52:02, 17.08s/it]Step... (34 / 849 | Loss: 5.060239315032959, Learning Rate: 6.800000000000001e-05)\r\n",
      "Step... (36 / 849 | Loss: 5.061531066894531, Learning Rate: 7.2e-05)\r\n",
      "Train steps ... :   4%|▉                     | 36/849 [10:12<3:40:56, 16.31s/it]Step... (36 / 849 | Loss: 4.7124528884887695, Learning Rate: 7.2e-05)\r\n",
      "Step... (38 / 849 | Loss: 5.303952217102051, Learning Rate: 7.6e-05)\r\n",
      "Train steps ... :   4%|▉                     | 38/849 [10:46<3:44:54, 16.64s/it]Step... (38 / 849 | Loss: 4.710020542144775, Learning Rate: 7.6e-05)\r\n",
      "Step... (40 / 849 | Loss: 5.334726810455322, Learning Rate: 8e-05)\r\n",
      "Train steps ... :   5%|█                     | 40/849 [11:21<3:53:08, 17.29s/it]Step... (40 / 849 | Loss: 4.854804992675781, Learning Rate: 8e-05)\r\n",
      "Step... (42 / 849 | Loss: 4.914566516876221, Learning Rate: 8.4e-05)\r\n",
      "Train steps ... :   5%|█                     | 42/849 [11:56<3:52:29, 17.29s/it]Step... (42 / 849 | Loss: 5.121408939361572, Learning Rate: 8.4e-05)\r\n",
      "Step... (44 / 849 | Loss: 4.77544641494751, Learning Rate: 8.800000000000001e-05)\r\n",
      "Train steps ... :   5%|█▏                    | 44/849 [12:29<3:46:43, 16.90s/it]Step... (44 / 849 | Loss: 5.041174411773682, Learning Rate: 8.800000000000001e-05)\r\n",
      "Step... (46 / 849 | Loss: 4.894540309906006, Learning Rate: 9.200000000000001e-05)\r\n",
      "Train steps ... :   5%|█▏                    | 46/849 [13:05<3:51:50, 17.32s/it]Step... (46 / 849 | Loss: 4.645544052124023, Learning Rate: 9.200000000000001e-05)\r\n",
      "Step... (48 / 849 | Loss: 4.708868980407715, Learning Rate: 9.6e-05)\r\n",
      "Train steps ... :   6%|█▏                    | 48/849 [13:39<3:53:07, 17.46s/it]Step... (48 / 849 | Loss: 4.923166751861572, Learning Rate: 9.6e-05)\r\n",
      "Step... (50 / 849 | Loss: 5.085672855377197, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   6%|█▎                    | 50/849 [14:10<3:38:28, 16.41s/it]Step... (50 / 849 | Loss: 4.886203289031982, Learning Rate: 0.0001)\r\n",
      "Step... (52 / 849 | Loss: 4.867840766906738, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   6%|█▎                    | 52/849 [14:46<3:46:17, 17.04s/it]Step... (52 / 849 | Loss: 4.691649436950684, Learning Rate: 0.0001)\r\n",
      "Step... (54 / 849 | Loss: 5.157509803771973, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   6%|█▍                    | 54/849 [15:20<3:45:54, 17.05s/it]Step... (54 / 849 | Loss: 4.86011266708374, Learning Rate: 0.0001)\r\n",
      "Step... (56 / 849 | Loss: 5.056248188018799, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   7%|█▍                    | 56/849 [15:53<3:43:49, 16.93s/it]Step... (56 / 849 | Loss: 5.268540382385254, Learning Rate: 0.0001)\r\n",
      "Step... (58 / 849 | Loss: 4.791100978851318, Learning Rate: 0.0001)\r\n",
      "Step... (58 / 849 | Loss: 4.744386672973633, Learning Rate: 0.0001)\r\n",
      "Step... (60 / 849 | Loss: 5.011678218841553, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   7%|█▌                    | 60/849 [17:01<3:45:16, 17.13s/it]Step... (60 / 849 | Loss: 4.81601095199585, Learning Rate: 0.0001)\r\n",
      "Step... (62 / 849 | Loss: 5.0797295570373535, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   7%|█▌                    | 62/849 [17:35<3:44:30, 17.12s/it]Step... (62 / 849 | Loss: 4.86379861831665, Learning Rate: 0.0001)\r\n",
      "Step... (64 / 849 | Loss: 5.086512088775635, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   8%|█▋                    | 64/849 [18:07<3:39:01, 16.74s/it]Step... (64 / 849 | Loss: 5.130709171295166, Learning Rate: 0.0001)\r\n",
      "Step... (66 / 849 | Loss: 4.859235763549805, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   8%|█▋                    | 66/849 [18:42<3:40:25, 16.89s/it]Step... (66 / 849 | Loss: 4.943033695220947, Learning Rate: 0.0001)\r\n",
      "Step... (68 / 849 | Loss: 4.671104431152344, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   8%|█▊                    | 68/849 [19:17<3:44:17, 17.23s/it]Step... (68 / 849 | Loss: 4.649686813354492, Learning Rate: 0.0001)\r\n",
      "Step... (70 / 849 | Loss: 4.797696590423584, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   8%|█▊                    | 70/849 [19:51<3:44:23, 17.28s/it]Step... (70 / 849 | Loss: 5.017622947692871, Learning Rate: 0.0001)\r\n",
      "Step... (72 / 849 | Loss: 4.925110340118408, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   8%|█▊                    | 72/849 [20:24<3:35:21, 16.63s/it]Step... (72 / 849 | Loss: 4.976876258850098, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   9%|█▉                    | 74/849 [21:00<3:42:06, 17.20s/it]Step... (74 / 849 | Loss: 5.234046936035156, Learning Rate: 0.0001)\r\n",
      "Step... (74 / 849 | Loss: 4.824843406677246, Learning Rate: 0.0001)\r\n",
      "Step... (76 / 849 | Loss: 4.932360649108887, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   9%|█▉                    | 76/849 [21:35<3:47:54, 17.69s/it]Step... (76 / 849 | Loss: 4.6950883865356445, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   9%|██                    | 78/849 [22:07<3:37:18, 16.91s/it]Step... (78 / 849 | Loss: 4.683427810668945, Learning Rate: 0.0001)\r\n",
      "Step... (78 / 849 | Loss: 4.838442325592041, Learning Rate: 0.0001)\r\n",
      "Step... (80 / 849 | Loss: 4.688381671905518, Learning Rate: 0.0001)\r\n",
      "Train steps ... :   9%|██                    | 80/849 [22:41<3:35:23, 16.80s/it]Step... (80 / 849 | Loss: 5.022634506225586, Learning Rate: 0.0001)\r\n",
      "Step... (82 / 849 | Loss: 4.663265705108643, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  10%|██                    | 82/849 [23:16<3:40:02, 17.21s/it]Step... (82 / 849 | Loss: 4.841357707977295, Learning Rate: 0.0001)\r\n",
      "Step... (84 / 849 | Loss: 5.428979873657227, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  10%|██▏                   | 84/849 [23:50<3:38:41, 17.15s/it]Step... (84 / 849 | Loss: 4.6862030029296875, Learning Rate: 0.0001)\r\n",
      "Step... (86 / 849 | Loss: 4.67625093460083, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  10%|██▏                   | 86/849 [24:22<3:28:07, 16.37s/it]Step... (86 / 849 | Loss: 4.76801061630249, Learning Rate: 0.0001)\r\n",
      "Step... (88 / 849 | Loss: 4.724405765533447, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  10%|██▎                   | 88/849 [24:58<3:38:28, 17.22s/it]Step... (88 / 849 | Loss: 4.822447776794434, Learning Rate: 0.0001)\r\n",
      "Step... (90 / 849 | Loss: 4.704946994781494, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  11%|██▎                   | 90/849 [25:33<3:41:10, 17.48s/it]Step... (90 / 849 | Loss: 4.679778575897217, Learning Rate: 0.0001)\r\n",
      "Step... (92 / 849 | Loss: 5.004372596740723, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  11%|██▍                   | 92/849 [26:07<3:35:07, 17.05s/it]Step... (92 / 849 | Loss: 4.63994836807251, Learning Rate: 0.0001)\r\n",
      "Step... (94 / 849 | Loss: 4.609230041503906, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  11%|██▍                   | 94/849 [26:39<3:29:52, 16.68s/it]Step... (94 / 849 | Loss: 4.898248672485352, Learning Rate: 0.0001)\r\n",
      "Step... (96 / 849 | Loss: 4.753488063812256, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  11%|██▍                   | 96/849 [27:15<3:35:56, 17.21s/it]Step... (96 / 849 | Loss: 4.94833517074585, Learning Rate: 0.0001)\r\n",
      "Step... (98 / 849 | Loss: 4.783133506774902, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  12%|██▌                   | 98/849 [27:49<3:34:23, 17.13s/it]Step... (98 / 849 | Loss: 4.634994983673096, Learning Rate: 0.0001)\r\n",
      "Step... (100 / 849 | Loss: 4.9516191482543945, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  12%|██▍                  | 100/849 [28:20<3:25:01, 16.42s/it]Step... (100 / 849 | Loss: 5.179518699645996, Learning Rate: 0.0001)\r\n",
      "Step... (102 / 849 | Loss: 4.975218296051025, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  12%|██▌                  | 102/849 [28:56<3:31:03, 16.95s/it]Step... (102 / 849 | Loss: 4.6111345291137695, Learning Rate: 0.0001)\r\n",
      "Step... (104 / 849 | Loss: 4.666359901428223, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  12%|██▌                  | 104/849 [29:32<3:38:07, 17.57s/it]Step... (104 / 849 | Loss: 5.035711765289307, Learning Rate: 0.0001)\r\n",
      "Step... (106 / 849 | Loss: 4.882554054260254, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  12%|██▌                  | 106/849 [30:06<3:33:55, 17.27s/it]Step... (106 / 849 | Loss: 4.761587619781494, Learning Rate: 0.0001)\r\n",
      "Step... (108 / 849 | Loss: 4.997616291046143, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  13%|██▋                  | 108/849 [30:39<3:28:57, 16.92s/it]Step... (108 / 849 | Loss: 4.866532802581787, Learning Rate: 0.0001)\r\n",
      "Step... (110 / 849 | Loss: 4.736485958099365, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  13%|██▋                  | 110/849 [31:15<3:33:20, 17.32s/it]Step... (110 / 849 | Loss: 5.137353897094727, Learning Rate: 0.0001)\r\n",
      "Step... (112 / 849 | Loss: 4.820034503936768, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  13%|██▊                  | 112/849 [31:48<3:31:30, 17.22s/it]Step... (112 / 849 | Loss: 5.0588483810424805, Learning Rate: 0.0001)\r\n",
      "Step... (114 / 849 | Loss: 5.190577507019043, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  13%|██▊                  | 114/849 [32:21<3:24:31, 16.70s/it]Step... (114 / 849 | Loss: 4.895992755889893, Learning Rate: 0.0001)\r\n",
      "Step... (116 / 849 | Loss: 4.845898628234863, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  14%|██▊                  | 116/849 [32:55<3:25:40, 16.84s/it]Step... (116 / 849 | Loss: 4.5830254554748535, Learning Rate: 0.0001)\r\n",
      "Step... (118 / 849 | Loss: 4.883040904998779, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  14%|██▉                  | 118/849 [33:30<3:28:51, 17.14s/it]Step... (118 / 849 | Loss: 4.665030479431152, Learning Rate: 0.0001)\r\n",
      "Step... (120 / 849 | Loss: 4.701745510101318, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  14%|██▉                  | 120/849 [34:04<3:30:10, 17.30s/it]Step... (120 / 849 | Loss: 4.480609893798828, Learning Rate: 0.0001)\r\n",
      "Step... (122 / 849 | Loss: 4.816168308258057, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  14%|███                  | 122/849 [34:36<3:20:34, 16.55s/it]Step... (122 / 849 | Loss: 4.48339319229126, Learning Rate: 0.0001)\r\n",
      "Step... (124 / 849 | Loss: 4.748940467834473, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  15%|███                  | 124/849 [35:12<3:27:53, 17.20s/it]Step... (124 / 849 | Loss: 4.71050500869751, Learning Rate: 0.0001)\r\n",
      "Step... (126 / 849 | Loss: 5.359053134918213, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  15%|███                  | 126/849 [35:48<3:33:18, 17.70s/it]Step... (126 / 849 | Loss: 5.223339557647705, Learning Rate: 0.0001)\r\n",
      "Step... (128 / 849 | Loss: 5.446550369262695, Learning Rate: 0.0001)\r\n",
      "Step... (128 / 849 | Loss: 4.779707908630371, Learning Rate: 0.0001)\r\n",
      "Step... (130 / 849 | Loss: 4.646399021148682, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  15%|███▏                 | 130/849 [36:54<3:20:28, 16.73s/it]Step... (130 / 849 | Loss: 4.9433159828186035, Learning Rate: 0.0001)\r\n",
      "Step... (132 / 849 | Loss: 4.798180103302002, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  16%|███▎                 | 132/849 [37:28<3:21:26, 16.86s/it]Step... (132 / 849 | Loss: 4.649153232574463, Learning Rate: 0.0001)\r\n",
      "Step... (134 / 849 | Loss: 4.718898296356201, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  16%|███▎                 | 134/849 [38:02<3:24:12, 17.14s/it]Step... (134 / 849 | Loss: 4.849207401275635, Learning Rate: 0.0001)\r\n",
      "Step... (136 / 849 | Loss: 4.898336887359619, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  16%|███▎                 | 136/849 [38:34<3:15:57, 16.49s/it]Step... (136 / 849 | Loss: 4.950578689575195, Learning Rate: 0.0001)\r\n",
      "Step... (138 / 849 | Loss: 4.78698205947876, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  16%|███▍                 | 138/849 [39:10<3:24:01, 17.22s/it]Step... (138 / 849 | Loss: 5.122828006744385, Learning Rate: 0.0001)\r\n",
      "Step... (140 / 849 | Loss: 5.200767993927002, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  16%|███▍                 | 140/849 [39:46<3:27:12, 17.53s/it]Step... (140 / 849 | Loss: 4.771730899810791, Learning Rate: 0.0001)\r\n",
      "Step... (142 / 849 | Loss: 4.937921047210693, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  17%|███▌                 | 142/849 [40:19<3:22:46, 17.21s/it]Step... (142 / 849 | Loss: 4.7836761474609375, Learning Rate: 0.0001)\r\n",
      "Step... (144 / 849 | Loss: 4.732357025146484, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  17%|███▌                 | 144/849 [40:52<3:17:17, 16.79s/it]Step... (144 / 849 | Loss: 4.7555952072143555, Learning Rate: 0.0001)\r\n",
      "Step... (146 / 849 | Loss: 5.076054573059082, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  17%|███▌                 | 146/849 [41:27<3:18:50, 16.97s/it]Step... (146 / 849 | Loss: 4.467878818511963, Learning Rate: 0.0001)\r\n",
      "Step... (148 / 849 | Loss: 4.662023544311523, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  17%|███▋                 | 148/849 [42:01<3:22:13, 17.31s/it]Step... (148 / 849 | Loss: 5.124996662139893, Learning Rate: 0.0001)\r\n",
      "Step... (150 / 849 | Loss: 4.834681034088135, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  18%|███▋                 | 150/849 [42:33<3:12:25, 16.52s/it]Step... (150 / 849 | Loss: 5.084778785705566, Learning Rate: 0.0001)\r\n",
      "Step... (152 / 849 | Loss: 4.781002998352051, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  18%|███▊                 | 152/849 [43:09<3:18:08, 17.06s/it]Step... (152 / 849 | Loss: 5.042576313018799, Learning Rate: 0.0001)\r\n",
      "Step... (154 / 849 | Loss: 4.8745646476745605, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  18%|███▊                 | 154/849 [43:44<3:20:11, 17.28s/it]Step... (154 / 849 | Loss: 4.6802659034729, Learning Rate: 0.0001)\r\n",
      "Step... (156 / 849 | Loss: 4.92653751373291, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  18%|███▊                 | 156/849 [44:17<3:17:03, 17.06s/it]Step... (156 / 849 | Loss: 4.969577312469482, Learning Rate: 0.0001)\r\n",
      "Step... (158 / 849 | Loss: 4.909615516662598, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  19%|███▉                 | 158/849 [44:50<3:10:39, 16.55s/it]Step... (158 / 849 | Loss: 4.817335605621338, Learning Rate: 0.0001)\r\n",
      "Step... (160 / 849 | Loss: 4.6894965171813965, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  19%|███▉                 | 160/849 [45:25<3:15:42, 17.04s/it]Step... (160 / 849 | Loss: 4.565984725952148, Learning Rate: 0.0001)\r\n",
      "Step... (162 / 849 | Loss: 4.7970685958862305, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  19%|████                 | 162/849 [45:59<3:15:37, 17.09s/it]Step... (162 / 849 | Loss: 5.026390552520752, Learning Rate: 0.0001)\r\n",
      "Step... (164 / 849 | Loss: 5.167064666748047, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  19%|████                 | 164/849 [46:31<3:10:18, 16.67s/it]Step... (164 / 849 | Loss: 5.049484729766846, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  20%|████                 | 166/849 [47:05<3:11:49, 16.85s/it]Step... (166 / 849 | Loss: 4.628719806671143, Learning Rate: 0.0001)\r\n",
      "Step... (166 / 849 | Loss: 4.655500411987305, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  20%|████▏                | 168/849 [47:41<3:16:25, 17.31s/it]Step... (168 / 849 | Loss: 4.7036638259887695, Learning Rate: 0.0001)\r\n",
      "Step... (168 / 849 | Loss: 4.656780242919922, Learning Rate: 0.0001)\r\n",
      "Step... (170 / 849 | Loss: 4.822336673736572, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  20%|████▏                | 170/849 [48:15<3:15:29, 17.28s/it]Step... (170 / 849 | Loss: 4.9760918617248535, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  20%|████▎                | 172/849 [48:47<3:06:20, 16.52s/it]Step... (172 / 849 | Loss: 4.612796783447266, Learning Rate: 0.0001)\r\n",
      "Step... (172 / 849 | Loss: 4.641855239868164, Learning Rate: 0.0001)\r\n",
      "Step... (174 / 849 | Loss: 4.655486106872559, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  20%|████▎                | 174/849 [49:23<3:12:00, 17.07s/it]Step... (174 / 849 | Loss: 4.978700637817383, Learning Rate: 0.0001)\r\n",
      "Step... (176 / 849 | Loss: 4.710996150970459, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  21%|████▎                | 176/849 [49:58<3:16:52, 17.55s/it]Step... (176 / 849 | Loss: 4.998319149017334, Learning Rate: 0.0001)\r\n",
      "Step... (178 / 849 | Loss: 4.810979843139648, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  21%|████▍                | 178/849 [50:31<3:09:53, 16.98s/it]Step... (178 / 849 | Loss: 5.061830043792725, Learning Rate: 0.0001)\r\n",
      "Step... (180 / 849 | Loss: 4.505115032196045, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  21%|████▍                | 180/849 [51:05<3:08:27, 16.90s/it]Step... (180 / 849 | Loss: 4.822455883026123, Learning Rate: 0.0001)\r\n",
      "Step... (182 / 849 | Loss: 4.524715423583984, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  21%|████▌                | 182/849 [51:40<3:11:27, 17.22s/it]Step... (182 / 849 | Loss: 4.534769535064697, Learning Rate: 0.0001)\r\n",
      "Step... (184 / 849 | Loss: 4.670424938201904, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  22%|████▌                | 184/849 [52:13<3:10:34, 17.19s/it]Step... (184 / 849 | Loss: 4.896585464477539, Learning Rate: 0.0001)\r\n",
      "Step... (186 / 849 | Loss: 4.782938480377197, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  22%|████▌                | 186/849 [52:45<3:01:43, 16.45s/it]Step... (186 / 849 | Loss: 4.5677947998046875, Learning Rate: 0.0001)\r\n",
      "Step... (188 / 849 | Loss: 4.711831569671631, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  22%|████▋                | 188/849 [53:21<3:06:51, 16.96s/it]Step... (188 / 849 | Loss: 4.674766540527344, Learning Rate: 0.0001)\r\n",
      "Step... (190 / 849 | Loss: 4.703639984130859, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  22%|████▋                | 190/849 [53:56<3:11:09, 17.40s/it]Step... (190 / 849 | Loss: 4.556387424468994, Learning Rate: 0.0001)\r\n",
      "Step... (192 / 849 | Loss: 4.924307346343994, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  23%|████▋                | 192/849 [54:29<3:07:06, 17.09s/it]Step... (192 / 849 | Loss: 4.598020076751709, Learning Rate: 0.0001)\r\n",
      "Step... (194 / 849 | Loss: 4.677659034729004, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  23%|████▊                | 194/849 [55:02<3:02:03, 16.68s/it]Step... (194 / 849 | Loss: 4.7643961906433105, Learning Rate: 0.0001)\r\n",
      "Step... (196 / 849 | Loss: 4.511531352996826, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  23%|████▊                | 196/849 [55:37<3:05:18, 17.03s/it]Step... (196 / 849 | Loss: 4.5338921546936035, Learning Rate: 0.0001)\r\n",
      "Step... (198 / 849 | Loss: 4.557508945465088, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  23%|████▉                | 198/849 [56:11<3:07:10, 17.25s/it]Step... (198 / 849 | Loss: 4.9881110191345215, Learning Rate: 0.0001)\r\n",
      "Step... (200 / 849 | Loss: 4.689870834350586, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  24%|████▉                | 200/849 [56:43<2:56:51, 16.35s/it]Step... (200 / 849 | Loss: 4.583688735961914, Learning Rate: 0.0001)\r\n",
      "Step... (202 / 849 | Loss: 4.472494602203369, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  24%|████▉                | 202/849 [57:18<3:02:06, 16.89s/it]Step... (202 / 849 | Loss: 5.241312026977539, Learning Rate: 0.0001)\r\n",
      "Step... (204 / 849 | Loss: 4.88404655456543, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  24%|█████                | 204/849 [57:53<3:05:34, 17.26s/it]Step... (204 / 849 | Loss: 4.930953025817871, Learning Rate: 0.0001)\r\n",
      "Step... (206 / 849 | Loss: 4.648529052734375, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  24%|█████                | 206/849 [58:27<3:03:26, 17.12s/it]Step... (206 / 849 | Loss: 4.8170599937438965, Learning Rate: 0.0001)\r\n",
      "Step... (208 / 849 | Loss: 5.052475929260254, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  24%|█████▏               | 208/849 [58:59<2:55:29, 16.43s/it]Step... (208 / 849 | Loss: 4.522161483764648, Learning Rate: 0.0001)\r\n",
      "Step... (210 / 849 | Loss: 5.053701877593994, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  25%|█████▏               | 210/849 [59:34<3:01:01, 17.00s/it]Step... (210 / 849 | Loss: 4.655171871185303, Learning Rate: 0.0001)\r\n",
      "Step... (212 / 849 | Loss: 5.1438798904418945, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  25%|████▋              | 212/849 [1:00:08<3:01:35, 17.11s/it]Step... (212 / 849 | Loss: 4.548171520233154, Learning Rate: 0.0001)\r\n",
      "Step... (214 / 849 | Loss: 5.152778625488281, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  25%|████▊              | 214/849 [1:00:39<2:53:55, 16.43s/it]Step... (214 / 849 | Loss: 4.657505035400391, Learning Rate: 0.0001)\r\n",
      "Step... (216 / 849 | Loss: 4.710305213928223, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  25%|████▊              | 216/849 [1:01:13<2:54:16, 16.52s/it]Step... (216 / 849 | Loss: 4.550594329833984, Learning Rate: 0.0001)\r\n",
      "Step... (218 / 849 | Loss: 4.542385578155518, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  26%|████▉              | 218/849 [1:01:47<2:56:18, 16.76s/it]Step... (218 / 849 | Loss: 5.115503787994385, Learning Rate: 0.0001)\r\n",
      "Step... (220 / 849 | Loss: 4.859785556793213, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  26%|████▉              | 220/849 [1:02:21<2:58:15, 17.00s/it]Step... (220 / 849 | Loss: 4.834929466247559, Learning Rate: 0.0001)\r\n",
      "Step... (222 / 849 | Loss: 4.430667877197266, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  26%|████▉              | 222/849 [1:02:52<2:50:22, 16.30s/it]Step... (222 / 849 | Loss: 4.471049785614014, Learning Rate: 0.0001)\r\n",
      "Step... (224 / 849 | Loss: 4.788700103759766, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  26%|█████              | 224/849 [1:03:27<2:55:01, 16.80s/it]Step... (224 / 849 | Loss: 4.6516032218933105, Learning Rate: 0.0001)\r\n",
      "Step... (226 / 849 | Loss: 4.8571577072143555, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  27%|█████              | 226/849 [1:04:02<2:58:54, 17.23s/it]Step... (226 / 849 | Loss: 4.719632625579834, Learning Rate: 0.0001)\r\n",
      "Step... (228 / 849 | Loss: 5.068331241607666, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  27%|█████              | 228/849 [1:04:35<2:56:59, 17.10s/it]Step... (228 / 849 | Loss: 4.884277820587158, Learning Rate: 0.0001)\r\n",
      "Step... (230 / 849 | Loss: 4.743539333343506, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  27%|█████▏             | 230/849 [1:05:09<2:54:25, 16.91s/it]Step... (230 / 849 | Loss: 5.013459205627441, Learning Rate: 0.0001)\r\n",
      "Step... (232 / 849 | Loss: 4.6701788902282715, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  27%|█████▏             | 232/849 [1:05:44<2:55:25, 17.06s/it]Step... (232 / 849 | Loss: 4.673776626586914, Learning Rate: 0.0001)\r\n",
      "Step... (234 / 849 | Loss: 4.99566650390625, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  28%|█████▏             | 234/849 [1:06:18<2:58:05, 17.37s/it]Step... (234 / 849 | Loss: 4.578216075897217, Learning Rate: 0.0001)\r\n",
      "Step... (236 / 849 | Loss: 5.073813438415527, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  28%|█████▎             | 236/849 [1:06:50<2:49:17, 16.57s/it]Step... (236 / 849 | Loss: 4.857611656188965, Learning Rate: 0.0001)\r\n",
      "Step... (238 / 849 | Loss: 4.625672340393066, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  28%|█████▎             | 238/849 [1:07:26<2:52:37, 16.95s/it]Step... (238 / 849 | Loss: 4.6701250076293945, Learning Rate: 0.0001)\r\n",
      "Step... (240 / 849 | Loss: 4.449700355529785, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  28%|█████▎             | 240/849 [1:08:01<2:57:26, 17.48s/it]Step... (240 / 849 | Loss: 5.005715370178223, Learning Rate: 0.0001)\r\n",
      "Step... (242 / 849 | Loss: 4.851649284362793, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  29%|█████▍             | 242/849 [1:08:35<2:55:45, 17.37s/it]Step... (242 / 849 | Loss: 5.151676177978516, Learning Rate: 0.0001)\r\n",
      "Step... (244 / 849 | Loss: 4.573947429656982, Learning Rate: 0.0001)\r\n",
      "Step... (244 / 849 | Loss: 4.663757801055908, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  29%|█████▌             | 246/849 [1:09:43<2:52:10, 17.13s/it]Step... (246 / 849 | Loss: 4.673319339752197, Learning Rate: 0.0001)\r\n",
      "Step... (246 / 849 | Loss: 4.629720687866211, Learning Rate: 0.0001)\r\n",
      "Step... (248 / 849 | Loss: 4.80064582824707, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  29%|█████▌             | 248/849 [1:10:18<2:55:24, 17.51s/it]Step... (248 / 849 | Loss: 4.742506980895996, Learning Rate: 0.0001)\r\n",
      "Step... (250 / 849 | Loss: 4.804941654205322, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  29%|█████▌             | 250/849 [1:10:50<2:46:04, 16.64s/it]Step... (250 / 849 | Loss: 4.740948677062988, Learning Rate: 0.0001)\r\n",
      "Step... (252 / 849 | Loss: 4.684383869171143, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  30%|█████▋             | 252/849 [1:11:25<2:48:29, 16.93s/it]Step... (252 / 849 | Loss: 4.930948257446289, Learning Rate: 0.0001)\r\n",
      "Step... (254 / 849 | Loss: 4.791845798492432, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  30%|█████▋             | 254/849 [1:12:01<2:54:55, 17.64s/it]Step... (254 / 849 | Loss: 4.573068141937256, Learning Rate: 0.0001)\r\n",
      "Step... (256 / 849 | Loss: 4.912681579589844, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  30%|█████▋             | 256/849 [1:12:36<2:53:46, 17.58s/it]Step... (256 / 849 | Loss: 5.174662113189697, Learning Rate: 0.0001)\r\n",
      "Step... (258 / 849 | Loss: 4.658069610595703, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  30%|█████▊             | 258/849 [1:13:09<2:46:37, 16.92s/it]Step... (258 / 849 | Loss: 4.960941791534424, Learning Rate: 0.0001)\r\n",
      "Step... (260 / 849 | Loss: 4.9404473304748535, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  31%|█████▊             | 260/849 [1:13:45<2:49:31, 17.27s/it]Step... (260 / 849 | Loss: 5.041767597198486, Learning Rate: 0.0001)\r\n",
      "Step... (262 / 849 | Loss: 4.725522518157959, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  31%|█████▊             | 262/849 [1:14:19<2:50:17, 17.41s/it]Step... (262 / 849 | Loss: 4.8231706619262695, Learning Rate: 0.0001)\r\n",
      "Step... (264 / 849 | Loss: 5.170633792877197, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  31%|█████▉             | 264/849 [1:14:51<2:42:20, 16.65s/it]Step... (264 / 849 | Loss: 5.006189823150635, Learning Rate: 0.0001)\r\n",
      "Step... (266 / 849 | Loss: 4.908897399902344, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  31%|█████▉             | 266/849 [1:15:25<2:43:27, 16.82s/it]Step... (266 / 849 | Loss: 4.964395999908447, Learning Rate: 0.0001)\r\n",
      "Step... (268 / 849 | Loss: 4.633501052856445, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  32%|█████▉             | 268/849 [1:16:01<2:47:58, 17.35s/it]Step... (268 / 849 | Loss: 4.695786952972412, Learning Rate: 0.0001)\r\n",
      "Step... (270 / 849 | Loss: 4.656811714172363, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  32%|██████             | 270/849 [1:16:36<2:48:49, 17.50s/it]Step... (270 / 849 | Loss: 4.998565196990967, Learning Rate: 0.0001)\r\n",
      "Step... (272 / 849 | Loss: 4.60422945022583, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  32%|██████             | 272/849 [1:17:08<2:41:25, 16.79s/it]Step... (272 / 849 | Loss: 4.409970760345459, Learning Rate: 0.0001)\r\n",
      "Step... (274 / 849 | Loss: 4.878278732299805, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  32%|██████▏            | 274/849 [1:17:44<2:44:53, 17.21s/it]Step... (274 / 849 | Loss: 4.610642433166504, Learning Rate: 0.0001)\r\n",
      "Step... (276 / 849 | Loss: 4.8108110427856445, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  33%|██████▏            | 276/849 [1:18:19<2:47:15, 17.51s/it]Step... (276 / 849 | Loss: 4.875751495361328, Learning Rate: 0.0001)\r\n",
      "Step... (278 / 849 | Loss: 4.77837610244751, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  33%|██████▏            | 278/849 [1:18:52<2:40:59, 16.92s/it]Step... (278 / 849 | Loss: 5.026259422302246, Learning Rate: 0.0001)\r\n",
      "Step... (280 / 849 | Loss: 4.774720668792725, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  33%|██████▎            | 280/849 [1:19:25<2:37:52, 16.65s/it]Step... (280 / 849 | Loss: 4.791360855102539, Learning Rate: 0.0001)\r\n",
      "Step... (282 / 849 | Loss: 5.081188678741455, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  33%|██████▎            | 282/849 [1:20:00<2:42:09, 17.16s/it]Step... (282 / 849 | Loss: 4.7441840171813965, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  33%|██████▎            | 283/849 [1:20:16<2:37:00, 16.64s/it]\r\n",
      "\r\n",
      "Evaluating - Inference ...:   0%|                         | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "Evaluating - Inference ...: 100%|█████████████████| 1/1 [00:01<00:00,  1.56s/it]\r\n",
      "\r\n",
      "\r\n",
      "Evaluating - Generation ...:   0%|                        | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "Evaluating - Generation ...: 100%|████████████████| 1/1 [01:12<00:00, 72.76s/it]\r\n",
      "\r\n",
      "config.json: 100%|█████████████████████████████| 635/635 [00:00<00:00, 3.40MB/s]\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\r\n",
      "text_config is None. Initializing the ClapTextConfig with default values.\r\n",
      "audio_config is None. initializing the ClapAudioConfig with default values.\r\n",
      "Model config ClapConfig {\r\n",
      "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"ClapModel\"\r\n",
      "  ],\r\n",
      "  \"audio_config\": {\r\n",
      "    \"depths\": [\r\n",
      "      2,\r\n",
      "      2,\r\n",
      "      12,\r\n",
      "      2\r\n",
      "    ],\r\n",
      "    \"hidden_size\": 1024,\r\n",
      "    \"model_type\": \"clap_audio_model\",\r\n",
      "    \"patch_embeds_hidden_size\": 128\r\n",
      "  },\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"logit_scale_init_value\": 14.285714285714285,\r\n",
      "  \"model_type\": \"clap\",\r\n",
      "  \"num_hidden_layers\": 16,\r\n",
      "  \"projection_dim\": 512,\r\n",
      "  \"projection_hidden_act\": \"relu\",\r\n",
      "  \"text_config\": {\r\n",
      "    \"model_type\": \"clap_text_model\"\r\n",
      "  },\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\"\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "pytorch_model.bin:   0%|                             | 0.00/776M [00:00<?, ?B/s]\u001b[A\r\n",
      "pytorch_model.bin:   1%|▎                   | 10.5M/776M [00:00<00:19, 38.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   3%|▌                   | 21.0M/776M [00:00<00:18, 40.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   4%|▊                   | 31.5M/776M [00:00<00:17, 41.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   5%|█                   | 41.9M/776M [00:01<00:17, 41.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   7%|█▎                  | 52.4M/776M [00:01<00:17, 41.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   8%|█▌                  | 62.9M/776M [00:01<00:17, 40.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   9%|█▉                  | 73.4M/776M [00:01<00:17, 40.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  11%|██▏                 | 83.9M/776M [00:02<00:17, 40.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  12%|██▍                 | 94.4M/776M [00:02<00:16, 41.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  14%|██▊                  | 105M/776M [00:02<00:16, 41.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  15%|███                  | 115M/776M [00:02<00:16, 40.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  16%|███▍                 | 126M/776M [00:03<00:15, 41.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  18%|███▋                 | 136M/776M [00:03<00:15, 40.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  19%|███▉                 | 147M/776M [00:03<00:15, 40.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  20%|████▎                | 157M/776M [00:03<00:15, 40.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  22%|████▌                | 168M/776M [00:04<00:15, 39.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  23%|████▊                | 178M/776M [00:04<00:17, 34.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  24%|█████                | 189M/776M [00:04<00:17, 33.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  26%|█████▍               | 199M/776M [00:05<00:17, 33.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  27%|█████▋               | 210M/776M [00:05<00:16, 35.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  28%|█████▉               | 220M/776M [00:05<00:15, 36.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  30%|██████▏              | 231M/776M [00:06<00:16, 34.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  31%|██████▌              | 241M/776M [00:06<00:15, 34.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  32%|██████▊              | 252M/776M [00:06<00:14, 36.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  34%|███████              | 262M/776M [00:06<00:13, 37.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  35%|███████▎             | 273M/776M [00:07<00:13, 38.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  36%|███████▋             | 283M/776M [00:07<00:12, 38.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  38%|███████▉             | 294M/776M [00:07<00:12, 39.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  39%|████████▏            | 304M/776M [00:07<00:11, 39.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  41%|████████▌            | 315M/776M [00:08<00:11, 40.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  42%|████████▊            | 325M/776M [00:08<00:11, 40.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  43%|█████████            | 336M/776M [00:08<00:10, 40.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  45%|█████████▎           | 346M/776M [00:08<00:10, 40.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  46%|█████████▋           | 357M/776M [00:09<00:10, 40.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  47%|█████████▉           | 367M/776M [00:09<00:10, 38.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  49%|██████████▏          | 377M/776M [00:09<00:10, 39.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  50%|██████████▍          | 388M/776M [00:10<00:10, 38.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  51%|██████████▊          | 398M/776M [00:10<00:09, 39.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  53%|███████████          | 409M/776M [00:10<00:09, 39.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  54%|███████████▎         | 419M/776M [00:10<00:09, 39.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  55%|███████████▋         | 430M/776M [00:11<00:09, 37.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  57%|███████████▉         | 440M/776M [00:11<00:09, 36.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  58%|████████████▏        | 451M/776M [00:11<00:11, 29.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  59%|████████████▍        | 461M/776M [00:12<00:10, 30.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  61%|████████████▊        | 472M/776M [00:12<00:09, 33.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  62%|█████████████        | 482M/776M [00:12<00:08, 35.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  63%|█████████████▎       | 493M/776M [00:13<00:07, 36.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  65%|█████████████▌       | 503M/776M [00:13<00:07, 37.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  66%|█████████████▉       | 514M/776M [00:13<00:06, 37.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  68%|██████████████▏      | 524M/776M [00:13<00:06, 38.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  69%|██████████████▍      | 535M/776M [00:14<00:06, 38.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  70%|██████████████▋      | 545M/776M [00:14<00:05, 39.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  72%|███████████████      | 556M/776M [00:14<00:05, 39.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  73%|███████████████▎     | 566M/776M [00:14<00:05, 39.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  74%|███████████████▌     | 577M/776M [00:15<00:04, 40.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  76%|███████████████▉     | 587M/776M [00:15<00:04, 40.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  77%|████████████████▏    | 598M/776M [00:15<00:04, 40.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  78%|████████████████▍    | 608M/776M [00:15<00:04, 41.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  80%|████████████████▋    | 619M/776M [00:16<00:03, 41.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  81%|█████████████████    | 629M/776M [00:16<00:03, 41.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  82%|█████████████████▎   | 640M/776M [00:16<00:03, 41.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  84%|█████████████████▌   | 650M/776M [00:16<00:03, 40.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  85%|█████████████████▊   | 661M/776M [00:17<00:02, 40.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  86%|██████████████████▏  | 671M/776M [00:17<00:02, 41.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  88%|██████████████████▍  | 682M/776M [00:17<00:02, 41.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  89%|██████████████████▋  | 692M/776M [00:17<00:02, 41.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  90%|███████████████████  | 703M/776M [00:18<00:01, 42.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  92%|███████████████████▎ | 713M/776M [00:18<00:01, 42.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  93%|███████████████████▌ | 724M/776M [00:18<00:01, 42.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  95%|███████████████████▊ | 734M/776M [00:18<00:01, 42.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  96%|████████████████████▏| 744M/776M [00:19<00:00, 41.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  97%|████████████████████▍| 755M/776M [00:19<00:00, 40.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  99%|████████████████████▋| 765M/776M [00:19<00:00, 41.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin: 100%|█████████████████████| 776M/776M [00:19<00:00, 38.9MB/s]\r\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\r\n",
      "Attempting to create safetensors variant\r\n",
      "All model checkpoint weights were used when initializing ClapModel.\r\n",
      "\r\n",
      "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\r\n",
      "Attempting to convert .bin model on the fly to safetensors.\r\n",
      "\r\n",
      "preprocessor_config.json: 100%|████████████████| 541/541 [00:00<00:00, 3.09MB/s]\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\r\n",
      "Feature extractor ClapFeatureExtractor {\r\n",
      "  \"chunk_length_s\": 10,\r\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\r\n",
      "  \"feature_size\": 64,\r\n",
      "  \"fft_window_size\": 1024,\r\n",
      "  \"frequency_max\": 14000,\r\n",
      "  \"frequency_min\": 50,\r\n",
      "  \"hop_length\": 480,\r\n",
      "  \"max_length_s\": 10,\r\n",
      "  \"n_fft\": 1024,\r\n",
      "  \"nb_frequency_bins\": 513,\r\n",
      "  \"nb_max_frames\": 1000,\r\n",
      "  \"nb_max_samples\": 480000,\r\n",
      "  \"padding\": \"repeatpad\",\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"ClapProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 48000,\r\n",
      "  \"top_db\": null,\r\n",
      "  \"truncation\": \"rand_trunc\"\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.36k/1.36k [00:00<00:00, 11.8MB/s]\r\n",
      "\r\n",
      "vocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 15.1MB/s]\r\n",
      "\r\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 40.3MB/s]\r\n",
      "\r\n",
      "tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:00<00:00, 29.3MB/s]\r\n",
      "\r\n",
      "special_tokens_map.json: 100%|█████████████████| 280/280 [00:00<00:00, 2.12MB/s]\r\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\r\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\r\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\r\n",
      "loading file added_tokens.json from cache at None\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\r\n",
      "Processor ClapProcessor:\r\n",
      "- feature_extractor: ClapFeatureExtractor {\r\n",
      "  \"chunk_length_s\": 10,\r\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\r\n",
      "  \"feature_size\": 64,\r\n",
      "  \"fft_window_size\": 1024,\r\n",
      "  \"frequency_max\": 14000,\r\n",
      "  \"frequency_min\": 50,\r\n",
      "  \"hop_length\": 480,\r\n",
      "  \"max_length_s\": 10,\r\n",
      "  \"n_fft\": 1024,\r\n",
      "  \"nb_frequency_bins\": 513,\r\n",
      "  \"nb_max_frames\": 1000,\r\n",
      "  \"nb_max_samples\": 480000,\r\n",
      "  \"padding\": \"repeatpad\",\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"ClapProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 48000,\r\n",
      "  \"top_db\": null,\r\n",
      "  \"truncation\": \"rand_trunc\"\r\n",
      "}\r\n",
      "\r\n",
      "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\r\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "  \"processor_class\": \"ClapProcessor\"\r\n",
      "}\r\n",
      "\r\n",
      "Downloading: \"https://download.pytorch.org/torchaudio/models/squim_objective_dns2020.pth\" to /root/.cache/torch/hub/checkpoints/squim_objective_dns2020.pth\r\n",
      "\r\n",
      "  0%|                                               | 0.00/28.2M [00:00<?, ?B/s]\u001b[A\r\n",
      " 10%|███▋                                  | 2.75M/28.2M [00:00<00:00, 28.0MB/s]\u001b[A\r\n",
      " 32%|████████████▎                         | 9.12M/28.2M [00:00<00:00, 50.2MB/s]\u001b[A\r\n",
      " 58%|██████████████████████▏               | 16.5M/28.2M [00:00<00:00, 56.6MB/s]\u001b[A\r\n",
      "100%|██████████████████████████████████████| 28.2M/28.2M [00:00<00:00, 50.0MB/s]\r\n",
      "\r\n",
      "Downloading builder script: 100%|██████████| 4.49k/4.49k [00:00<00:00, 12.4MB/s]\r\n",
      "\r\n",
      "config.json: 100%|█████████████████████████| 1.01k/1.01k [00:00<00:00, 6.63MB/s]\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "pytorch_model.bin:   0%|                            | 0.00/3.06G [00:00<?, ?B/s]\u001b[A\r\n",
      "pytorch_model.bin:   0%|                   | 10.5M/3.06G [00:00<00:40, 74.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   1%|▏                  | 31.5M/3.06G [00:00<00:51, 59.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   2%|▎                  | 52.4M/3.06G [00:00<00:48, 62.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   3%|▌                  | 83.9M/3.06G [00:01<00:40, 73.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   3%|▋                   | 105M/3.06G [00:01<00:53, 55.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   4%|▉                   | 136M/3.06G [00:02<00:42, 69.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   5%|█                   | 157M/3.06G [00:02<00:43, 67.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   6%|█▏                  | 189M/3.06G [00:02<00:38, 74.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   7%|█▎                  | 210M/3.06G [00:03<00:37, 75.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   8%|█▌                  | 241M/3.06G [00:03<00:44, 63.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   9%|█▋                  | 262M/3.06G [00:03<00:41, 66.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:   9%|█▊                  | 283M/3.06G [00:04<00:34, 80.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  10%|█▉                  | 304M/3.06G [00:04<00:46, 59.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  10%|██                  | 315M/3.06G [00:05<01:10, 39.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  11%|██▏                 | 325M/3.06G [00:05<01:01, 44.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  11%|██▎                 | 346M/3.06G [00:06<01:10, 38.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  12%|██▍                 | 367M/3.06G [00:06<01:04, 41.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  12%|██▍                 | 377M/3.06G [00:06<00:57, 46.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  13%|██▌                 | 398M/3.06G [00:06<00:50, 52.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  14%|██▋                 | 419M/3.06G [00:07<00:51, 51.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  14%|██▊                 | 430M/3.06G [00:07<00:51, 50.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  15%|██▉                 | 451M/3.06G [00:07<00:45, 57.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  15%|███                 | 472M/3.06G [00:08<00:55, 46.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  16%|███▎                | 503M/3.06G [00:09<00:48, 52.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  17%|███▍                | 524M/3.06G [00:09<00:46, 54.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  18%|███▌                | 545M/3.06G [00:09<00:37, 66.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  18%|███▋                | 556M/3.06G [00:09<00:40, 61.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  19%|███▊                | 577M/3.06G [00:10<00:39, 62.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  20%|███▉                | 598M/3.06G [00:10<00:38, 64.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  20%|███▉                | 608M/3.06G [00:10<00:35, 68.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  21%|████                | 629M/3.06G [00:10<00:33, 71.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  21%|████▎               | 650M/3.06G [00:10<00:26, 91.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  22%|████▍               | 671M/3.06G [00:11<00:37, 64.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  22%|████▍               | 682M/3.06G [00:11<00:50, 47.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  23%|████▌               | 703M/3.06G [00:11<00:36, 64.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  24%|████▋               | 724M/3.06G [00:12<00:43, 53.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  24%|████▊               | 734M/3.06G [00:13<01:00, 38.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  25%|████▉               | 755M/3.06G [00:13<00:45, 50.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  25%|█████               | 765M/3.06G [00:14<01:12, 31.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  26%|█████▏              | 786M/3.06G [00:14<01:09, 32.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  26%|█████▏              | 797M/3.06G [00:14<01:00, 37.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  27%|█████▎              | 818M/3.06G [00:15<01:08, 32.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  27%|█████▍              | 839M/3.06G [00:16<01:06, 33.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  28%|█████▋              | 870M/3.06G [00:16<00:54, 40.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  29%|█████▊              | 891M/3.06G [00:17<00:54, 39.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  30%|██████              | 923M/3.06G [00:18<00:52, 40.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  31%|██████▏             | 944M/3.06G [00:18<00:50, 41.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  32%|██████▍             | 975M/3.06G [00:19<00:46, 44.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  33%|██████▌             | 996M/3.06G [00:19<00:45, 45.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  34%|██████▍            | 1.03G/3.06G [00:20<00:41, 48.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  34%|██████▌            | 1.05G/3.06G [00:20<00:45, 43.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  35%|██████▋            | 1.08G/3.06G [00:21<00:42, 47.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  36%|██████▊            | 1.10G/3.06G [00:21<00:41, 46.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  37%|██████▉            | 1.12G/3.06G [00:21<00:33, 58.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  37%|███████            | 1.13G/3.06G [00:22<00:38, 49.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  38%|███████▏           | 1.15G/3.06G [00:22<00:37, 50.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  38%|███████▎           | 1.17G/3.06G [00:22<00:29, 64.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  39%|███████▎           | 1.18G/3.06G [00:23<00:41, 44.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  39%|███████▍           | 1.21G/3.06G [00:23<00:40, 45.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  40%|███████▋           | 1.23G/3.06G [00:23<00:31, 58.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  40%|███████▋           | 1.24G/3.06G [00:24<00:39, 45.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  41%|███████▊           | 1.26G/3.06G [00:24<00:41, 42.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  42%|███████▉           | 1.28G/3.06G [00:25<00:31, 56.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  42%|████████           | 1.29G/3.06G [00:25<00:35, 49.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  43%|████████▏          | 1.31G/3.06G [00:25<00:40, 43.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  44%|████████▎          | 1.33G/3.06G [00:26<00:30, 56.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  44%|████████▎          | 1.34G/3.06G [00:26<00:35, 48.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  45%|████████▍          | 1.36G/3.06G [00:26<00:34, 49.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  45%|████████▌          | 1.38G/3.06G [00:27<00:26, 63.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  46%|████████▋          | 1.39G/3.06G [00:27<00:34, 48.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  46%|████████▊          | 1.42G/3.06G [00:28<00:44, 36.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  47%|████████▉          | 1.44G/3.06G [00:28<00:32, 50.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  47%|████████▉          | 1.45G/3.06G [00:28<00:37, 42.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  48%|█████████▏         | 1.47G/3.06G [00:29<00:33, 46.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  49%|█████████▎         | 1.49G/3.06G [00:29<00:26, 60.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  49%|█████████▎         | 1.50G/3.06G [00:29<00:35, 43.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  50%|█████████▍         | 1.52G/3.06G [00:30<00:32, 47.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  50%|█████████▌         | 1.54G/3.06G [00:30<00:23, 63.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  51%|█████████▋         | 1.55G/3.06G [00:30<00:29, 50.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  51%|█████████▊         | 1.57G/3.06G [00:31<00:32, 45.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  52%|█████████▉         | 1.59G/3.06G [00:31<00:23, 61.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  53%|██████████         | 1.61G/3.06G [00:31<00:26, 54.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  53%|██████████         | 1.63G/3.06G [00:32<00:39, 36.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  54%|██████████▏        | 1.65G/3.06G [00:32<00:29, 48.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  54%|██████████▎        | 1.66G/3.06G [00:33<00:36, 38.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  55%|██████████▍        | 1.68G/3.06G [00:33<00:30, 45.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  56%|██████████▌        | 1.70G/3.06G [00:33<00:24, 55.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  56%|██████████▋        | 1.71G/3.06G [00:33<00:26, 51.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  57%|██████████▊        | 1.73G/3.06G [00:34<00:26, 50.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  58%|██████████▉        | 1.76G/3.06G [00:35<00:25, 51.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  58%|███████████        | 1.78G/3.06G [00:35<00:23, 53.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  59%|███████████▏       | 1.80G/3.06G [00:35<00:19, 64.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  59%|███████████▎       | 1.81G/3.06G [00:35<00:25, 49.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  60%|███████████▎       | 1.82G/3.06G [00:36<00:26, 46.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  60%|███████████▍       | 1.84G/3.06G [00:36<00:26, 45.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  61%|███████████▌       | 1.86G/3.06G [00:36<00:21, 56.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  61%|███████████▌       | 1.87G/3.06G [00:37<00:23, 49.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  62%|███████████▋       | 1.89G/3.06G [00:37<00:23, 49.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  62%|███████████▊       | 1.91G/3.06G [00:37<00:18, 63.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  63%|███████████▉       | 1.92G/3.06G [00:38<00:32, 34.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  63%|████████████       | 1.94G/3.06G [00:39<00:40, 27.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  64%|████████████▏      | 1.96G/3.06G [00:39<00:28, 38.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  65%|████████████▎      | 1.97G/3.06G [00:40<00:31, 34.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  65%|████████████▍      | 1.99G/3.06G [00:40<00:31, 34.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  66%|████████████▌      | 2.01G/3.06G [00:40<00:22, 46.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  66%|████████████▌      | 2.02G/3.06G [00:41<00:24, 41.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  67%|████████████▋      | 2.04G/3.06G [00:41<00:23, 42.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  68%|████████████▊      | 2.07G/3.06G [00:41<00:17, 56.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  68%|████████████▉      | 2.08G/3.06G [00:42<00:19, 50.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  69%|█████████████      | 2.10G/3.06G [00:42<00:20, 46.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  69%|█████████████▏     | 2.12G/3.06G [00:42<00:15, 59.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  70%|█████████████▏     | 2.13G/3.06G [00:43<00:19, 48.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  70%|█████████████▎     | 2.15G/3.06G [00:43<00:19, 45.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  71%|█████████████▍     | 2.17G/3.06G [00:43<00:15, 58.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  71%|█████████████▌     | 2.18G/3.06G [00:44<00:19, 44.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  72%|█████████████▋     | 2.20G/3.06G [00:44<00:20, 42.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  72%|█████████████▊     | 2.21G/3.06G [00:44<00:17, 46.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  73%|█████████████▊     | 2.22G/3.06G [00:45<00:16, 51.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  73%|█████████████▉     | 2.23G/3.06G [00:45<00:20, 40.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  74%|██████████████     | 2.25G/3.06G [00:45<00:16, 48.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  74%|██████████████▏    | 2.28G/3.06G [00:46<00:12, 61.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  75%|██████████████▏    | 2.29G/3.06G [00:46<00:16, 45.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  75%|██████████████▎    | 2.31G/3.06G [00:47<00:17, 43.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  76%|██████████████▍    | 2.32G/3.06G [00:47<00:16, 46.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  76%|██████████████▍    | 2.33G/3.06G [00:47<00:14, 51.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  77%|██████████████▌    | 2.34G/3.06G [00:47<00:18, 39.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  77%|██████████████▋    | 2.36G/3.06G [00:48<00:14, 47.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  78%|██████████████▊    | 2.38G/3.06G [00:48<00:14, 45.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  78%|██████████████▊    | 2.39G/3.06G [00:48<00:15, 41.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  79%|██████████████▉    | 2.41G/3.06G [00:49<00:15, 41.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  80%|███████████████▏   | 2.43G/3.06G [00:49<00:10, 57.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  80%|███████████████▏   | 2.44G/3.06G [00:49<00:12, 50.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  80%|███████████████▎   | 2.45G/3.06G [00:49<00:10, 56.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  81%|███████████████▎   | 2.46G/3.06G [00:50<00:12, 48.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  81%|███████████████▍   | 2.49G/3.06G [00:50<00:08, 63.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  82%|███████████████▌   | 2.50G/3.06G [00:50<00:11, 46.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  82%|███████████████▋   | 2.52G/3.06G [00:51<00:11, 45.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  83%|███████████████▊   | 2.54G/3.06G [00:51<00:08, 59.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  83%|███████████████▊   | 2.55G/3.06G [00:51<00:11, 43.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  84%|███████████████▉   | 2.57G/3.06G [00:52<00:10, 47.7MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  85%|████████████████   | 2.59G/3.06G [00:52<00:08, 52.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  85%|████████████████▏  | 2.60G/3.06G [00:53<00:09, 45.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  85%|████████████████▏  | 2.61G/3.06G [00:53<00:09, 46.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  86%|████████████████▎  | 2.62G/3.06G [00:53<00:11, 36.6MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  86%|████████████████▍  | 2.64G/3.06G [00:53<00:07, 52.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  87%|████████████████▍  | 2.65G/3.06G [00:54<00:08, 45.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  87%|████████████████▌  | 2.66G/3.06G [00:54<00:07, 53.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  88%|████████████████▋  | 2.67G/3.06G [00:54<00:10, 36.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  88%|████████████████▊  | 2.69G/3.06G [00:55<00:09, 36.5MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  89%|████████████████▊  | 2.71G/3.06G [00:55<00:10, 34.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  89%|████████████████▉  | 2.72G/3.06G [00:55<00:08, 40.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  89%|████████████████▉  | 2.73G/3.06G [00:56<00:10, 32.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  90%|█████████████████  | 2.75G/3.06G [00:57<00:10, 30.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  90%|█████████████████▏ | 2.76G/3.06G [00:57<00:10, 27.3MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  91%|█████████████████▏ | 2.77G/3.06G [00:57<00:08, 33.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  91%|█████████████████▎ | 2.78G/3.06G [00:58<00:09, 27.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  92%|█████████████████▍ | 2.80G/3.06G [00:58<00:06, 42.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  92%|█████████████████▍ | 2.81G/3.06G [00:59<00:08, 28.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  93%|█████████████████▌ | 2.83G/3.06G [00:59<00:07, 28.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  93%|█████████████████▋ | 2.85G/3.06G [01:00<00:04, 40.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  94%|█████████████████▊ | 2.86G/3.06G [01:00<00:06, 27.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  94%|█████████████████▉ | 2.88G/3.06G [01:01<00:05, 29.4MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  95%|██████████████████ | 2.90G/3.06G [01:01<00:03, 39.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  95%|██████████████████▏| 2.92G/3.06G [01:01<00:03, 40.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  96%|██████████████████▎| 2.94G/3.06G [01:02<00:03, 37.9MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  97%|██████████████████▍| 2.96G/3.06G [01:02<00:02, 48.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  97%|██████████████████▍| 2.97G/3.06G [01:03<00:02, 39.2MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  98%|██████████████████▌| 2.99G/3.06G [01:03<00:01, 37.8MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  98%|██████████████████▋| 3.01G/3.06G [01:04<00:00, 48.0MB/s]\u001b[A\r\n",
      "pytorch_model.bin:  99%|██████████████████▊| 3.02G/3.06G [01:04<00:00, 36.1MB/s]\u001b[A\r\n",
      "pytorch_model.bin: 100%|███████████████████| 3.06G/3.06G [01:05<00:00, 46.8MB/s]\r\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/pytorch_model.bin\r\n",
      "Attempting to create safetensors variant\r\n",
      "Generate config GenerationConfig {\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"use_cache\": false\r\n",
      "}\r\n",
      "\r\n",
      "Attempting to convert .bin model on the fly to safetensors.\r\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\r\n",
      "\r\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at cahya/whisper-medium-id.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\r\n",
      "Generation config file not found, using a generation config created from the model config.\r\n",
      "\r\n",
      "tokenizer_config.json: 100%|███████████████████| 832/832 [00:00<00:00, 6.49MB/s]\r\n",
      "\r\n",
      "vocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 30.6MB/s]\r\n",
      "\r\n",
      "merges.txt: 100%|████████████████████████████| 494k/494k [00:00<00:00, 28.3MB/s]\r\n",
      "\r\n",
      "normalizer.json: 100%|██████████████████████| 52.7k/52.7k [00:00<00:00, 133MB/s]\r\n",
      "\r\n",
      "added_tokens.json: 100%|███████████████████| 2.11k/2.11k [00:00<00:00, 16.1MB/s]\r\n",
      "\r\n",
      "special_tokens_map.json: 100%|█████████████| 2.06k/2.06k [00:00<00:00, 17.0MB/s]\r\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/vocab.json\r\n",
      "loading file tokenizer.json from cache at None\r\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/merges.txt\r\n",
      "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/normalizer.json\r\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/added_tokens.json\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/tokenizer_config.json\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "\r\n",
      "preprocessor_config.json: 100%|███████████████| 185k/185k [00:00<00:00, 115MB/s]\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/preprocessor_config.json\r\n",
      "Feature extractor WhisperFeatureExtractor {\r\n",
      "  \"chunk_length\": 30,\r\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\r\n",
      "  \"feature_size\": 80,\r\n",
      "  \"hop_length\": 160,\r\n",
      "  \"n_fft\": 400,\r\n",
      "  \"n_samples\": 480000,\r\n",
      "  \"nb_max_frames\": 3000,\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"WhisperProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 16000\r\n",
      "}\r\n",
      "\r\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n",
      "  warnings.warn(\r\n",
      "Increase max_length from 448 to 448 since input is conditioned on previous segment.\r\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\n",
      "Eval results for step (283 / 849 | Eval Loss: 4.805445194244385 | Eval clap: 0.1934000849723816 | Eval wer: 80.64516129032258 | Eval clean_wer: 93.54838709677419 | Eval noisy_word_error: 74.19354838709677 | Eval percent_clean_samples: 0.25 |)\r\n",
      "Train steps ... :  33%|██████            | 284/849 [1:23:52<12:02:06, 76.68s/it]Step... (284 / 849 | Loss: 5.0667500495910645, Learning Rate: 0.0001)\r\n",
      "Step... (284 / 849 | Loss: 4.646101474761963, Learning Rate: 0.0001)\r\n",
      "Step... (286 / 849 | Loss: 4.8134002685546875, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  34%|██████▍            | 286/849 [1:24:29<7:17:15, 46.60s/it]Step... (286 / 849 | Loss: 4.735479831695557, Learning Rate: 0.0001)\r\n",
      "Step... (288 / 849 | Loss: 4.974966526031494, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  34%|██████▍            | 288/849 [1:25:04<4:59:23, 32.02s/it]Step... (288 / 849 | Loss: 4.564556121826172, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  34%|██████▍            | 290/849 [1:25:39<3:48:47, 24.56s/it]Step... (290 / 849 | Loss: 4.95628547668457, Learning Rate: 0.0001)\r\n",
      "Step... (290 / 849 | Loss: 4.5642619132995605, Learning Rate: 0.0001)\r\n",
      "Step... (292 / 849 | Loss: 4.84760046005249, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  34%|██████▌            | 292/849 [1:26:12<3:08:21, 20.29s/it]Step... (292 / 849 | Loss: 4.758020877838135, Learning Rate: 0.0001)\r\n",
      "Step... (294 / 849 | Loss: 4.698984146118164, Learning Rate: 0.0001)\r\n",
      "Step... (294 / 849 | Loss: 4.560538291931152, Learning Rate: 0.0001)\r\n",
      "Step... (296 / 849 | Loss: 4.213935375213623, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  35%|██████▌            | 296/849 [1:27:21<2:46:12, 18.03s/it]Step... (296 / 849 | Loss: 4.625793933868408, Learning Rate: 0.0001)\r\n",
      "Step... (298 / 849 | Loss: 4.861575603485107, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  35%|██████▋            | 298/849 [1:27:53<2:36:06, 17.00s/it]Step... (298 / 849 | Loss: 4.931567668914795, Learning Rate: 0.0001)\r\n",
      "Step... (300 / 849 | Loss: 4.744855880737305, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  35%|██████▋            | 300/849 [1:28:27<2:34:45, 16.91s/it]Step... (300 / 849 | Loss: 4.562659740447998, Learning Rate: 0.0001)\r\n",
      "Step... (302 / 849 | Loss: 4.685432434082031, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  36%|██████▊            | 302/849 [1:29:01<2:35:49, 17.09s/it]Step... (302 / 849 | Loss: 4.423738479614258, Learning Rate: 0.0001)\r\n",
      "Step... (304 / 849 | Loss: 4.899443626403809, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  36%|██████▊            | 304/849 [1:29:36<2:36:26, 17.22s/it]Step... (304 / 849 | Loss: 4.656971454620361, Learning Rate: 0.0001)\r\n",
      "Step... (306 / 849 | Loss: 4.442172527313232, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  36%|██████▊            | 306/849 [1:30:08<2:29:45, 16.55s/it]Step... (306 / 849 | Loss: 4.895682334899902, Learning Rate: 0.0001)\r\n",
      "Step... (308 / 849 | Loss: 4.963366508483887, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  36%|██████▉            | 308/849 [1:30:44<2:34:51, 17.18s/it]Step... (308 / 849 | Loss: 4.56696891784668, Learning Rate: 0.0001)\r\n",
      "Step... (310 / 849 | Loss: 4.688089370727539, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  37%|██████▉            | 310/849 [1:31:18<2:35:01, 17.26s/it]Step... (310 / 849 | Loss: 4.524630069732666, Learning Rate: 0.0001)\r\n",
      "Step... (312 / 849 | Loss: 5.240494728088379, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  37%|██████▉            | 312/849 [1:31:50<2:30:24, 16.81s/it]Step... (312 / 849 | Loss: 5.001765251159668, Learning Rate: 0.0001)\r\n",
      "Step... (314 / 849 | Loss: 4.800049781799316, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  37%|███████            | 314/849 [1:32:24<2:27:48, 16.58s/it]Step... (314 / 849 | Loss: 4.52114725112915, Learning Rate: 0.0001)\r\n",
      "Step... (316 / 849 | Loss: 4.770978927612305, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  37%|███████            | 316/849 [1:32:58<2:29:24, 16.82s/it]Step... (316 / 849 | Loss: 4.87569522857666, Learning Rate: 0.0001)\r\n",
      "Step... (318 / 849 | Loss: 4.770887851715088, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  37%|███████            | 318/849 [1:33:31<2:30:17, 16.98s/it]Step... (318 / 849 | Loss: 4.990152359008789, Learning Rate: 0.0001)\r\n",
      "Step... (320 / 849 | Loss: 5.087886333465576, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  38%|███████▏           | 320/849 [1:34:03<2:24:34, 16.40s/it]Step... (320 / 849 | Loss: 4.7978386878967285, Learning Rate: 0.0001)\r\n",
      "Step... (322 / 849 | Loss: 4.911332130432129, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  38%|███████▏           | 322/849 [1:34:39<2:30:20, 17.12s/it]Step... (322 / 849 | Loss: 4.8505425453186035, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  38%|███████▎           | 324/849 [1:35:14<2:31:25, 17.31s/it]Step... (324 / 849 | Loss: 4.517734527587891, Learning Rate: 0.0001)\r\n",
      "Step... (324 / 849 | Loss: 4.662728786468506, Learning Rate: 0.0001)\r\n",
      "Step... (326 / 849 | Loss: 4.884176731109619, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  38%|███████▎           | 326/849 [1:35:47<2:28:31, 17.04s/it]Step... (326 / 849 | Loss: 4.982669353485107, Learning Rate: 0.0001)\r\n",
      "Step... (328 / 849 | Loss: 4.831912994384766, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  39%|███████▎           | 328/849 [1:36:20<2:25:12, 16.72s/it]Step... (328 / 849 | Loss: 4.582165241241455, Learning Rate: 0.0001)\r\n",
      "Step... (330 / 849 | Loss: 4.733097553253174, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  39%|███████▍           | 330/849 [1:36:55<2:26:43, 16.96s/it]Step... (330 / 849 | Loss: 4.6234259605407715, Learning Rate: 0.0001)\r\n",
      "Step... (332 / 849 | Loss: 4.536648273468018, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  39%|███████▍           | 332/849 [1:37:28<2:26:19, 16.98s/it]Step... (332 / 849 | Loss: 4.573963165283203, Learning Rate: 0.0001)\r\n",
      "Step... (334 / 849 | Loss: 4.480977535247803, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  39%|███████▍           | 334/849 [1:38:00<2:19:24, 16.24s/it]Step... (334 / 849 | Loss: 4.752264499664307, Learning Rate: 0.0001)\r\n",
      "Step... (336 / 849 | Loss: 4.878480911254883, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  40%|███████▌           | 336/849 [1:38:37<2:26:52, 17.18s/it]Step... (336 / 849 | Loss: 4.580019474029541, Learning Rate: 0.0001)\r\n",
      "Step... (338 / 849 | Loss: 4.535496711730957, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  40%|███████▌           | 338/849 [1:39:12<2:28:12, 17.40s/it]Step... (338 / 849 | Loss: 4.754594802856445, Learning Rate: 0.0001)\r\n",
      "Step... (340 / 849 | Loss: 4.696277141571045, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  40%|███████▌           | 340/849 [1:39:46<2:27:35, 17.40s/it]Step... (340 / 849 | Loss: 4.804708480834961, Learning Rate: 0.0001)\r\n",
      "Step... (342 / 849 | Loss: 4.735450744628906, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  40%|███████▋           | 342/849 [1:40:19<2:22:05, 16.82s/it]Step... (342 / 849 | Loss: 4.706855297088623, Learning Rate: 0.0001)\r\n",
      "Step... (344 / 849 | Loss: 4.893078804016113, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  41%|███████▋           | 344/849 [1:40:55<2:25:33, 17.29s/it]Step... (344 / 849 | Loss: 4.890349864959717, Learning Rate: 0.0001)\r\n",
      "Step... (346 / 849 | Loss: 4.686807632446289, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  41%|███████▋           | 346/849 [1:41:29<2:25:14, 17.33s/it]Step... (346 / 849 | Loss: 4.577195644378662, Learning Rate: 0.0001)\r\n",
      "Step... (348 / 849 | Loss: 4.904057025909424, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  41%|███████▊           | 348/849 [1:42:01<2:18:58, 16.64s/it]Step... (348 / 849 | Loss: 5.0158467292785645, Learning Rate: 0.0001)\r\n",
      "Step... (350 / 849 | Loss: 4.518095970153809, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  41%|███████▊           | 350/849 [1:42:35<2:19:40, 16.79s/it]Step... (350 / 849 | Loss: 4.9952592849731445, Learning Rate: 0.0001)\r\n",
      "Step... (352 / 849 | Loss: 4.5482892990112305, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  41%|███████▉           | 352/849 [1:43:10<2:21:57, 17.14s/it]Step... (352 / 849 | Loss: 4.836169719696045, Learning Rate: 0.0001)\r\n",
      "Step... (354 / 849 | Loss: 4.839380741119385, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  42%|███████▉           | 354/849 [1:43:44<2:21:53, 17.20s/it]Step... (354 / 849 | Loss: 4.91286563873291, Learning Rate: 0.0001)\r\n",
      "Step... (356 / 849 | Loss: 4.768091678619385, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  42%|███████▉           | 356/849 [1:44:16<2:15:50, 16.53s/it]Step... (356 / 849 | Loss: 4.844475269317627, Learning Rate: 0.0001)\r\n",
      "Step... (358 / 849 | Loss: 4.425314426422119, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  42%|████████           | 358/849 [1:44:51<2:17:41, 16.83s/it]Step... (358 / 849 | Loss: 4.783807277679443, Learning Rate: 0.0001)\r\n",
      "Step... (360 / 849 | Loss: 4.580298900604248, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  42%|████████           | 360/849 [1:45:26<2:21:56, 17.42s/it]Step... (360 / 849 | Loss: 4.63024377822876, Learning Rate: 0.0001)\r\n",
      "Step... (362 / 849 | Loss: 4.8671555519104, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  43%|████████           | 362/849 [1:46:00<2:19:54, 17.24s/it]Step... (362 / 849 | Loss: 4.641223907470703, Learning Rate: 0.0001)\r\n",
      "Step... (364 / 849 | Loss: 4.6576948165893555, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  43%|████████▏          | 364/849 [1:46:34<2:18:06, 17.09s/it]Step... (364 / 849 | Loss: 4.838100910186768, Learning Rate: 0.0001)\r\n",
      "Step... (366 / 849 | Loss: 4.970424175262451, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  43%|████████▏          | 366/849 [1:47:09<2:20:01, 17.40s/it]Step... (366 / 849 | Loss: 5.134913444519043, Learning Rate: 0.0001)\r\n",
      "Step... (368 / 849 | Loss: 4.712441444396973, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  43%|████████▏          | 368/849 [1:47:44<2:19:48, 17.44s/it]Step... (368 / 849 | Loss: 4.776247024536133, Learning Rate: 0.0001)\r\n",
      "Step... (370 / 849 | Loss: 4.8823113441467285, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  44%|████████▎          | 370/849 [1:48:16<2:12:21, 16.58s/it]Step... (370 / 849 | Loss: 4.5767717361450195, Learning Rate: 0.0001)\r\n",
      "Step... (372 / 849 | Loss: 4.554271221160889, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  44%|████████▎          | 372/849 [1:48:51<2:14:51, 16.96s/it]Step... (372 / 849 | Loss: 4.5905632972717285, Learning Rate: 0.0001)\r\n",
      "Step... (374 / 849 | Loss: 4.824374675750732, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  44%|████████▎          | 374/849 [1:49:26<2:17:45, 17.40s/it]Step... (374 / 849 | Loss: 4.68584680557251, Learning Rate: 0.0001)\r\n",
      "Step... (376 / 849 | Loss: 4.598435878753662, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  44%|████████▍          | 376/849 [1:49:59<2:13:03, 16.88s/it]Step... (376 / 849 | Loss: 5.097177028656006, Learning Rate: 0.0001)\r\n",
      "Step... (378 / 849 | Loss: 4.7649407386779785, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  45%|████████▍          | 378/849 [1:50:32<2:10:56, 16.68s/it]Step... (378 / 849 | Loss: 4.499046802520752, Learning Rate: 0.0001)\r\n",
      "Step... (380 / 849 | Loss: 4.956329822540283, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  45%|████████▌          | 380/849 [1:51:07<2:12:41, 16.98s/it]Step... (380 / 849 | Loss: 4.667811870574951, Learning Rate: 0.0001)\r\n",
      "Step... (382 / 849 | Loss: 4.674790382385254, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  45%|████████▌          | 382/849 [1:51:41<2:13:58, 17.21s/it]Step... (382 / 849 | Loss: 4.625154972076416, Learning Rate: 0.0001)\r\n",
      "Step... (384 / 849 | Loss: 4.80724573135376, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  45%|████████▌          | 384/849 [1:52:12<2:07:07, 16.40s/it]Step... (384 / 849 | Loss: 5.057290077209473, Learning Rate: 0.0001)\r\n",
      "Step... (386 / 849 | Loss: 4.686399459838867, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  45%|████████▋          | 386/849 [1:52:48<2:11:13, 17.01s/it]Step... (386 / 849 | Loss: 4.44970178604126, Learning Rate: 0.0001)\r\n",
      "Step... (388 / 849 | Loss: 4.720546245574951, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  46%|████████▋          | 388/849 [1:53:23<2:12:29, 17.24s/it]Step... (388 / 849 | Loss: 4.462208271026611, Learning Rate: 0.0001)\r\n",
      "Step... (390 / 849 | Loss: 4.722535610198975, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  46%|████████▋          | 390/849 [1:53:57<2:12:19, 17.30s/it]Step... (390 / 849 | Loss: 4.846113204956055, Learning Rate: 0.0001)\r\n",
      "Step... (392 / 849 | Loss: 4.594620704650879, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  46%|████████▊          | 392/849 [1:54:30<2:06:47, 16.65s/it]Step... (392 / 849 | Loss: 4.909256935119629, Learning Rate: 0.0001)\r\n",
      "Step... (394 / 849 | Loss: 4.759766101837158, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  46%|████████▊          | 394/849 [1:55:06<2:11:12, 17.30s/it]Step... (394 / 849 | Loss: 4.452847480773926, Learning Rate: 0.0001)\r\n",
      "Step... (396 / 849 | Loss: 4.801157474517822, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  47%|████████▊          | 396/849 [1:55:40<2:11:25, 17.41s/it]Step... (396 / 849 | Loss: 4.539691925048828, Learning Rate: 0.0001)\r\n",
      "Step... (398 / 849 | Loss: 4.850919246673584, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  47%|████████▉          | 398/849 [1:56:13<2:05:34, 16.71s/it]Step... (398 / 849 | Loss: 4.699232578277588, Learning Rate: 0.0001)\r\n",
      "Step... (400 / 849 | Loss: 4.761620044708252, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  47%|████████▉          | 400/849 [1:56:48<2:07:35, 17.05s/it]Step... (400 / 849 | Loss: 4.578614711761475, Learning Rate: 0.0001)\r\n",
      "\r\n",
      "Step... (402 / 849 | Loss: 4.616716384887695, Learning Rate: 0.0001)\r\n",
      "Step... (404 / 849 | Loss: 4.992373466491699, Learning Rate: 0.0001)\r\n",
      "Step... (404 / 849 | Loss: 4.885103702545166, Learning Rate: 0.0001)\r\n",
      "Step... (406 / 849 | Loss: 4.520564556121826, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  48%|█████████          | 406/849 [1:58:27<2:00:50, 16.37s/it]Step... (406 / 849 | Loss: 4.549261569976807, Learning Rate: 0.0001)\r\n",
      "Step... (408 / 849 | Loss: 4.5731730461120605, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  48%|█████████▏         | 408/849 [1:59:04<2:07:01, 17.28s/it]Step... (408 / 849 | Loss: 4.440775394439697, Learning Rate: 0.0001)\r\n",
      "Step... (410 / 849 | Loss: 4.433868408203125, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  48%|█████████▏         | 410/849 [1:59:39<2:08:14, 17.53s/it]Step... (410 / 849 | Loss: 4.853610992431641, Learning Rate: 0.0001)\r\n",
      "Step... (412 / 849 | Loss: 4.921057224273682, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  49%|█████████▏         | 412/849 [2:00:12<2:04:23, 17.08s/it]Step... (412 / 849 | Loss: 4.889401912689209, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  49%|█████████▎         | 414/849 [2:00:46<2:02:14, 16.86s/it]Step... (414 / 849 | Loss: 4.630136489868164, Learning Rate: 0.0001)\r\n",
      "Step... (414 / 849 | Loss: 4.876030921936035, Learning Rate: 0.0001)\r\n",
      "Step... (416 / 849 | Loss: 4.874527931213379, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  49%|█████████▎         | 416/849 [2:01:21<2:04:03, 17.19s/it]Step... (416 / 849 | Loss: 4.518021583557129, Learning Rate: 0.0001)\r\n",
      "Step... (418 / 849 | Loss: 4.963209629058838, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  49%|█████████▎         | 418/849 [2:01:54<2:02:32, 17.06s/it]Step... (418 / 849 | Loss: 4.890678405761719, Learning Rate: 0.0001)\r\n",
      "Step... (420 / 849 | Loss: 4.725546360015869, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  49%|█████████▍         | 420/849 [2:02:26<1:56:47, 16.33s/it]Step... (420 / 849 | Loss: 4.926268100738525, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  50%|█████████▍         | 422/849 [2:03:01<1:59:36, 16.81s/it]Step... (422 / 849 | Loss: 4.639881610870361, Learning Rate: 0.0001)\r\n",
      "Step... (422 / 849 | Loss: 4.6229047775268555, Learning Rate: 0.0001)\r\n",
      "Step... (424 / 849 | Loss: 4.555230617523193, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  50%|█████████▍         | 424/849 [2:03:35<2:01:39, 17.18s/it]Step... (424 / 849 | Loss: 4.48243522644043, Learning Rate: 0.0001)\r\n",
      "Step... (426 / 849 | Loss: 4.701657772064209, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  50%|█████████▌         | 426/849 [2:04:09<1:59:40, 16.97s/it]Step... (426 / 849 | Loss: 4.92702054977417, Learning Rate: 0.0001)\r\n",
      "Step... (428 / 849 | Loss: 4.605017185211182, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  50%|█████████▌         | 428/849 [2:04:41<1:56:12, 16.56s/it]Step... (428 / 849 | Loss: 4.927089214324951, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  51%|█████████▌         | 430/849 [2:05:18<2:02:18, 17.51s/it]Step... (430 / 849 | Loss: 4.722891330718994, Learning Rate: 0.0001)\r\n",
      "Step... (430 / 849 | Loss: 5.038778305053711, Learning Rate: 0.0001)\r\n",
      "Step... (432 / 849 | Loss: 4.629770278930664, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  51%|█████████▋         | 432/849 [2:05:53<2:02:18, 17.60s/it]Step... (432 / 849 | Loss: 4.820840358734131, Learning Rate: 0.0001)\r\n",
      "Step... (434 / 849 | Loss: 4.859890460968018, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  51%|█████████▋         | 434/849 [2:06:25<1:55:06, 16.64s/it]Step... (434 / 849 | Loss: 4.782006740570068, Learning Rate: 0.0001)\r\n",
      "Step... (436 / 849 | Loss: 4.789305210113525, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  51%|█████████▊         | 436/849 [2:07:01<1:57:36, 17.09s/it]Step... (436 / 849 | Loss: 4.596018314361572, Learning Rate: 0.0001)\r\n",
      "Step... (438 / 849 | Loss: 4.457302570343018, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  52%|█████████▊         | 438/849 [2:07:35<1:57:10, 17.11s/it]Step... (438 / 849 | Loss: 4.391750812530518, Learning Rate: 0.0001)\r\n",
      "Step... (440 / 849 | Loss: 4.586737155914307, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  52%|█████████▊         | 440/849 [2:08:08<1:55:09, 16.89s/it]Step... (440 / 849 | Loss: 4.764599323272705, Learning Rate: 0.0001)\r\n",
      "Step... (442 / 849 | Loss: 4.855320453643799, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  52%|█████████▉         | 442/849 [2:08:40<1:51:57, 16.50s/it]Step... (442 / 849 | Loss: 4.586843490600586, Learning Rate: 0.0001)\r\n",
      "Step... (444 / 849 | Loss: 4.768375873565674, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  52%|█████████▉         | 444/849 [2:09:15<1:54:04, 16.90s/it]Step... (444 / 849 | Loss: 4.770600318908691, Learning Rate: 0.0001)\r\n",
      "Step... (446 / 849 | Loss: 4.569289207458496, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  53%|█████████▉         | 446/849 [2:09:50<1:57:19, 17.47s/it]Step... (446 / 849 | Loss: 4.47578763961792, Learning Rate: 0.0001)\r\n",
      "Step... (448 / 849 | Loss: 5.1011176109313965, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  53%|██████████         | 448/849 [2:10:23<1:52:50, 16.88s/it]Step... (448 / 849 | Loss: 5.249030113220215, Learning Rate: 0.0001)\r\n",
      "Step... (450 / 849 | Loss: 4.91800594329834, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  53%|██████████         | 450/849 [2:10:57<1:52:20, 16.89s/it]Step... (450 / 849 | Loss: 4.405595779418945, Learning Rate: 0.0001)\r\n",
      "Step... (452 / 849 | Loss: 4.978283405303955, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  53%|██████████         | 452/849 [2:11:31<1:52:34, 17.01s/it]Step... (452 / 849 | Loss: 4.81433629989624, Learning Rate: 0.0001)\r\n",
      "Step... (454 / 849 | Loss: 4.960560321807861, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  53%|██████████▏        | 454/849 [2:12:05<1:51:15, 16.90s/it]Step... (454 / 849 | Loss: 4.43371057510376, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  54%|██████████▏        | 456/849 [2:12:37<1:47:12, 16.37s/it]Step... (456 / 849 | Loss: 4.642618656158447, Learning Rate: 0.0001)\r\n",
      "Step... (456 / 849 | Loss: 4.535168647766113, Learning Rate: 0.0001)\r\n",
      "Step... (458 / 849 | Loss: 5.179173946380615, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  54%|██████████▏        | 458/849 [2:13:13<1:51:55, 17.18s/it]Step... (458 / 849 | Loss: 4.631808280944824, Learning Rate: 0.0001)\r\n",
      "Step... (460 / 849 | Loss: 4.416834354400635, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  54%|██████████▎        | 460/849 [2:13:48<1:53:09, 17.45s/it]Step... (460 / 849 | Loss: 4.5442681312561035, Learning Rate: 0.0001)\r\n",
      "Step... (462 / 849 | Loss: 4.9521050453186035, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  54%|██████████▎        | 462/849 [2:14:21<1:49:11, 16.93s/it]Step... (462 / 849 | Loss: 4.908331394195557, Learning Rate: 0.0001)\r\n",
      "Step... (464 / 849 | Loss: 4.557433128356934, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  55%|██████████▍        | 464/849 [2:14:54<1:47:40, 16.78s/it]Step... (464 / 849 | Loss: 4.779916286468506, Learning Rate: 0.0001)\r\n",
      "Step... (466 / 849 | Loss: 4.703450679779053, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  55%|██████████▍        | 466/849 [2:15:30<1:50:27, 17.30s/it]Step... (466 / 849 | Loss: 4.519168853759766, Learning Rate: 0.0001)\r\n",
      "Step... (468 / 849 | Loss: 4.466994285583496, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  55%|██████████▍        | 468/849 [2:16:04<1:50:32, 17.41s/it]Step... (468 / 849 | Loss: 5.178910732269287, Learning Rate: 0.0001)\r\n",
      "Step... (470 / 849 | Loss: 4.6867804527282715, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  55%|██████████▌        | 470/849 [2:16:36<1:44:37, 16.56s/it]Step... (470 / 849 | Loss: 4.644751071929932, Learning Rate: 0.0001)\r\n",
      "Step... (472 / 849 | Loss: 4.38037633895874, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  56%|██████████▌        | 472/849 [2:17:12<1:46:53, 17.01s/it]Step... (472 / 849 | Loss: 4.5617218017578125, Learning Rate: 0.0001)\r\n",
      "Step... (474 / 849 | Loss: 4.4926676750183105, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  56%|██████████▌        | 474/849 [2:17:47<1:48:55, 17.43s/it]Step... (474 / 849 | Loss: 4.8965911865234375, Learning Rate: 0.0001)\r\n",
      "Step... (476 / 849 | Loss: 4.7446136474609375, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  56%|██████████▋        | 476/849 [2:18:20<1:46:00, 17.05s/it]Step... (476 / 849 | Loss: 4.632349491119385, Learning Rate: 0.0001)\r\n",
      "Step... (478 / 849 | Loss: 4.566478729248047, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  56%|██████████▋        | 478/849 [2:18:53<1:43:11, 16.69s/it]Step... (478 / 849 | Loss: 4.716568470001221, Learning Rate: 0.0001)\r\n",
      "Step... (480 / 849 | Loss: 4.6495490074157715, Learning Rate: 0.0001)\r\n",
      "Step... (480 / 849 | Loss: 4.639834403991699, Learning Rate: 0.0001)\r\n",
      "Step... (482 / 849 | Loss: 4.686953544616699, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  57%|██████████▊        | 482/849 [2:20:02<1:45:54, 17.32s/it]Step... (482 / 849 | Loss: 4.659207344055176, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  57%|██████████▊        | 484/849 [2:20:33<1:39:39, 16.38s/it]Step... (484 / 849 | Loss: 4.781260013580322, Learning Rate: 0.0001)\r\n",
      "Step... (484 / 849 | Loss: 4.8451104164123535, Learning Rate: 0.0001)\r\n",
      "Step... (486 / 849 | Loss: 4.858202934265137, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  57%|██████████▉        | 486/849 [2:21:10<1:43:14, 17.07s/it]Step... (486 / 849 | Loss: 4.776971817016602, Learning Rate: 0.0001)\r\n",
      "Step... (488 / 849 | Loss: 4.680586338043213, Learning Rate: 0.0001)\r\n",
      "Step... (488 / 849 | Loss: 4.919003963470459, Learning Rate: 0.0001)\r\n",
      "Step... (490 / 849 | Loss: 4.505546569824219, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  58%|██████████▉        | 490/849 [2:22:18<1:43:07, 17.24s/it]Step... (490 / 849 | Loss: 4.966756820678711, Learning Rate: 0.0001)\r\n",
      "Step... (492 / 849 | Loss: 4.499711513519287, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  58%|███████████        | 492/849 [2:22:51<1:39:03, 16.65s/it]Step... (492 / 849 | Loss: 4.497114658355713, Learning Rate: 0.0001)\r\n",
      "Step... (494 / 849 | Loss: 4.836039066314697, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  58%|███████████        | 494/849 [2:23:25<1:39:57, 16.89s/it]Step... (494 / 849 | Loss: 4.511162757873535, Learning Rate: 0.0001)\r\n",
      "Step... (496 / 849 | Loss: 4.694551467895508, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  58%|███████████        | 496/849 [2:23:59<1:40:56, 17.16s/it]Step... (496 / 849 | Loss: 4.499631881713867, Learning Rate: 0.0001)\r\n",
      "Step... (498 / 849 | Loss: 4.9340386390686035, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  59%|███████████▏       | 498/849 [2:24:31<1:35:32, 16.33s/it]Step... (498 / 849 | Loss: 4.8033013343811035, Learning Rate: 0.0001)\r\n",
      "Step... (500 / 849 | Loss: 4.549471378326416, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  59%|███████████▏       | 500/849 [2:25:04<1:36:05, 16.52s/it]Step... (500 / 849 | Loss: 4.535867691040039, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  59%|███████████▏       | 502/849 [2:25:51<1:52:59, 19.54s/it]Step... (502 / 849 | Loss: 4.502933025360107, Learning Rate: 0.0001)\r\n",
      "Step... (502 / 849 | Loss: 4.547170162200928, Learning Rate: 0.0001)\r\n",
      "Step... (504 / 849 | Loss: 4.694980144500732, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  59%|███████████▎       | 504/849 [2:26:25<1:44:41, 18.21s/it]Step... (504 / 849 | Loss: 4.604928016662598, Learning Rate: 0.0001)\r\n",
      "Step... (506 / 849 | Loss: 4.518651962280273, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  60%|███████████▎       | 506/849 [2:26:57<1:37:16, 17.01s/it]Step... (506 / 849 | Loss: 4.506012439727783, Learning Rate: 0.0001)\r\n",
      "Step... (508 / 849 | Loss: 4.879162311553955, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  60%|███████████▎       | 508/849 [2:27:33<1:38:41, 17.37s/it]Step... (508 / 849 | Loss: 4.628238201141357, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  60%|███████████▍       | 510/849 [2:28:07<1:38:05, 17.36s/it]Step... (510 / 849 | Loss: 4.660763263702393, Learning Rate: 0.0001)\r\n",
      "Step... (510 / 849 | Loss: 4.917210102081299, Learning Rate: 0.0001)\r\n",
      "Step... (512 / 849 | Loss: 4.833425045013428, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  60%|███████████▍       | 512/849 [2:28:39<1:34:23, 16.81s/it]Step... (512 / 849 | Loss: 4.870018005371094, Learning Rate: 0.0001)\r\n",
      "Step... (514 / 849 | Loss: 4.536600589752197, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  61%|███████████▌       | 514/849 [2:29:12<1:31:47, 16.44s/it]Step... (514 / 849 | Loss: 4.325780391693115, Learning Rate: 0.0001)\r\n",
      "Step... (516 / 849 | Loss: 4.733672142028809, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  61%|███████████▌       | 516/849 [2:29:47<1:33:59, 16.93s/it]Step... (516 / 849 | Loss: 4.880641460418701, Learning Rate: 0.0001)\r\n",
      "Step... (518 / 849 | Loss: 4.685025215148926, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  61%|███████████▌       | 518/849 [2:30:21<1:35:07, 17.24s/it]Step... (518 / 849 | Loss: 4.787874221801758, Learning Rate: 0.0001)\r\n",
      "Step... (520 / 849 | Loss: 4.9842848777771, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  61%|███████████▋       | 520/849 [2:30:53<1:30:44, 16.55s/it]Step... (520 / 849 | Loss: 4.64723014831543, Learning Rate: 0.0001)\r\n",
      "Step... (522 / 849 | Loss: 4.394593238830566, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  61%|███████████▋       | 522/849 [2:31:29<1:33:16, 17.12s/it]Step... (522 / 849 | Loss: 5.047556400299072, Learning Rate: 0.0001)\r\n",
      "Step... (524 / 849 | Loss: 4.844417572021484, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  62%|███████████▋       | 524/849 [2:32:04<1:34:22, 17.42s/it]Step... (524 / 849 | Loss: 5.067024230957031, Learning Rate: 0.0001)\r\n",
      "Step... (526 / 849 | Loss: 4.361120700836182, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  62%|███████████▊       | 526/849 [2:32:38<1:33:05, 17.29s/it]Step... (526 / 849 | Loss: 4.553508281707764, Learning Rate: 0.0001)\r\n",
      "Step... (528 / 849 | Loss: 4.820897102355957, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  62%|███████████▊       | 528/849 [2:33:11<1:30:32, 16.92s/it]Step... (528 / 849 | Loss: 4.578353404998779, Learning Rate: 0.0001)\r\n",
      "Step... (530 / 849 | Loss: 4.593021392822266, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  62%|███████████▊       | 530/849 [2:33:48<1:32:33, 17.41s/it]Step... (530 / 849 | Loss: 4.647757053375244, Learning Rate: 0.0001)\r\n",
      "Step... (532 / 849 | Loss: 4.513911724090576, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  63%|███████████▉       | 532/849 [2:34:21<1:31:01, 17.23s/it]Step... (532 / 849 | Loss: 4.484719753265381, Learning Rate: 0.0001)\r\n",
      "Step... (534 / 849 | Loss: 5.023650646209717, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  63%|███████████▉       | 534/849 [2:34:52<1:25:20, 16.25s/it]Step... (534 / 849 | Loss: 4.567379951477051, Learning Rate: 0.0001)\r\n",
      "Step... (536 / 849 | Loss: 4.696585655212402, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  63%|███████████▉       | 536/849 [2:35:27<1:27:43, 16.82s/it]Step... (536 / 849 | Loss: 4.456769943237305, Learning Rate: 0.0001)\r\n",
      "Step... (538 / 849 | Loss: 4.742642879486084, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  63%|████████████       | 538/849 [2:36:02<1:28:17, 17.03s/it]Step... (538 / 849 | Loss: 4.454151153564453, Learning Rate: 0.0001)\r\n",
      "Step... (540 / 849 | Loss: 4.832315921783447, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  64%|████████████       | 540/849 [2:36:36<1:28:09, 17.12s/it]Step... (540 / 849 | Loss: 4.822075843811035, Learning Rate: 0.0001)\r\n",
      "Step... (542 / 849 | Loss: 4.411585330963135, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  64%|████████████▏      | 542/849 [2:37:08<1:25:20, 16.68s/it]Step... (542 / 849 | Loss: 4.564450263977051, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  64%|████████████▏      | 544/849 [2:37:44<1:27:36, 17.23s/it]Step... (544 / 849 | Loss: 4.457005500793457, Learning Rate: 0.0001)\r\n",
      "Step... (544 / 849 | Loss: 4.511995315551758, Learning Rate: 0.0001)\r\n",
      "Step... (546 / 849 | Loss: 4.675199508666992, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  64%|████████████▏      | 546/849 [2:38:18<1:27:16, 17.28s/it]Step... (546 / 849 | Loss: 4.816940784454346, Learning Rate: 0.0001)\r\n",
      "Step... (548 / 849 | Loss: 4.892849445343018, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  65%|████████████▎      | 548/849 [2:38:51<1:23:42, 16.69s/it]Step... (548 / 849 | Loss: 5.299999237060547, Learning Rate: 0.0001)\r\n",
      "Step... (550 / 849 | Loss: 4.741169452667236, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  65%|████████████▎      | 550/849 [2:39:24<1:23:14, 16.70s/it]Step... (550 / 849 | Loss: 4.973569393157959, Learning Rate: 0.0001)\r\n",
      "Step... (552 / 849 | Loss: 4.530478477478027, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  65%|████████████▎      | 552/849 [2:39:59<1:24:14, 17.02s/it]Step... (552 / 849 | Loss: 4.596240520477295, Learning Rate: 0.0001)\r\n",
      "Step... (554 / 849 | Loss: 4.526013374328613, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  65%|████████████▍      | 554/849 [2:40:33<1:24:08, 17.11s/it]Step... (554 / 849 | Loss: 4.5839056968688965, Learning Rate: 0.0001)\r\n",
      "Step... (556 / 849 | Loss: 5.071901798248291, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  65%|████████████▍      | 556/849 [2:41:06<1:21:17, 16.65s/it]Step... (556 / 849 | Loss: 4.505147457122803, Learning Rate: 0.0001)\r\n",
      "Step... (558 / 849 | Loss: 4.596303462982178, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  66%|████████████▍      | 558/849 [2:41:41<1:22:58, 17.11s/it]Step... (558 / 849 | Loss: 4.7366485595703125, Learning Rate: 0.0001)\r\n",
      "Step... (560 / 849 | Loss: 4.441059112548828, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  66%|████████████▌      | 560/849 [2:42:16<1:24:05, 17.46s/it]Step... (560 / 849 | Loss: 4.560579299926758, Learning Rate: 0.0001)\r\n",
      "Step... (562 / 849 | Loss: 4.735999584197998, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  66%|████████████▌      | 562/849 [2:42:49<1:21:39, 17.07s/it]Step... (562 / 849 | Loss: 4.935473918914795, Learning Rate: 0.0001)\r\n",
      "Step... (564 / 849 | Loss: 4.489042282104492, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  66%|████████████▌      | 564/849 [2:43:23<1:20:18, 16.91s/it]Step... (564 / 849 | Loss: 4.970888137817383, Learning Rate: 0.0001)\r\n",
      "Step... (566 / 849 | Loss: 4.524825572967529, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  67%|████████████▋      | 566/849 [2:43:59<1:21:56, 17.37s/it]Step... (566 / 849 | Loss: 4.454277515411377, Learning Rate: 0.0001)\r\n",
      "\r\n",
      "\r\n",
      "Evaluating - Inference ...:   0%|                         | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "Evaluating - Inference ...: 100%|█████████████████| 1/1 [00:01<00:00,  1.55s/it]\r\n",
      "\r\n",
      "\r\n",
      "Evaluating - Generation ...:   0%|                        | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "Evaluating - Generation ...: 100%|███████████████| 1/1 [01:59<00:00, 119.35s/it]\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\r\n",
      "text_config is None. Initializing the ClapTextConfig with default values.\r\n",
      "audio_config is None. initializing the ClapAudioConfig with default values.\r\n",
      "Model config ClapConfig {\r\n",
      "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"ClapModel\"\r\n",
      "  ],\r\n",
      "  \"audio_config\": {\r\n",
      "    \"depths\": [\r\n",
      "      2,\r\n",
      "      2,\r\n",
      "      12,\r\n",
      "      2\r\n",
      "    ],\r\n",
      "    \"hidden_size\": 1024,\r\n",
      "    \"model_type\": \"clap_audio_model\",\r\n",
      "    \"patch_embeds_hidden_size\": 128\r\n",
      "  },\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"logit_scale_init_value\": 14.285714285714285,\r\n",
      "  \"model_type\": \"clap\",\r\n",
      "  \"num_hidden_layers\": 16,\r\n",
      "  \"projection_dim\": 512,\r\n",
      "  \"projection_hidden_act\": \"relu\",\r\n",
      "  \"text_config\": {\r\n",
      "    \"model_type\": \"clap_text_model\"\r\n",
      "  },\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\"\r\n",
      "}\r\n",
      "\r\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\r\n",
      "Attempting to create safetensors variant\r\n",
      "All model checkpoint weights were used when initializing ClapModel.\r\n",
      "\r\n",
      "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\r\n",
      "Attempting to convert .bin model on the fly to safetensors.\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\r\n",
      "Feature extractor ClapFeatureExtractor {\r\n",
      "  \"chunk_length_s\": 10,\r\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\r\n",
      "  \"feature_size\": 64,\r\n",
      "  \"fft_window_size\": 1024,\r\n",
      "  \"frequency_max\": 14000,\r\n",
      "  \"frequency_min\": 50,\r\n",
      "  \"hop_length\": 480,\r\n",
      "  \"max_length_s\": 10,\r\n",
      "  \"n_fft\": 1024,\r\n",
      "  \"nb_frequency_bins\": 513,\r\n",
      "  \"nb_max_frames\": 1000,\r\n",
      "  \"nb_max_samples\": 480000,\r\n",
      "  \"padding\": \"repeatpad\",\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"ClapProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 48000,\r\n",
      "  \"top_db\": null,\r\n",
      "  \"truncation\": \"rand_trunc\"\r\n",
      "}\r\n",
      "\r\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\r\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\r\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\r\n",
      "loading file added_tokens.json from cache at None\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\r\n",
      "Processor ClapProcessor:\r\n",
      "- feature_extractor: ClapFeatureExtractor {\r\n",
      "  \"chunk_length_s\": 10,\r\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\r\n",
      "  \"feature_size\": 64,\r\n",
      "  \"fft_window_size\": 1024,\r\n",
      "  \"frequency_max\": 14000,\r\n",
      "  \"frequency_min\": 50,\r\n",
      "  \"hop_length\": 480,\r\n",
      "  \"max_length_s\": 10,\r\n",
      "  \"n_fft\": 1024,\r\n",
      "  \"nb_frequency_bins\": 513,\r\n",
      "  \"nb_max_frames\": 1000,\r\n",
      "  \"nb_max_samples\": 480000,\r\n",
      "  \"padding\": \"repeatpad\",\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"ClapProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 48000,\r\n",
      "  \"top_db\": null,\r\n",
      "  \"truncation\": \"rand_trunc\"\r\n",
      "}\r\n",
      "\r\n",
      "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\r\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "  \"processor_class\": \"ClapProcessor\"\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/pytorch_model.bin\r\n",
      "Attempting to create safetensors variant\r\n",
      "Generate config GenerationConfig {\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"use_cache\": false\r\n",
      "}\r\n",
      "\r\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\r\n",
      "\r\n",
      "Attempting to convert .bin model on the fly to safetensors.\r\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at cahya/whisper-medium-id.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\r\n",
      "Generation config file not found, using a generation config created from the model config.\r\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/vocab.json\r\n",
      "loading file tokenizer.json from cache at None\r\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/merges.txt\r\n",
      "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/normalizer.json\r\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/added_tokens.json\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/tokenizer_config.json\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/preprocessor_config.json\r\n",
      "Feature extractor WhisperFeatureExtractor {\r\n",
      "  \"chunk_length\": 30,\r\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\r\n",
      "  \"feature_size\": 80,\r\n",
      "  \"hop_length\": 160,\r\n",
      "  \"n_fft\": 400,\r\n",
      "  \"n_samples\": 480000,\r\n",
      "  \"nb_max_frames\": 3000,\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"WhisperProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 16000\r\n",
      "}\r\n",
      "\r\n",
      "Increase max_length from 448 to 448 since input is conditioned on previous segment.\r\n",
      "Increase max_length from 448 to 448 since input is conditioned on previous segment.\r\n",
      "Eval results for step (566 / 849 | Eval Loss: 4.759355068206787 | Eval clap: 0.1918008029460907 | Eval wer: 72.58064516129032 | Eval clean_wer: 67.04545454545455 | Eval noisy_word_error: 77.55102040816327 | Eval percent_clean_samples: 0.375 |)\r\n",
      "Step... (568 / 849 | Loss: 4.40195369720459, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  67%|████████████▋      | 568/849 [2:47:52<4:33:04, 58.31s/it]Step... (568 / 849 | Loss: 5.183750629425049, Learning Rate: 0.0001)\r\n",
      "Step... (570 / 849 | Loss: 4.821115970611572, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  67%|████████████▊      | 570/849 [2:48:29<2:55:50, 37.82s/it]Step... (570 / 849 | Loss: 4.965200901031494, Learning Rate: 0.0001)\r\n",
      "Step... (572 / 849 | Loss: 4.927312850952148, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  67%|████████████▊      | 572/849 [2:49:05<2:07:12, 27.55s/it]Step... (572 / 849 | Loss: 4.756781578063965, Learning Rate: 0.0001)\r\n",
      "Step... (574 / 849 | Loss: 4.825780868530273, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  68%|████████████▊      | 574/849 [2:49:39<1:42:22, 22.34s/it]Step... (574 / 849 | Loss: 4.927520751953125, Learning Rate: 0.0001)\r\n",
      "Step... (576 / 849 | Loss: 4.740842819213867, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  68%|████████████▉      | 576/849 [2:50:12<1:28:13, 19.39s/it]Step... (576 / 849 | Loss: 4.5665459632873535, Learning Rate: 0.0001)\r\n",
      "Step... (578 / 849 | Loss: 5.069671630859375, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  68%|████████████▉      | 578/849 [2:50:49<1:24:22, 18.68s/it]Step... (578 / 849 | Loss: 4.83118200302124, Learning Rate: 0.0001)\r\n",
      "Step... (580 / 849 | Loss: 4.812381267547607, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  68%|████████████▉      | 580/849 [2:51:23<1:20:38, 17.99s/it]Step... (580 / 849 | Loss: 4.580976963043213, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  69%|█████████████      | 582/849 [2:51:56<1:16:30, 17.19s/it]Step... (582 / 849 | Loss: 4.818609714508057, Learning Rate: 0.0001)\r\n",
      "Step... (582 / 849 | Loss: 5.137726306915283, Learning Rate: 0.0001)\r\n",
      "Step... (584 / 849 | Loss: 4.740356922149658, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  69%|█████████████      | 584/849 [2:52:30<1:15:44, 17.15s/it]Step... (584 / 849 | Loss: 4.8941216468811035, Learning Rate: 0.0001)\r\n",
      "Step... (586 / 849 | Loss: 4.603383541107178, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  69%|█████████████      | 586/849 [2:53:05<1:15:53, 17.31s/it]Step... (586 / 849 | Loss: 4.45041561126709, Learning Rate: 0.0001)\r\n",
      "Step... (588 / 849 | Loss: 4.688348293304443, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  69%|█████████████▏     | 588/849 [2:53:40<1:15:50, 17.44s/it]Step... (588 / 849 | Loss: 4.564975261688232, Learning Rate: 0.0001)\r\n",
      "Step... (590 / 849 | Loss: 4.561665058135986, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  69%|█████████████▏     | 590/849 [2:54:12<1:12:15, 16.74s/it]Step... (590 / 849 | Loss: 4.525677680969238, Learning Rate: 0.0001)\r\n",
      "Step... (592 / 849 | Loss: 4.435649394989014, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  70%|█████████████▏     | 592/849 [2:54:48<1:13:39, 17.20s/it]Step... (592 / 849 | Loss: 4.768796443939209, Learning Rate: 0.0001)\r\n",
      "Step... (594 / 849 | Loss: 4.642824172973633, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  70%|█████████████▎     | 594/849 [2:55:23<1:14:32, 17.54s/it]Step... (594 / 849 | Loss: 4.728646278381348, Learning Rate: 0.0001)\r\n",
      "Step... (596 / 849 | Loss: 4.327951908111572, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  70%|█████████████▎     | 596/849 [2:55:57<1:12:40, 17.24s/it]Step... (596 / 849 | Loss: 5.0949387550354, Learning Rate: 0.0001)\r\n",
      "Step... (598 / 849 | Loss: 4.505540370941162, Learning Rate: 0.0001)\r\n",
      "Step... (598 / 849 | Loss: 4.892964839935303, Learning Rate: 0.0001)\r\n",
      "Step... (600 / 849 | Loss: 4.4573564529418945, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  71%|█████████████▍     | 600/849 [2:57:06<1:11:30, 17.23s/it]Step... (600 / 849 | Loss: 4.552042484283447, Learning Rate: 0.0001)\r\n",
      "Step... (602 / 849 | Loss: 4.782072067260742, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  71%|█████████████▍     | 602/849 [2:57:40<1:11:35, 17.39s/it]Step... (602 / 849 | Loss: 4.5110697746276855, Learning Rate: 0.0001)\r\n",
      "Step... (604 / 849 | Loss: 4.38785982131958, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  71%|█████████████▌     | 604/849 [2:58:12<1:07:48, 16.61s/it]Step... (604 / 849 | Loss: 4.486959934234619, Learning Rate: 0.0001)\r\n",
      "Step... (606 / 849 | Loss: 4.665293216705322, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  71%|█████████████▌     | 606/849 [2:58:47<1:08:33, 16.93s/it]Step... (606 / 849 | Loss: 4.7412848472595215, Learning Rate: 0.0001)\r\n",
      "Step... (608 / 849 | Loss: 4.433405876159668, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  72%|█████████████▌     | 608/849 [2:59:22<1:09:27, 17.29s/it]Step... (608 / 849 | Loss: 4.488577842712402, Learning Rate: 0.0001)\r\n",
      "Step... (610 / 849 | Loss: 4.6077399253845215, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  72%|█████████████▋     | 610/849 [2:59:55<1:07:48, 17.02s/it]Step... (610 / 849 | Loss: 4.809092044830322, Learning Rate: 0.0001)\r\n",
      "Step... (612 / 849 | Loss: 4.834125518798828, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  72%|█████████████▋     | 612/849 [3:00:27<1:05:13, 16.51s/it]Step... (612 / 849 | Loss: 4.6401753425598145, Learning Rate: 0.0001)\r\n",
      "Step... (614 / 849 | Loss: 4.946944713592529, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  72%|█████████████▋     | 614/849 [3:01:03<1:06:58, 17.10s/it]Step... (614 / 849 | Loss: 4.823784828186035, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  73%|█████████████▊     | 616/849 [3:01:37<1:06:43, 17.18s/it]Step... (616 / 849 | Loss: 4.745372295379639, Learning Rate: 0.0001)\r\n",
      "Step... (616 / 849 | Loss: 4.648489952087402, Learning Rate: 0.0001)\r\n",
      "Step... (618 / 849 | Loss: 4.552534580230713, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  73%|█████████████▊     | 618/849 [3:02:08<1:02:35, 16.26s/it]Step... (618 / 849 | Loss: 4.740697383880615, Learning Rate: 0.0001)\r\n",
      "Step... (620 / 849 | Loss: 4.930487632751465, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  73%|█████████████▉     | 620/849 [3:02:44<1:04:54, 17.01s/it]Step... (620 / 849 | Loss: 4.424993991851807, Learning Rate: 0.0001)\r\n",
      "Step... (622 / 849 | Loss: 4.250054836273193, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  73%|█████████████▉     | 622/849 [3:03:19<1:05:05, 17.21s/it]Step... (622 / 849 | Loss: 4.6085524559021, Learning Rate: 0.0001)\r\n",
      "Step... (624 / 849 | Loss: 4.650742530822754, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  73%|█████████████▉     | 624/849 [3:03:52<1:04:07, 17.10s/it]Step... (624 / 849 | Loss: 4.933779239654541, Learning Rate: 0.0001)\r\n",
      "Step... (626 / 849 | Loss: 4.850154876708984, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  74%|██████████████     | 626/849 [3:04:24<1:01:15, 16.48s/it]Step... (626 / 849 | Loss: 4.427939414978027, Learning Rate: 0.0001)\r\n",
      "Step... (628 / 849 | Loss: 4.630314350128174, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  74%|██████████████     | 628/849 [3:05:00<1:02:39, 17.01s/it]Step... (628 / 849 | Loss: 4.61436128616333, Learning Rate: 0.0001)\r\n",
      "Step... (630 / 849 | Loss: 4.840016841888428, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  74%|██████████████     | 630/849 [3:05:33<1:02:10, 17.04s/it]Step... (630 / 849 | Loss: 4.606327533721924, Learning Rate: 0.0001)\r\n",
      "Step... (632 / 849 | Loss: 4.913018226623535, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  74%|███████████████▋     | 632/849 [3:06:05<59:45, 16.52s/it]Step... (632 / 849 | Loss: 4.849221229553223, Learning Rate: 0.0001)\r\n",
      "Step... (634 / 849 | Loss: 4.566017150878906, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  75%|███████████████▋     | 634/849 [3:06:39<59:39, 16.65s/it]Step... (634 / 849 | Loss: 4.310379981994629, Learning Rate: 0.0001)\r\n",
      "Step... (636 / 849 | Loss: 4.661372661590576, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  75%|██████████████▏    | 636/849 [3:07:15<1:00:54, 17.16s/it]Step... (636 / 849 | Loss: 4.275242328643799, Learning Rate: 0.0001)\r\n",
      "Step... (638 / 849 | Loss: 4.83530330657959, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  75%|██████████████▎    | 638/849 [3:07:50<1:01:55, 17.61s/it]Step... (638 / 849 | Loss: 4.6181135177612305, Learning Rate: 0.0001)\r\n",
      "Step... (640 / 849 | Loss: 4.663527011871338, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  75%|███████████████▊     | 640/849 [3:08:22<58:29, 16.79s/it]Step... (640 / 849 | Loss: 4.492653846740723, Learning Rate: 0.0001)\r\n",
      "Step... (642 / 849 | Loss: 4.583356857299805, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  76%|███████████████▉     | 642/849 [3:08:58<59:24, 17.22s/it]Step... (642 / 849 | Loss: 4.529421806335449, Learning Rate: 0.0001)\r\n",
      "Step... (644 / 849 | Loss: 4.5572099685668945, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  76%|███████████████▉     | 644/849 [3:09:32<59:41, 17.47s/it]Step... (644 / 849 | Loss: 4.476719856262207, Learning Rate: 0.0001)\r\n",
      "Step... (646 / 849 | Loss: 4.920361042022705, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  76%|███████████████▉     | 646/849 [3:10:04<56:42, 16.76s/it]Step... (646 / 849 | Loss: 4.958080768585205, Learning Rate: 0.0001)\r\n",
      "Step... (648 / 849 | Loss: 4.659493446350098, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  76%|████████████████     | 648/849 [3:10:38<55:44, 16.64s/it]Step... (648 / 849 | Loss: 4.687828063964844, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  77%|████████████████     | 650/849 [3:11:12<56:01, 16.89s/it]Step... (650 / 849 | Loss: 4.422250270843506, Learning Rate: 0.0001)\r\n",
      "Step... (650 / 849 | Loss: 4.950478553771973, Learning Rate: 0.0001)\r\n",
      "Step... (652 / 849 | Loss: 4.892897605895996, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  77%|████████████████▏    | 652/849 [3:11:46<55:50, 17.01s/it]Step... (652 / 849 | Loss: 4.804361820220947, Learning Rate: 0.0001)\r\n",
      "Step... (654 / 849 | Loss: 4.489510536193848, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  77%|████████████████▏    | 654/849 [3:12:17<52:59, 16.31s/it]Step... (654 / 849 | Loss: 4.602776527404785, Learning Rate: 0.0001)\r\n",
      "Step... (656 / 849 | Loss: 4.479129791259766, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  77%|████████████████▏    | 656/849 [3:12:53<54:25, 16.92s/it]Step... (656 / 849 | Loss: 4.739134788513184, Learning Rate: 0.0001)\r\n",
      "Step... (658 / 849 | Loss: 4.597743988037109, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  78%|████████████████▎    | 658/849 [3:13:27<54:31, 17.13s/it]Step... (658 / 849 | Loss: 4.708016872406006, Learning Rate: 0.0001)\r\n",
      "Step... (660 / 849 | Loss: 4.705904006958008, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  78%|████████████████▎    | 660/849 [3:14:00<53:37, 17.02s/it]Step... (660 / 849 | Loss: 4.998495578765869, Learning Rate: 0.0001)\r\n",
      "Step... (662 / 849 | Loss: 4.775488376617432, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  78%|████████████████▎    | 662/849 [3:14:34<52:12, 16.75s/it]Step... (662 / 849 | Loss: 4.351401329040527, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  78%|████████████████▍    | 664/849 [3:15:09<52:46, 17.12s/it]Step... (664 / 849 | Loss: 4.602967262268066, Learning Rate: 0.0001)\r\n",
      "Step... (664 / 849 | Loss: 4.459289073944092, Learning Rate: 0.0001)\r\n",
      "Step... (666 / 849 | Loss: 4.788944244384766, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  78%|████████████████▍    | 666/849 [3:15:44<53:17, 17.47s/it]Step... (666 / 849 | Loss: 4.382242679595947, Learning Rate: 0.0001)\r\n",
      "Step... (668 / 849 | Loss: 4.69508695602417, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  79%|████████████████▌    | 668/849 [3:16:15<49:47, 16.51s/it]Step... (668 / 849 | Loss: 4.6234259605407715, Learning Rate: 0.0001)\r\n",
      "Step... (670 / 849 | Loss: 4.705169200897217, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  79%|████████████████▌    | 670/849 [3:16:51<51:05, 17.13s/it]Step... (670 / 849 | Loss: 4.524256706237793, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  79%|████████████████▌    | 672/849 [3:17:25<50:06, 16.99s/it]Step... (672 / 849 | Loss: 4.4233174324035645, Learning Rate: 0.0001)\r\n",
      "Step... (672 / 849 | Loss: 4.821974277496338, Learning Rate: 0.0001)\r\n",
      "Step... (674 / 849 | Loss: 4.680227756500244, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  79%|████████████████▋    | 674/849 [3:17:58<49:10, 16.86s/it]Step... (674 / 849 | Loss: 5.070242404937744, Learning Rate: 0.0001)\r\n",
      "Step... (676 / 849 | Loss: 4.733835220336914, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  80%|████████████████▋    | 676/849 [3:18:31<47:37, 16.52s/it]Step... (676 / 849 | Loss: 4.855888366699219, Learning Rate: 0.0001)\r\n",
      "Step... (678 / 849 | Loss: 4.5475592613220215, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  80%|████████████████▊    | 678/849 [3:19:06<48:18, 16.95s/it]Step... (678 / 849 | Loss: 4.409694671630859, Learning Rate: 0.0001)\r\n",
      "Step... (680 / 849 | Loss: 4.671347141265869, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  80%|████████████████▊    | 680/849 [3:19:41<48:51, 17.35s/it]Step... (680 / 849 | Loss: 4.575008869171143, Learning Rate: 0.0001)\r\n",
      "Step... (682 / 849 | Loss: 5.143188953399658, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  80%|████████████████▊    | 682/849 [3:20:14<47:04, 16.91s/it]Step... (682 / 849 | Loss: 4.978825569152832, Learning Rate: 0.0001)\r\n",
      "Step... (684 / 849 | Loss: 4.463481903076172, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  81%|████████████████▉    | 684/849 [3:20:48<46:39, 16.97s/it]Step... (684 / 849 | Loss: 4.794641494750977, Learning Rate: 0.0001)\r\n",
      "Step... (686 / 849 | Loss: 4.79768705368042, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  81%|████████████████▉    | 686/849 [3:21:23<46:46, 17.22s/it]Step... (686 / 849 | Loss: 4.524060249328613, Learning Rate: 0.0001)\r\n",
      "Step... (688 / 849 | Loss: 4.649971008300781, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  81%|█████████████████    | 688/849 [3:21:57<46:22, 17.28s/it]Step... (688 / 849 | Loss: 4.638955593109131, Learning Rate: 0.0001)\r\n",
      "Step... (690 / 849 | Loss: 4.7152204513549805, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  81%|█████████████████    | 690/849 [3:22:30<44:15, 16.70s/it]Step... (690 / 849 | Loss: 4.743102550506592, Learning Rate: 0.0001)\r\n",
      "Step... (692 / 849 | Loss: 4.6117634773254395, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  82%|█████████████████    | 692/849 [3:23:06<44:52, 17.15s/it]Step... (692 / 849 | Loss: 4.788092136383057, Learning Rate: 0.0001)\r\n",
      "Step... (694 / 849 | Loss: 4.520345687866211, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  82%|█████████████████▏   | 694/849 [3:23:39<44:22, 17.17s/it]Step... (694 / 849 | Loss: 4.804401397705078, Learning Rate: 0.0001)\r\n",
      "Step... (696 / 849 | Loss: 4.635290622711182, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  82%|█████████████████▏   | 696/849 [3:24:12<42:45, 16.77s/it]Step... (696 / 849 | Loss: 4.491870403289795, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  82%|█████████████████▎   | 698/849 [3:24:46<42:13, 16.78s/it]Step... (698 / 849 | Loss: 4.573983669281006, Learning Rate: 0.0001)\r\n",
      "Step... (698 / 849 | Loss: 4.386025428771973, Learning Rate: 0.0001)\r\n",
      "Step... (700 / 849 | Loss: 4.463795185089111, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  82%|█████████████████▎   | 700/849 [3:25:21<42:32, 17.13s/it]Step... (700 / 849 | Loss: 4.536543369293213, Learning Rate: 0.0001)\r\n",
      "Step... (702 / 849 | Loss: 4.72606086730957, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  83%|█████████████████▎   | 702/849 [3:25:55<42:12, 17.23s/it]Step... (702 / 849 | Loss: 5.0058770179748535, Learning Rate: 0.0001)\r\n",
      "Step... (704 / 849 | Loss: 4.788848400115967, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  83%|█████████████████▍   | 704/849 [3:26:27<39:56, 16.53s/it]Step... (704 / 849 | Loss: 4.389477252960205, Learning Rate: 0.0001)\r\n",
      "Step... (706 / 849 | Loss: 4.475358009338379, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  83%|█████████████████▍   | 706/849 [3:27:03<40:47, 17.12s/it]Step... (706 / 849 | Loss: 4.741019248962402, Learning Rate: 0.0001)\r\n",
      "Step... (708 / 849 | Loss: 4.394096851348877, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  83%|█████████████████▌   | 708/849 [3:27:38<41:16, 17.56s/it]Step... (708 / 849 | Loss: 4.468301773071289, Learning Rate: 0.0001)\r\n",
      "Step... (710 / 849 | Loss: 4.8786163330078125, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  84%|█████████████████▌   | 710/849 [3:28:13<40:21, 17.42s/it]Step... (710 / 849 | Loss: 4.804721832275391, Learning Rate: 0.0001)\r\n",
      "Step... (712 / 849 | Loss: 4.3859734535217285, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  84%|█████████████████▌   | 712/849 [3:28:46<38:44, 16.97s/it]Step... (712 / 849 | Loss: 4.532658100128174, Learning Rate: 0.0001)\r\n",
      "Step... (714 / 849 | Loss: 4.738131046295166, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  84%|█████████████████▋   | 714/849 [3:29:21<38:45, 17.22s/it]Step... (714 / 849 | Loss: 4.489649295806885, Learning Rate: 0.0001)\r\n",
      "Step... (716 / 849 | Loss: 5.193564414978027, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  84%|█████████████████▋   | 716/849 [3:29:55<38:13, 17.25s/it]Step... (716 / 849 | Loss: 4.514872074127197, Learning Rate: 0.0001)\r\n",
      "Step... (718 / 849 | Loss: 4.649017333984375, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  85%|█████████████████▊   | 718/849 [3:30:26<35:42, 16.35s/it]Step... (718 / 849 | Loss: 4.51045036315918, Learning Rate: 0.0001)\r\n",
      "Step... (720 / 849 | Loss: 4.42446231842041, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  85%|█████████████████▊   | 720/849 [3:31:02<36:36, 17.03s/it]Step... (720 / 849 | Loss: 4.367575645446777, Learning Rate: 0.0001)\r\n",
      "Step... (722 / 849 | Loss: 4.770732402801514, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  85%|█████████████████▊   | 722/849 [3:31:37<36:37, 17.30s/it]Step... (722 / 849 | Loss: 4.75316047668457, Learning Rate: 0.0001)\r\n",
      "Step... (724 / 849 | Loss: 4.821231842041016, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  85%|█████████████████▉   | 724/849 [3:32:11<35:39, 17.11s/it]Step... (724 / 849 | Loss: 4.804616928100586, Learning Rate: 0.0001)\r\n",
      "Step... (726 / 849 | Loss: 5.012178897857666, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  86%|█████████████████▉   | 726/849 [3:32:44<34:20, 16.75s/it]Step... (726 / 849 | Loss: 4.521848201751709, Learning Rate: 0.0001)\r\n",
      "Step... (728 / 849 | Loss: 4.7264018058776855, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  86%|██████████████████   | 728/849 [3:33:21<35:10, 17.44s/it]Step... (728 / 849 | Loss: 4.8665666580200195, Learning Rate: 0.0001)\r\n",
      "Step... (730 / 849 | Loss: 4.636754512786865, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  86%|██████████████████   | 730/849 [3:33:55<34:40, 17.48s/it]Step... (730 / 849 | Loss: 4.747036933898926, Learning Rate: 0.0001)\r\n",
      "Step... (732 / 849 | Loss: 4.724388122558594, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  86%|██████████████████   | 732/849 [3:34:27<32:50, 16.84s/it]Step... (732 / 849 | Loss: 4.65650749206543, Learning Rate: 0.0001)\r\n",
      "Step... (734 / 849 | Loss: 4.528967380523682, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  86%|██████████████████▏  | 734/849 [3:35:02<32:28, 16.94s/it]Step... (734 / 849 | Loss: 4.735355377197266, Learning Rate: 0.0001)\r\n",
      "Step... (736 / 849 | Loss: 4.412875175476074, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  87%|██████████████████▏  | 736/849 [3:35:37<32:41, 17.36s/it]Step... (736 / 849 | Loss: 4.934597969055176, Learning Rate: 0.0001)\r\n",
      "Step... (738 / 849 | Loss: 5.037350177764893, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  87%|██████████████████▎  | 738/849 [3:36:11<31:57, 17.27s/it]Step... (738 / 849 | Loss: 4.7411699295043945, Learning Rate: 0.0001)\r\n",
      "Step... (740 / 849 | Loss: 4.51928186416626, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  87%|██████████████████▎  | 740/849 [3:36:43<30:03, 16.55s/it]Step... (740 / 849 | Loss: 4.779555320739746, Learning Rate: 0.0001)\r\n",
      "Step... (742 / 849 | Loss: 4.540915012359619, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  87%|██████████████████▎  | 742/849 [3:37:20<30:57, 17.36s/it]Step... (742 / 849 | Loss: 4.444367408752441, Learning Rate: 0.0001)\r\n",
      "Step... (744 / 849 | Loss: 4.859278202056885, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  88%|██████████████████▍  | 744/849 [3:37:55<30:52, 17.64s/it]Step... (744 / 849 | Loss: 4.494266033172607, Learning Rate: 0.0001)\r\n",
      "Step... (746 / 849 | Loss: 4.953475475311279, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  88%|██████████████████▍  | 746/849 [3:38:28<29:18, 17.07s/it]Step... (746 / 849 | Loss: 4.933716773986816, Learning Rate: 0.0001)\r\n",
      "Step... (748 / 849 | Loss: 4.526286602020264, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  88%|██████████████████▌  | 748/849 [3:39:01<28:18, 16.82s/it]Step... (748 / 849 | Loss: 5.035686016082764, Learning Rate: 0.0001)\r\n",
      "Step... (750 / 849 | Loss: 4.286152362823486, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  88%|██████████████████▌  | 750/849 [3:39:37<28:19, 17.17s/it]Step... (750 / 849 | Loss: 4.59115743637085, Learning Rate: 0.0001)\r\n",
      "Step... (752 / 849 | Loss: 4.838750839233398, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  89%|██████████████████▌  | 752/849 [3:40:11<28:01, 17.34s/it]Step... (752 / 849 | Loss: 4.8017120361328125, Learning Rate: 0.0001)\r\n",
      "Step... (754 / 849 | Loss: 5.096357822418213, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  89%|██████████████████▋  | 754/849 [3:40:43<26:07, 16.50s/it]Step... (754 / 849 | Loss: 4.480827808380127, Learning Rate: 0.0001)\r\n",
      "Step... (756 / 849 | Loss: 4.406985282897949, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  89%|██████████████████▋  | 756/849 [3:41:18<26:11, 16.90s/it]Step... (756 / 849 | Loss: 4.754458427429199, Learning Rate: 0.0001)\r\n",
      "Step... (758 / 849 | Loss: 4.4974212646484375, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  89%|██████████████████▋  | 758/849 [3:41:52<26:07, 17.23s/it]Step... (758 / 849 | Loss: 4.669251441955566, Learning Rate: 0.0001)\r\n",
      "Step... (760 / 849 | Loss: 4.524630546569824, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  90%|██████████████████▊  | 760/849 [3:42:26<25:20, 17.08s/it]Step... (760 / 849 | Loss: 5.008352756500244, Learning Rate: 0.0001)\r\n",
      "Step... (762 / 849 | Loss: 4.68955135345459, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  90%|██████████████████▊  | 762/849 [3:42:59<24:15, 16.73s/it]Step... (762 / 849 | Loss: 4.49589204788208, Learning Rate: 0.0001)\r\n",
      "Step... (764 / 849 | Loss: 4.45705509185791, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  90%|██████████████████▉  | 764/849 [3:43:35<24:24, 17.23s/it]Step... (764 / 849 | Loss: 4.728730201721191, Learning Rate: 0.0001)\r\n",
      "Step... (766 / 849 | Loss: 4.781803607940674, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  90%|██████████████████▉  | 766/849 [3:44:09<23:56, 17.31s/it]Step... (766 / 849 | Loss: 5.006139755249023, Learning Rate: 0.0001)\r\n",
      "Step... (768 / 849 | Loss: 4.526967525482178, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  90%|██████████████████▉  | 768/849 [3:44:40<22:09, 16.41s/it]Step... (768 / 849 | Loss: 4.89237117767334, Learning Rate: 0.0001)\r\n",
      "Step... (770 / 849 | Loss: 5.035686016082764, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  91%|███████████████████  | 770/849 [3:45:16<22:32, 17.12s/it]Step... (770 / 849 | Loss: 4.748961925506592, Learning Rate: 0.0001)\r\n",
      "Step... (772 / 849 | Loss: 4.629953861236572, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  91%|███████████████████  | 772/849 [3:45:51<22:09, 17.26s/it]Step... (772 / 849 | Loss: 4.578351974487305, Learning Rate: 0.0001)\r\n",
      "Step... (774 / 849 | Loss: 4.3763017654418945, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  91%|███████████████████▏ | 774/849 [3:46:25<21:29, 17.19s/it]Step... (774 / 849 | Loss: 4.739258289337158, Learning Rate: 0.0001)\r\n",
      "Step... (776 / 849 | Loss: 4.688876628875732, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  91%|███████████████████▏ | 776/849 [3:46:57<20:09, 16.57s/it]Step... (776 / 849 | Loss: 4.563923358917236, Learning Rate: 0.0001)\r\n",
      "Step... (778 / 849 | Loss: 4.652889728546143, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  92%|███████████████████▏ | 778/849 [3:47:33<20:15, 17.12s/it]Step... (778 / 849 | Loss: 4.563147068023682, Learning Rate: 0.0001)\r\n",
      "Step... (780 / 849 | Loss: 4.428131580352783, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  92%|███████████████████▎ | 780/849 [3:48:08<20:06, 17.49s/it]Step... (780 / 849 | Loss: 4.729153633117676, Learning Rate: 0.0001)\r\n",
      "Step... (782 / 849 | Loss: 4.878449440002441, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  92%|███████████████████▎ | 782/849 [3:48:41<18:56, 16.96s/it]Step... (782 / 849 | Loss: 5.0124945640563965, Learning Rate: 0.0001)\r\n",
      "Step... (784 / 849 | Loss: 4.8472161293029785, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  92%|███████████████████▍ | 784/849 [3:49:15<18:18, 16.90s/it]Step... (784 / 849 | Loss: 4.777019023895264, Learning Rate: 0.0001)\r\n",
      "Step... (786 / 849 | Loss: 4.371471881866455, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  93%|███████████████████▍ | 786/849 [3:49:50<18:08, 17.27s/it]Step... (786 / 849 | Loss: 4.611437797546387, Learning Rate: 0.0001)\r\n",
      "Step... (788 / 849 | Loss: 4.856086254119873, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  93%|███████████████████▍ | 788/849 [3:50:23<17:23, 17.11s/it]Step... (788 / 849 | Loss: 4.552695274353027, Learning Rate: 0.0001)\r\n",
      "Step... (790 / 849 | Loss: 4.485332489013672, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  93%|███████████████████▌ | 790/849 [3:50:55<16:12, 16.48s/it]Step... (790 / 849 | Loss: 4.618839263916016, Learning Rate: 0.0001)\r\n",
      "Step... (792 / 849 | Loss: 4.733015537261963, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  93%|███████████████████▌ | 792/849 [3:51:30<16:02, 16.89s/it]Step... (792 / 849 | Loss: 4.534082412719727, Learning Rate: 0.0001)\r\n",
      "Step... (794 / 849 | Loss: 4.89312744140625, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  94%|███████████████████▋ | 794/849 [3:52:04<15:44, 17.17s/it]Step... (794 / 849 | Loss: 4.8239216804504395, Learning Rate: 0.0001)\r\n",
      "Step... (796 / 849 | Loss: 4.808960914611816, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  94%|███████████████████▋ | 796/849 [3:52:37<14:44, 16.69s/it]Step... (796 / 849 | Loss: 4.940762519836426, Learning Rate: 0.0001)\r\n",
      "Step... (798 / 849 | Loss: 4.475229740142822, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  94%|███████████████████▋ | 798/849 [3:53:10<14:04, 16.56s/it]Step... (798 / 849 | Loss: 4.4510722160339355, Learning Rate: 0.0001)\r\n",
      "Step... (800 / 849 | Loss: 4.934704303741455, Learning Rate: 0.0001)\r\n",
      "Step... (800 / 849 | Loss: 4.76311731338501, Learning Rate: 0.0001)\r\n",
      "Step... (802 / 849 | Loss: 4.648016929626465, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  94%|███████████████████▊ | 802/849 [3:54:21<13:43, 17.53s/it]Step... (802 / 849 | Loss: 4.7063751220703125, Learning Rate: 0.0001)\r\n",
      "Step... (804 / 849 | Loss: 4.497519016265869, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  95%|███████████████████▉ | 804/849 [3:54:53<12:33, 16.75s/it]Step... (804 / 849 | Loss: 4.439584732055664, Learning Rate: 0.0001)\r\n",
      "Step... (806 / 849 | Loss: 4.846558094024658, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  95%|███████████████████▉ | 806/849 [3:55:29<12:28, 17.41s/it]Step... (806 / 849 | Loss: 5.100453853607178, Learning Rate: 0.0001)\r\n",
      "Step... (808 / 849 | Loss: 4.5374226570129395, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  95%|███████████████████▉ | 808/849 [3:56:05<12:05, 17.69s/it]Step... (808 / 849 | Loss: 4.964290618896484, Learning Rate: 0.0001)\r\n",
      "Step... (810 / 849 | Loss: 4.610713481903076, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  95%|████████████████████ | 810/849 [3:56:38<11:15, 17.31s/it]Step... (810 / 849 | Loss: 5.129340171813965, Learning Rate: 0.0001)\r\n",
      "Step... (812 / 849 | Loss: 4.640525817871094, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  96%|████████████████████ | 812/849 [3:57:11<10:18, 16.71s/it]Step... (812 / 849 | Loss: 4.45657205581665, Learning Rate: 0.0001)\r\n",
      "Step... (814 / 849 | Loss: 4.575628280639648, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  96%|████████████████████▏| 814/849 [3:57:45<09:50, 16.88s/it]Step... (814 / 849 | Loss: 4.5143327713012695, Learning Rate: 0.0001)\r\n",
      "Step... (816 / 849 | Loss: 4.635283470153809, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  96%|████████████████████▏| 816/849 [3:58:19<09:19, 16.96s/it]Step... (816 / 849 | Loss: 4.540409564971924, Learning Rate: 0.0001)\r\n",
      "Step... (818 / 849 | Loss: 4.547802925109863, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  96%|████████████████████▏| 818/849 [3:58:49<08:16, 16.01s/it]Step... (818 / 849 | Loss: 4.540453910827637, Learning Rate: 0.0001)\r\n",
      "Step... (820 / 849 | Loss: 4.5396270751953125, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  97%|████████████████████▎| 820/849 [3:59:25<08:07, 16.82s/it]Step... (820 / 849 | Loss: 4.541543960571289, Learning Rate: 0.0001)\r\n",
      "Step... (822 / 849 | Loss: 4.914821147918701, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  97%|████████████████████▎| 822/849 [4:00:00<07:45, 17.23s/it]Step... (822 / 849 | Loss: 4.902485370635986, Learning Rate: 0.0001)\r\n",
      "Step... (824 / 849 | Loss: 5.014888286590576, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  97%|████████████████████▍| 824/849 [4:00:33<07:02, 16.92s/it]Step... (824 / 849 | Loss: 4.419981479644775, Learning Rate: 0.0001)\r\n",
      "Step... (826 / 849 | Loss: 4.527982234954834, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  97%|████████████████████▍| 826/849 [4:01:05<06:16, 16.36s/it]Step... (826 / 849 | Loss: 4.664799690246582, Learning Rate: 0.0001)\r\n",
      "Step... (828 / 849 | Loss: 4.4567365646362305, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  98%|████████████████████▍| 828/849 [4:01:40<05:52, 16.79s/it]Step... (828 / 849 | Loss: 4.43194055557251, Learning Rate: 0.0001)\r\n",
      "Step... (830 / 849 | Loss: 4.664073467254639, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  98%|████████████████████▌| 830/849 [4:02:14<05:24, 17.09s/it]Step... (830 / 849 | Loss: 4.757176876068115, Learning Rate: 0.0001)\r\n",
      "Step... (832 / 849 | Loss: 4.9133501052856445, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  98%|████████████████████▌| 832/849 [4:02:46<04:42, 16.59s/it]Step... (832 / 849 | Loss: 4.834813594818115, Learning Rate: 0.0001)\r\n",
      "Step... (834 / 849 | Loss: 4.669877052307129, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  98%|████████████████████▋| 834/849 [4:03:21<04:13, 16.91s/it]Step... (834 / 849 | Loss: 4.937753200531006, Learning Rate: 0.0001)\r\n",
      "Step... (836 / 849 | Loss: 5.035917282104492, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  98%|████████████████████▋| 836/849 [4:03:55<03:43, 17.16s/it]Step... (836 / 849 | Loss: 4.800719261169434, Learning Rate: 0.0001)\r\n",
      "Step... (838 / 849 | Loss: 4.46959924697876, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  99%|████████████████████▋| 838/849 [4:04:30<03:09, 17.22s/it]Step... (838 / 849 | Loss: 4.7993950843811035, Learning Rate: 0.0001)\r\n",
      "Step... (840 / 849 | Loss: 4.525678634643555, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  99%|████████████████████▊| 840/849 [4:05:02<02:29, 16.63s/it]Step... (840 / 849 | Loss: 4.491907596588135, Learning Rate: 0.0001)\r\n",
      "Step... (842 / 849 | Loss: 4.7694573402404785, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  99%|████████████████████▊| 842/849 [4:05:38<02:00, 17.28s/it]Step... (842 / 849 | Loss: 4.667273044586182, Learning Rate: 0.0001)\r\n",
      "Step... (844 / 849 | Loss: 4.6668267250061035, Learning Rate: 0.0001)\r\n",
      "Train steps ... :  99%|████████████████████▉| 844/849 [4:06:12<01:27, 17.42s/it]Step... (844 / 849 | Loss: 4.408161163330078, Learning Rate: 0.0001)\r\n",
      "Step... (846 / 849 | Loss: 4.606276035308838, Learning Rate: 0.0001)\r\n",
      "Train steps ... : 100%|████████████████████▉| 846/849 [4:06:45<00:50, 16.97s/it]Step... (846 / 849 | Loss: 4.362549781799316, Learning Rate: 0.0001)\r\n",
      "Step... (848 / 849 | Loss: 4.59244966506958, Learning Rate: 0.0001)\r\n",
      "Train steps ... : 100%|████████████████████▉| 848/849 [4:07:19<00:16, 16.89s/it]Step... (848 / 849 | Loss: 4.4899468421936035, Learning Rate: 0.0001)\r\n",
      "Train steps ... : 100%|█████████████████████| 849/849 [4:07:37<00:00, 17.14s/it]Configuration saved in ./output_dir_training/config.json\r\n",
      "Configuration saved in ./output_dir_training/generation_config.json\r\n",
      "Model weights saved in ./output_dir_training/model.safetensors\r\n",
      "\r\n",
      "\r\n",
      "Evaluating - Inference ...:   0%|                         | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "Evaluating - Inference ...: 100%|█████████████████| 1/1 [00:01<00:00,  1.52s/it]\r\n",
      "\r\n",
      "\r\n",
      "Evaluating - Generation ...:   0%|                        | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\r\n",
      "\r\n",
      "Evaluating - Generation ...: 100%|████████████████| 1/1 [00:56<00:00, 56.28s/it]\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/config.json\r\n",
      "text_config is None. Initializing the ClapTextConfig with default values.\r\n",
      "audio_config is None. initializing the ClapAudioConfig with default values.\r\n",
      "Model config ClapConfig {\r\n",
      "  \"_name_or_path\": \"laion/larger_clap_music_and_speech\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"ClapModel\"\r\n",
      "  ],\r\n",
      "  \"audio_config\": {\r\n",
      "    \"depths\": [\r\n",
      "      2,\r\n",
      "      2,\r\n",
      "      12,\r\n",
      "      2\r\n",
      "    ],\r\n",
      "    \"hidden_size\": 1024,\r\n",
      "    \"model_type\": \"clap_audio_model\",\r\n",
      "    \"patch_embeds_hidden_size\": 128\r\n",
      "  },\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_factor\": 1.0,\r\n",
      "  \"logit_scale_init_value\": 14.285714285714285,\r\n",
      "  \"model_type\": \"clap\",\r\n",
      "  \"num_hidden_layers\": 16,\r\n",
      "  \"projection_dim\": 512,\r\n",
      "  \"projection_hidden_act\": \"relu\",\r\n",
      "  \"text_config\": {\r\n",
      "    \"model_type\": \"clap_text_model\"\r\n",
      "  },\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\"\r\n",
      "}\r\n",
      "\r\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/pytorch_model.bin\r\n",
      "Attempting to create safetensors variant\r\n",
      "All model checkpoint weights were used when initializing ClapModel.\r\n",
      "\r\n",
      "All the weights of ClapModel were initialized from the model checkpoint at laion/larger_clap_music_and_speech.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ClapModel for predictions without further training.\r\n",
      "Attempting to convert .bin model on the fly to safetensors.\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/preprocessor_config.json\r\n",
      "Feature extractor ClapFeatureExtractor {\r\n",
      "  \"chunk_length_s\": 10,\r\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\r\n",
      "  \"feature_size\": 64,\r\n",
      "  \"fft_window_size\": 1024,\r\n",
      "  \"frequency_max\": 14000,\r\n",
      "  \"frequency_min\": 50,\r\n",
      "  \"hop_length\": 480,\r\n",
      "  \"max_length_s\": 10,\r\n",
      "  \"n_fft\": 1024,\r\n",
      "  \"nb_frequency_bins\": 513,\r\n",
      "  \"nb_max_frames\": 1000,\r\n",
      "  \"nb_max_samples\": 480000,\r\n",
      "  \"padding\": \"repeatpad\",\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"ClapProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 48000,\r\n",
      "  \"top_db\": null,\r\n",
      "  \"truncation\": \"rand_trunc\"\r\n",
      "}\r\n",
      "\r\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/vocab.json\r\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/merges.txt\r\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer.json\r\n",
      "loading file added_tokens.json from cache at None\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--laion--larger_clap_music_and_speech/snapshots/195c3a3e68faebb3e2088b9a79e79b43ddbda76b/tokenizer_config.json\r\n",
      "Processor ClapProcessor:\r\n",
      "- feature_extractor: ClapFeatureExtractor {\r\n",
      "  \"chunk_length_s\": 10,\r\n",
      "  \"feature_extractor_type\": \"ClapFeatureExtractor\",\r\n",
      "  \"feature_size\": 64,\r\n",
      "  \"fft_window_size\": 1024,\r\n",
      "  \"frequency_max\": 14000,\r\n",
      "  \"frequency_min\": 50,\r\n",
      "  \"hop_length\": 480,\r\n",
      "  \"max_length_s\": 10,\r\n",
      "  \"n_fft\": 1024,\r\n",
      "  \"nb_frequency_bins\": 513,\r\n",
      "  \"nb_max_frames\": 1000,\r\n",
      "  \"nb_max_samples\": 480000,\r\n",
      "  \"padding\": \"repeatpad\",\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"ClapProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 48000,\r\n",
      "  \"top_db\": null,\r\n",
      "  \"truncation\": \"rand_trunc\"\r\n",
      "}\r\n",
      "\r\n",
      "- tokenizer: RobertaTokenizerFast(name_or_path='laion/larger_clap_music_and_speech', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\r\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\r\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "  \"processor_class\": \"ClapProcessor\"\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/config.json\r\n",
      "Model config WhisperConfig {\r\n",
      "  \"_name_or_path\": \"cahya/whisper-medium-id\",\r\n",
      "  \"activation_dropout\": 0.0,\r\n",
      "  \"activation_function\": \"gelu\",\r\n",
      "  \"apply_spec_augment\": false,\r\n",
      "  \"architectures\": [\r\n",
      "    \"WhisperForConditionalGeneration\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"classifier_proj_size\": 256,\r\n",
      "  \"d_model\": 1024,\r\n",
      "  \"decoder_attention_heads\": 16,\r\n",
      "  \"decoder_ffn_dim\": 4096,\r\n",
      "  \"decoder_layerdrop\": 0.0,\r\n",
      "  \"decoder_layers\": 24,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"dropout\": 0.0,\r\n",
      "  \"encoder_attention_heads\": 16,\r\n",
      "  \"encoder_ffn_dim\": 4096,\r\n",
      "  \"encoder_layerdrop\": 0.0,\r\n",
      "  \"encoder_layers\": 24,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"forced_decoder_ids\": null,\r\n",
      "  \"init_std\": 0.02,\r\n",
      "  \"is_encoder_decoder\": true,\r\n",
      "  \"mask_feature_length\": 10,\r\n",
      "  \"mask_feature_min_masks\": 0,\r\n",
      "  \"mask_feature_prob\": 0.0,\r\n",
      "  \"mask_time_length\": 10,\r\n",
      "  \"mask_time_min_masks\": 2,\r\n",
      "  \"mask_time_prob\": 0.05,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"max_source_positions\": 1500,\r\n",
      "  \"max_target_positions\": 448,\r\n",
      "  \"median_filter_width\": 7,\r\n",
      "  \"model_type\": \"whisper\",\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_mel_bins\": 80,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"scale_embedding\": false,\r\n",
      "  \"torch_dtype\": \"float32\",\r\n",
      "  \"transformers_version\": \"4.46.1\",\r\n",
      "  \"use_cache\": false,\r\n",
      "  \"use_weighted_layer_sum\": false,\r\n",
      "  \"vocab_size\": 51865\r\n",
      "}\r\n",
      "\r\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/pytorch_model.bin\r\n",
      "Attempting to create safetensors variant\r\n",
      "Generate config GenerationConfig {\r\n",
      "  \"begin_suppress_tokens\": [\r\n",
      "    220,\r\n",
      "    50257\r\n",
      "  ],\r\n",
      "  \"bos_token_id\": 50257,\r\n",
      "  \"decoder_start_token_id\": 50258,\r\n",
      "  \"eos_token_id\": 50257,\r\n",
      "  \"max_length\": 448,\r\n",
      "  \"pad_token_id\": 50257,\r\n",
      "  \"use_cache\": false\r\n",
      "}\r\n",
      "\r\n",
      "Attempting to convert .bin model on the fly to safetensors.\r\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\r\n",
      "\r\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at cahya/whisper-medium-id.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\r\n",
      "Generation config file not found, using a generation config created from the model config.\r\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/vocab.json\r\n",
      "loading file tokenizer.json from cache at None\r\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/merges.txt\r\n",
      "loading file normalizer.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/normalizer.json\r\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/added_tokens.json\r\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/special_tokens_map.json\r\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/tokenizer_config.json\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--cahya--whisper-medium-id/snapshots/d7048bffc7e227c624f7030acc8f189798275d5a/preprocessor_config.json\r\n",
      "Feature extractor WhisperFeatureExtractor {\r\n",
      "  \"chunk_length\": 30,\r\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\r\n",
      "  \"feature_size\": 80,\r\n",
      "  \"hop_length\": 160,\r\n",
      "  \"n_fft\": 400,\r\n",
      "  \"n_samples\": 480000,\r\n",
      "  \"nb_max_frames\": 3000,\r\n",
      "  \"padding_side\": \"right\",\r\n",
      "  \"padding_value\": 0.0,\r\n",
      "  \"processor_class\": \"WhisperProcessor\",\r\n",
      "  \"return_attention_mask\": false,\r\n",
      "  \"sampling_rate\": 16000\r\n",
      "}\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n",
      "  warnings.warn(\r\n",
      "Increase max_length from 448 to 448 since input is conditioned on previous segment.\r\n",
      "Eval results for step (849 / 849 | Eval Loss: 4.73675012588501 | Eval clap: 0.21093344688415527 | Eval wer: 51.075268817204304 |)\r\n",
      "Train steps ... : 100%|█████████████████████| 849/849 [4:10:01<00:00, 17.67s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch ./training/run_parler_tts_training.py \\\n",
    "    --model_name_or_path \"parler-tts/parler-tts-mini-v1\" \\\n",
    "    --feature_extractor_name \"parler-tts/dac_44khZ_8kbps\" \\\n",
    "    --description_tokenizer_name \"parler-tts/parler-tts-mini-v1\" \\\n",
    "    --prompt_tokenizer_name \"parler-tts/parler-tts-mini-v1\" \\\n",
    "    --asr_model_name_or_path \"cahya/whisper-medium-id\" \\\n",
    "    --report_to \"none\" \\\n",
    "    --overwrite_output_dir true \\\n",
    "    --train_dataset_name \"Amadeus99/youtube-transcript-dataset\" \\\n",
    "    --train_metadata_dataset_name \"Amadeus99/youtube-transcript-dataset-processed\" \\\n",
    "    --train_dataset_config_name \"default\" \\\n",
    "    --train_split_name \"train\" \\\n",
    "    --eval_dataset_name \"Amadeus99/youtube-transcript-dataset\" \\\n",
    "    --eval_metadata_dataset_name \"Amadeus99/youtube-transcript-dataset-processed\" \\\n",
    "    --eval_dataset_config_name \"default\" \\\n",
    "    --eval_split_name \"test\" \\\n",
    "    --max_eval_samples 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --target_audio_column_name \"audio\" \\\n",
    "    --description_column_name \"text_description\" \\\n",
    "    --prompt_column_name \"text\" \\\n",
    "    --max_duration_in_seconds 20 \\\n",
    "    --min_duration_in_seconds 7.0 \\\n",
    "    --max_text_length 400 \\\n",
    "    --preprocessing_num_workers 2 \\\n",
    "    --do_train true \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --gradient_accumulation_steps 18 \\\n",
    "    --gradient_checkpointing true \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --adam_beta1 0.9 \\\n",
    "    --adam_beta2 0.99 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --lr_scheduler_type \"constant_with_warmup\" \\\n",
    "    --warmup_steps 50 \\\n",
    "    --logging_steps 2 \\\n",
    "    --freeze_text_encoder true \\\n",
    "    --audio_encoder_per_device_batch_size 5 \\\n",
    "    --dtype \"float16\" \\\n",
    "    --seed 42 \\\n",
    "    --output_dir \"./output_dir_training/\" \\\n",
    "    --temporary_save_to_disk \"./audio_code_tmp/\" \\\n",
    "    --save_to_disk \"./tmp_dataset_audio/\" \\\n",
    "    --dataloader_num_workers 2 \\\n",
    "    --do_eval \\\n",
    "    --predict_with_generate \\\n",
    "    --include_inputs_for_metrics \\\n",
    "    --save_total_limit 2 \\\n",
    "    --group_by_length true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc20a50e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:21:57.715153Z",
     "iopub.status.busy": "2024-12-02T04:21:57.714305Z",
     "iopub.status.idle": "2024-12-02T04:22:59.799303Z",
     "shell.execute_reply": "2024-12-02T04:22:59.798585Z"
    },
    "papermill": {
     "duration": 62.349861,
     "end_time": "2024-12-02T04:22:59.801269",
     "exception": false,
     "start_time": "2024-12-02T04:21:57.451408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"/kaggle/working/parler-tts/output_dir_training\", torch_dtype=torch.float16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b81a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T04:23:00.328095Z",
     "iopub.status.busy": "2024-12-02T04:23:00.327497Z",
     "iopub.status.idle": "2024-12-02T04:23:44.554288Z",
     "shell.execute_reply": "2024-12-02T04:23:44.553401Z"
    },
    "papermill": {
     "duration": 44.491849,
     "end_time": "2024-12-02T04:23:44.556452",
     "exception": false,
     "start_time": "2024-12-02T04:23:00.064603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8befa471238248c3ac3de33a9d9945a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad07690d68e047d188b6040fb817891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1003863f535c4122a7ed7f5ad66cb69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Amadeus99/parler-tts-mini-v1-yt-v1/commit/e2db281035da7e0262202eb444103c527d1c17b6', commit_message='Upload tokenizer', commit_description='', oid='e2db281035da7e0262202eb444103c527d1c17b6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Amadeus99/parler-tts-mini-v1-yt-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='Amadeus99/parler-tts-mini-v1-yt-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"parler-tts-mini-v1-yt-v1\")\n",
    "tokenizer.push_to_hub(\"parler-tts-mini-v1-yt-v1\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18298.021258,
   "end_time": "2024-12-02T04:23:47.346945",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-01T23:18:49.325687",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "026d752847a3442fbc8af62fa9a33349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44c745b4a70349cdad95a0105a483131",
       "placeholder": "​",
       "style": "IPY_MODEL_6fb49b8aa1494e02908a5397b312f02f",
       "value": " 792k/792k [00:00&lt;00:00, 3.55MB/s]"
      }
     },
     "05879545398c4022871606e82e1cc2b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b376c974e4942ffab6178a9fed63ceb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b45c0a4ebb384748a5b64a4c1648f0b1",
       "placeholder": "​",
       "style": "IPY_MODEL_130352db26ba493da7c98fcb8eabafc3",
       "value": "model.safetensors: 100%"
      }
     },
     "1003863f535c4122a7ed7f5ad66cb69b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_95480ec7798244a78d68340ec24cf3a2",
        "IPY_MODEL_5649f37cbeb34a4ba95079aa6df3dc0f",
        "IPY_MODEL_026d752847a3442fbc8af62fa9a33349"
       ],
       "layout": "IPY_MODEL_4348e27112204112949378eb9d60a8e1"
      }
     },
     "130352db26ba493da7c98fcb8eabafc3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2509203f2a7b48edbcc4cb9777a9f875": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b19260afdfd413fad0c6fc5305bfc22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_429926013ab142c08072ec79eaa3109b",
       "placeholder": "​",
       "style": "IPY_MODEL_612631e189dd465b874d08c5bcb60025",
       "value": " 1.76G/1.76G [00:32&lt;00:00, 53.0MB/s]"
      }
     },
     "2fa2a2864ae34c9fa22935a18dbf35b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e60116626f1a4f98866b75411cb714b5",
       "max": 1755809324.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ec88d77d93fe4b388e1136e67519b3f5",
       "value": 1755809324.0
      }
     },
     "429926013ab142c08072ec79eaa3109b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4348e27112204112949378eb9d60a8e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44c745b4a70349cdad95a0105a483131": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f1cd7d8bfc54f2e9e8712ec73dfc057": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5649f37cbeb34a4ba95079aa6df3dc0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2509203f2a7b48edbcc4cb9777a9f875",
       "max": 791656.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eeeb337bda984be8bf47fc72505f7c5d",
       "value": 791656.0
      }
     },
     "612631e189dd465b874d08c5bcb60025": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6fb49b8aa1494e02908a5397b312f02f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7df2fa3307e04118afdb73a57549a0e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8befa471238248c3ac3de33a9d9945a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0b376c974e4942ffab6178a9fed63ceb",
        "IPY_MODEL_2fa2a2864ae34c9fa22935a18dbf35b8",
        "IPY_MODEL_2b19260afdfd413fad0c6fc5305bfc22"
       ],
       "layout": "IPY_MODEL_05879545398c4022871606e82e1cc2b4"
      }
     },
     "95480ec7798244a78d68340ec24cf3a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7df2fa3307e04118afdb73a57549a0e4",
       "placeholder": "​",
       "style": "IPY_MODEL_e3ab17afb0d64fdea26920177fb8cee2",
       "value": "spiece.model: 100%"
      }
     },
     "9911331a747b46b8a9c676b2770e872e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad07690d68e047d188b6040fb817891a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ed1c5d8441a04ceb8d6b93e311b046c6",
        "IPY_MODEL_b77ca050091d4662bca97eed79beca0d",
        "IPY_MODEL_c4ad9fe5c9324187b1fb17557b68d640"
       ],
       "layout": "IPY_MODEL_f2caac29c8024a9f8e5124bc9ca36d25"
      }
     },
     "b45c0a4ebb384748a5b64a4c1648f0b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b77ca050091d4662bca97eed79beca0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9911331a747b46b8a9c676b2770e872e",
       "max": 5174.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fb5f04c7ed5c49998473800dc7b9173f",
       "value": 5174.0
      }
     },
     "c4ad9fe5c9324187b1fb17557b68d640": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e5c4767adeb14d9c93026eca45b70875",
       "placeholder": "​",
       "style": "IPY_MODEL_c7d971de190e48659e8a0cf2d491e044",
       "value": " 5.17k/5.17k [00:00&lt;00:00, 533kB/s]"
      }
     },
     "c7d971de190e48659e8a0cf2d491e044": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e3ab17afb0d64fdea26920177fb8cee2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e5c4767adeb14d9c93026eca45b70875": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e60116626f1a4f98866b75411cb714b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ebe6cf36e7f44d8baa07e3c8a0b93748": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ec88d77d93fe4b388e1136e67519b3f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ed1c5d8441a04ceb8d6b93e311b046c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f1cd7d8bfc54f2e9e8712ec73dfc057",
       "placeholder": "​",
       "style": "IPY_MODEL_ebe6cf36e7f44d8baa07e3c8a0b93748",
       "value": "README.md: 100%"
      }
     },
     "eeeb337bda984be8bf47fc72505f7c5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f2caac29c8024a9f8e5124bc9ca36d25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb5f04c7ed5c49998473800dc7b9173f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
